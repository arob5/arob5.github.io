---
title: The Ensemble Kalman Filter
subtitle: I introduce the ensemble Kalman filter as a Monte Carlo approximation to the Kalman filter in the linear Gaussian state space setting, then discuss how is applied as an approximation even when these assumptions don't hold.
layout: default
date: 2024-07-30
keywords: Filtering, State-Space, Hidden-Markov-Model, Bayes, Data-Assimilation
published: true
---

As discussed in a previous [post](https://arob5.github.io/blog/2024/02/15/Kalman-Filter/),
the Kalman filter (KF) provides the
closed-form mean and covariance recursions characterizing the Gaussian filtering
distributions for linear Gaussian hidden Markov models. In a follow-up
[post](https://arob5.github.io/blog/2024/02/15/nonlinear-maps-of-Gaussians/) we
relaxed the linearity assumption, and considered approximating nonlinear dynamics
or observation operators using analytical linearizations, which in turn allowed
us to fall back on the standard KF recursions. In this post, we
introduce the Ensemble Kalman Filter (EnKF),
another approximation scheme rooted in KF methodology. In place of gradient-based
linearizations, the EnKF relies on Monte Carlo or *particle*-based
approximations to address the nonlinearity.

In practice, the EnKF is commonly used as an approximation to the true Bayesian
solution in settings with nonlinear dynamics or observation operators, and is
in particular favored over competitors when the dimension of the state space
is quite high (potentially in the millions or billions). In the linear Gaussian
setting, it can be viewed as a Monte Carlo approximation to the KF. In this
post our main focus will be on the discrete-time, nonlinear Gaussian state
space model
{% katexmm %}
\begin{align}
v_{k+1} &= g(v_k) + \eta_{k+1} && \eta_{k+1} \sim \mathcal{N}(0, Q) \tag{1} \newline
y_{k+1} &= h(v_{k+1}) + \epsilon_{k+1}, && \epsilon_{k+1} \sim \mathcal{N}(0, R) \newline
v_0 &\sim \mathcal{N}(m_0, C_0), &&\\{\epsilon_k\\} \perp \\{\eta_k\\} \perp v_0,
\end{align}
with states $v_k \in \mathbb{R}^d$, observations $y_k \in \mathbb{R}^n$,
forward dynamics operator $g: \mathbb{R}^d \to \mathbb{R}^d$, and observation
operator $h: \mathbb{R}^d \to \mathbb{R}^n$. In general, we will consider
$g$ and $h$ to be nonlinear, though we will discuss simplifications when one or
the other is linear. As in previous posts,
we will use the notation $Y_k = \{y_1, \dots, y_k\}$ to denote the collection
of observations through time $k$. We denote the forecast and filtering densities
at time $k+1$ by $\hat{\pi}_{k+1}(v_{k+1}) := p(v_{k+1}|Y_k)$ and
$\pi_{k+1}(v_{k+1}) := p(v_{k+1}|Y_{k+1})$, respectively.
{% endkatexmm %}

## Kalman Filter Review
{% katexmm %}
Before introducing the EnKF we very briefly review the KF, which I discuss in detail
in [this](https://arob5.github.io/blog/2024/02/15/Kalman-Filter/) post.
The KF is applicable in the special case when the state space model (1) has
linear dynamics and a linear observation operator; that is,
$g(v) = Gv$ and $h(v) = Hv$ for some matrices $G \in \mathbb{R}^{d \times d}$
and $H \in \mathbb{R}^{n \times d}$.
We recall that under these assumptions, the forecast $\hat{\pi}_k$ and
filtering $\pi_k$ distributions are both Gaussian and are given by

\begin{align}
&v\_{k+1}|Y_{k} \sim \mathcal{N}\left(\hat{m}\_{k+1}, \hat{C}\_{k+1} \right),
&&v\_{k+1}|Y\_{k+1} \sim \mathcal{N}\left(m\_{k+1}, C\_{k+1} \right)
\end{align}

where

\begin{align}
\hat{C}\_{k+1} &= G C_k G^\top + Q \newline \tag{2}
\hat{m}\_{k+1} &= Gm_k \newline
C\_{k+1} &= \left(H^\top R^{-1} H + \hat{C}^{-1}\_{k+1}\right)^{-1} \newline
m\_{k+1} &= C\_{k+1}\left(H^\top R^{-1}y\_{k+1} + \hat{C}^{-1}\_{k+1} \hat{m}\_{k+1} \right).
\end{align}
{% endkatexmm %}

## The Big Picture
{% katexmm %}
There are many different perspectives on the EnKF, so it is easy to get lost in
the equations and terminology. We therefore take a moment to orient ourselves
before proceeding with the EnKF algorithm. The problem we are trying to solve
here is no different from that of the last couple posts; namely, we want to
characterize the filtering distributions $\pi_k$ at each time step $k$ in such
a way that we can relatively cheaply compute the update
$$
\pi_k \mapsto \pi_{k+1}
$$
once the new observation $y_{k+1}$ becomes available at the next time step.
Given that the filtering distribution $\pi_{k+1}$ encodes a compromise between
(1) the dynamical model's prediction and (2) the observed data, it is not
surprising that this update boils down to (1) using the dynamical model
to forecast one time step ahead, and then (2) conditioning on the new data
point. We can thus decompose the map $\pi_k \mapsto \pi_{k+1}$ as
$$
\pi_k \mapsto \hat{\pi}_{k+1} \mapsto \pi_{k+1},
$$
with the two arrows encoding steps (1) and (2), respectively. In the linear Gaussian
setting, all of these distributions turned out to be Gaussian, and hence
characterizing these two updates reduced to deriving updating equations for the
mean and covariance. Under our current assumptions, this will not be the case in
general. Despite this, the EnKF still approximates the filtering distributions
with Gaussians. However, instead of propagating means and covariances, it
propagates an ensemble of samples. Our update maps from above will thus assume the form
$$
\{v_k^{(j)}\}_{j=1}^{J}
\mapsto \{\hat{v}_{k+1}^{(j)}\}_{j=1}^{J}
\mapsto \{v_{k+1}^{(j)}\}_{j=1}^{J},
$$
for an ensemble of particles indexed by $j = 1,2, \dots, J$.
This sample-based approximation is in the spirit of the particle filter, but
instead of trying to ensure the samples have the exact correct distributions,
the EnKF updates rely on Gaussian approximations. Therefore, one view of the
EnKF is that it is an algorithm providing Gaussian approximations to
the filtering distributions $\pi_k$. Given the fact that the EnKF provides
results that do not in general align with the exact Bayesian solution, there is
a justifiable argument to abandon the Bayesian interpretation of the EnKF and
instead view it as a derivative-free optimization algorithm. Indeed, the
commonly-cited metrics of the EnKF's superior performance over alternatives
concern its ability to produce point predictions of $\mathbb{E}[v_k|Y_k]$.
We will explore both of these perspectives throughout this post. The following
section introduces the EnKF as a Monte Carlo approximation of the
filtering distributions, and the following subsection reinterprets the algorithm
from an optimization perspective.
{% endkatexmm %}

## Introducing the EnKF: a Monte Carlo Approximation
{% katexmm %}
Recall from the previous
[post](https://arob5.github.io/blog/2024/02/15/nonlinear-maps-of-Gaussians/) that
one way to view the challenge imposed by the nonlinear operators in (1)
is that of approximating the distribution of nonlinear maps of Gaussian random
variables. The extended Kalman
filter addresses this challenge via derivative-based linearization. The EnKF
is a derivative-free approach, instead opting for Monte Carlo based approximations.
I will synonymously refer to such approximations as *sample-based*, *ensemble-based*,
or *particle-based*. We now consider how to go about approximating both the forecast
and analysis steps in this fashion.

### Forecast
Suppose that we have access to an ensemble $(v_k^{(j)})_{j=1}^{J}$
providing a particle-based approximation of the filtering distribution $\pi_k$.
The goal is now to approximate the filtering
distribution $\hat{\pi}_{k+1}$. To do so, we can define a new ensemble
$(\hat{v}_{k+1}^{(j)})_{j=1}^{J}$ by
$$
\hat{v}_{k+1}^{(j)} := g(v_k^{(j)}) + \eta_{k+1}^{(j)}, \tag{3}
$$
where $\eta_{k+1}^{(j)} \overset{iid}{\sim} \mathcal{N}(0, Q)$. This is a
straightforward Monte Carlo approximation of $\hat{\pi}_{k+1}$. At this point,
there are two sources of error in using $(\hat{v}_{k+1}^{(j)})_{j=1}^{J}$ to
approximate $\hat{\pi}_{k+1}$:
1. Errors in the input ensemble $(v_k^{(j)})_{j=1}^{J}$ accumulated from earlier
steps of the algorithm; i.e., it may be that $v_k^{(j)}$ is not distributed
according to $\pi_k$.
2. Monte Carlo error stemming from the fact that we are using a finite ensemble
size $J$ to represent the distribution.

The first source of error is systematic, while the latter can be reduced by
increasing the ensemble size. At this point, we should note that the forecast
step has not contributed any new systematic errors, instead just propagating
existing ones. In other words, if $v_k^{(j)} \sim \pi_k$ for all $j$, then
the only source of error in $(\hat{v}_{k+1}^{(j)})_{j=1}^{J}$ would be of the
Monte Carlo variety.

We let $\hat{m}_{k+1}$ and $\hat{C}_{k+1}$ denote the empirical (i.e., sample)
mean and covariance matrix of the forecast ensemble. Note that since
$\hat{\pi}_{k+1}$ may be non-Gaussian, it is likely not characterized by its first
two moments alone. Nonetheless, we may consider a Gaussian approximation to the
forecast distribution given by $\mathcal{N}(\hat{m}_{k+1}, \hat{C}_{k+1})$.
Adopting such an approximation would introduce another source of systematic error
stemming from the Gaussian approximation of a potentially non-Gaussian distribution.

### Analysis
We now focus on transforming the forecast ensemble $(\hat{v}_{k+1}^{(j)})_{j=1}^{J}$
to a new ensemble $(v_{k+1}^{(j)})_{j=1}^{J}$ that (approximately) encodes the
operation of conditioning on the data $y_{k+1}$. We will pursue the same general
strategy as we did with the extended Kalman filter in the previous post.
Specifically, notice that the filtering distribution $\pi_{k+1}$ is
a conditional of the joint distribution

\begin{align}
&\begin{bmatrix} \hat{v}\_{k+1} \newline y\_{k+1} \end{bmatrix} :=
\begin{bmatrix} v\_{k+1} \newline y\_{k+1} \end{bmatrix} \bigg| Y_k  =
\begin{bmatrix} v\_{k+1} \newline h(v\_{k+1}) + \epsilon\_{k+1} \end{bmatrix} \bigg| Y_k. \tag{4}
\end{align}

The strategy is thus to approximate this joint distribution with a Gaussian, and
then use the standard Gaussian conditioning identities to obtain the conditional.
We will utilize the forecast ensemble $(\hat{v}_{k+1}^{(j)})_{j=1}^{J}$
in deriving the Gaussian approximation. In particular, we consider a Gaussian
approximation of (4) with mean and covariance set to the empirical mean and
covariance of the particles
$\{(\hat{v}_{k+1}^{(j)}, y_{k+1}^{(j)})\}_{j=1}^{J}$, where

\begin{align}
y\_{k+1}^{(j)}
&:= h(\hat{v}^{(j)}\_{k+1}) + \epsilon^{(j)}\_{k+1},
&&\epsilon^{(j)}\_{k+1} \overset{iid}{\sim} \mathcal{N}(0, R).
\end{align}

This yields the approximation

\begin{align}
\begin{bmatrix} v\_{k+1} \newline y\_{k+1} \end{bmatrix} \bigg| Y_k
&\overset{d}{\approx}
\mathcal{N}\left(
\begin{bmatrix} \hat{m}\_{k+1} \newline \overline{y}\_{k+1} \end{bmatrix},
\begin{bmatrix} \hat{C}\_{k+1} & \hat{C}^{vy}\_{k+1} \newline
                \hat{C}^{yv}\_{k+1} & \hat{C}^y\_{k+1} \end{bmatrix}
\right), \tag{5}
\end{align}

where

\begin{align}
\hat{m}\_{k+1} &:= \frac{1}{J} \sum\_{j=1}^{J} \hat{v}^{(j)}\_{k+1} \newline
\overline{y}\_{k+1} &:= \frac{1}{J} \sum\_{j=1}^{J} y^{(j)}\_{k+1} \newline
\hat{C}\_{k+1} &:= \frac{1}{J-1} \sum\_{j-1}^{J} (v\_{k+1}^{(j)}-\hat{m}\_{k+1})(v\_{k+1}^{(j)}-\hat{m}\_{k+1})^\top \newline
\hat{C}\_{k+1}^{vy} &:= \frac{1}{J-1} \sum_{j-1}^{J} (v\_{k+1}^{(j)}-\hat{m}\_{k+1})(y\_{k+1}^{(j)}-\overline{y}\_{k+1})^\top,
\end{align}
and $\hat{C}^{yu} := (\hat{C}^{uy})^\top$. We have approximated the joint
distribution $p(u_{k+1},y_{k+1}|Y_k)$ with the Gaussian (5), whose mean
and covariance have been furnished from empirical estimates derived from the
forecast ensemble. Naturally, this approximation introduces a new source of
systematic error in the algorithm.

At this point, we could apply the typical Gaussian conditioning identities
to (5). Denoting the resulting conditional mean and covariance $m_{k+1}$ and
$C_{k+1}$, respectively, we could then define the Gaussian approximation
$\mathcal{N}(m_{k+1}, C_{k+1})$ to the filtering distribution $\pi_{k+1}$.
We could use this Gaussian to study the state of the system at time $k+1$,
and then draw samples from it to obtain the ensemble $(v_{k+1}^{(j)})_{j=1}^{J}$,
which can then be fed through the dynamical model to start the cycle over again
at time $k+2$. However, there is a neat way to avoid having to explicitly
compute $m_{k+1}$ and $C_{k+1}$ and instead directly update each particle
$\hat{v}_{k+1}^{(j)} \mapsto v_{k+1}^{(j)}$. Matheron's rule (see appendix)
provides a formula for converting samples from the joint distribution (5) to
conditional samples.
{% endkatexmm %}

## The Optimization Perspective
