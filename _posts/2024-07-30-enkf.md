---
title: The Ensemble Kalman Filter
subtitle: I introduce the ensemble Kalman filter as a Monte Carlo approximation to the Kalman filter in the linear Gaussian state space setting, then discuss how is applied as an approximation even when these assumptions don't hold.
layout: default
date: 2024-07-30
keywords: Filtering, State-Space, Hidden-Markov-Model, Bayes, Data-Assimilation
published: true
---

As discussed in a previous [post](https://arob5.github.io/blog/2024/02/15/Kalman-Filter/),
the Kalman filter (KF) provides the
closed-form mean and covariance recursions characterizing the Gaussian filtering
distributions for linear Gaussian hidden Markov models. In a follow-up
[post](https://arob5.github.io/blog/2024/02/15/nonlinear-maps-of-Gaussians/) we
relaxed the linearity assumption, and considered approximating nonlinear dynamics
or observation operators using analytical linearizations, which in turn allowed
us to fall back on the standard KF recursions. In this post, we
introduce the Ensemble Kalman Filter (EnKF),
another approximation scheme rooted in KF methodology. In place of gradient-based
linearizations, the EnKF relies on Monte Carlo or *particle*-based
approximations to address the nonlinearity.

In practice, the EnKF is commonly used as an approximation to the true Bayesian
solution in settings with nonlinear dynamics or observation operators, and is
in particular favored over competitors when the dimension of the state space
is quite high (potentially in the millions or billions). In the linear Gaussian
setting, it can be viewed as a Monte Carlo approximation to the KF. In this
post our main focus will be on the discrete-time, nonlinear Gaussian state
space model
{% katexmm %}
\begin{align}
v_{k+1} &= g(v_k) + \eta_{k+1} && \eta_{k+1} \sim \mathcal{N}(0, Q) \tag{1} \newline
y_{k+1} &= h(v_{k+1}) + \epsilon_{k+1}, && \epsilon_{k+1} \sim \mathcal{N}(0, R) \newline
v_0 &\sim \mathcal{N}(m_0, C_0), &&\\{\epsilon_k\\} \perp \\{\eta_k\\} \perp v_0,
\end{align}
with states $v_k \in \mathbb{R}^d$, observations $y_k \in \mathbb{R}^n$,
forward dynamics operator $g: \mathbb{R}^d \to \mathbb{R}^d$, and observation
operator $h: \mathbb{R}^d \to \mathbb{R}^n$. In general, we will consider
$g$ and $h$ to be nonlinear, though we will discuss simplifications when one or
the other is linear. As in previous posts,
we will use the notation $Y_k = \{y_1, \dots, y_k\}$ to denote the collection
of observations through time $k$. We denote the forecast and filtering densities
at time $k+1$ by $\hat{\pi}_{k+1}(v_{k+1}) := p(v_{k+1}|Y_k)$ and
$\pi_{k+1}(v_{k+1}) := p(v_{k+1}|Y_{k+1})$, respectively.
{% endkatexmm %}

## Kalman Filter Review
{% katexmm %}
Before introducing the EnKF we very briefly reivew the KF, which I discuss in detail
in [this](https://arob5.github.io/blog/2024/02/15/Kalman-Filter/) post.
The KF is applicable in the special case when the state space model (1) has
linear dynamics and a linear observation operator; that is,
$g(v) = Gv$ and $h(v) = Hv$ for some matrices $G \in \mathbb{R}^{d \times d}$
and $H \in \mathbb{R}^{n \times d}$.
We recall that under these assumptions, the forecast $\hat{\pi}_k$ and
filtering $\pi_k$ distributions are both Gaussian and are given by

\begin{align}
&v\_{k+1}|Y_{k} \sim \mathcal{N}\left(\hat{m}\_{k+1}, \hat{C}\_{k+1} \right),
&&v\_{k+1}|Y\_{k+1} \sim \mathcal{N}\left(m\_{k+1}, C\_{k+1} \right)
\end{align}

where

\begin{align}
\hat{C}\_{k+1} &= G C_k G^\top + Q \newline \tag{2}
\hat{m}\_{k+1} &= Gm_k \newline
C\_{k+1} &= \left(H^\top R^{-1} H + \hat{C}^{-1}\_{k+1}\right)^{-1} \newline
m\_{k+1} &= C\_{k+1}\left(H^\top R^{-1}y\_{k+1} + \hat{C}^{-1}\_{k+1} \hat{m}\_{k+1} \right).
\end{align}
{% endkatexmm %}

## Introducing the EnKF: a Monte Carlo Approximation
{% katexmm %}
Recall from the previous
[post](https://arob5.github.io/blog/2024/02/15/nonlinear-maps-of-Gaussians/) that
one way to view the challenge imposed by nonlinearity is that of approximating the
distribution of nonlinear maps of Gaussian random variables. The extended Kalman
filter addresses this challenge via derivative-based linearlization. The EnKF
is a derivative-free approach, instead opting for Monte Carlo based approximations.
I will synonymously refer to such approximations as *sample-based*, *ensemble-based*,
or *particle-based*. We now consider how to go about approximating both the forecast
and analysis steps in this fashion.

### Forecast
Suppose that we have access to an ensemble $\left(v_k^{(j)}\right)_{j=1}^{J}$
providing a particle-based approximation of the filtering distribution $\pi_k$,
such that $v_k^{(j)} \sim \pi_k$ for all $j$. The goal is now to approximate the filtering
distribution $\hat{\pi}_{k+1}$. To do so, we can define a new ensemble
$\left(\hat{v}_{k+1}^{(j)}\right)_{j=1}^{J}$ by
$$
\hat{v}_k^{(j)} := g\left(v_k^{(j)}\right) + \eta_{k+1}^{(j)}, \tag{3}
$$
where $\eta_{k+1}^{(j)} \overset{iid}{\sim} \mathcal{N}(0, Q)$. This is a
straightforward Monte Carlo approximation with the property that
$\hat{v}_k^{(j)} \sim \hat{\pi}_{k+1}$ for all $j$. In other words, the only
approximation error in $\left(\hat{v}_{k+1}^{(j)}\right)_{j=1}^{J}$ is the Monte
Carlo error stemming from the finite sample size; the samples are *exactly*
distributed according to the forecast distribution (owing to the assumption
that the $v_k^{(j)}$ were exactly distributed according to the $\pi_k$).

We let $\hat{m}_{k+1}$ and $\hat{C}_{k+1}$ denote the empirical (i.e., sample)
mean and covariance
matrix of the forecast ensemble. Note that since $\hat{\pi}_{k+1}$ may be nonlinear,
it is likely not characterized by its first two moments alone. Nonetheless, we may
consider a Gaussian approximation to the forecast distribution given by
$\mathcal{N}\left(\hat{m}_{k+1}, \hat{C}_{k+1}\right)$. Under this approximation
there are now two sources of error: (1) the Monte Carlo error in the mean and
covariance estimates; and (2) the error due to the Gaussian approximation.

### Analysis
We will pursue the same general strategy as we did with the extended Kalman filter.
Specifically, notice that the filtering distribution $\pi_{k+1}$ is
a conditional of the joint distribution of

\begin{align}
u\_{k+1} &:=
\begin{bmatrix} \hat{v}\_{k+1} \newline y\_{k+1} \end{bmatrix} :=
\begin{bmatrix} v\_{k+1} \newline y\_{k+1} \end{bmatrix} \bigg| Y_k  =
\begin{bmatrix} v\_{k+1} \newline h(v\_{k+1}) + \epsilon\_{k+1} \end{bmatrix} \bigg| Y_k. \tag{4}
\eng{align}

The strategy is thus to approximate this joint distribution with a Gaussian, and
then use the standard Gaussian conditioning identities to obtain the conditional.
We will utilize the forecast ensemble $\left(\hat{v}_{k+1}^{(j)}\right)_{j=1}^{J}$
in deriving the Gaussian approximation. In particular, we consider approximating the
distribution of (4) with a Gaussian with mean and covariance set to the
empirical mean and covariance of the particles

\begin{align}
u^{(j)}\_{k+1} &:=
\begin{bmatrix} \hat{v}^{(j)}\_{k+1} \newline h(\hat{v}^{(j)}\_{k+1}) + \epsilon^{(j)}\_{k+1} \end{bmatrix},
\begin{align}

where $\epsilon^{(j)}_{k+1} \overset{iid}{\sim} \mathcal{N}(0, R)$.
{% endkatexmm %}

## The Optimization Perspective 
