---
title: Integrated Mean Squared Prediction Error for Experimental Design with Gaussian Processes
subtitle: I describe various uses and characterizations of integrated variance criteria for experimental design with Gaussian processes.
layout: default
date: 2024-03-29
keywords: GP
published: false
---

{% katexmm %}
For Gaussian processes (GPs) the question of experimental design is that of
choosing the locations $X$. We call the $x_n$ *design points* and $X$ the
*design matrix*. In general, we would like to choose $X$ such that uncertainty
in the resulting predictive distribution $f_N$ is minimized in some sense.
A common approach in the literature is to used criteria based in information theory
to formalize what we mean by uncertainty. In this post, we will instead focus on
a popular alternative which instead derives criteria based on the predictive
variance of the GP; i.e., $k_N(x)$.

One reasonable approach is to construct the design $X$ one point at a time by
selecting the location which is currently the most uncertain; i.e.,
$$
x_{n+1} := \text{argmax}_{x \in \mathcal{X}} \text{Var}(f(x)|X_{n+1}, Y_{n+1}). \tag{1}
$$
Going forward we will use the succinct notation
$$
k_{n+1}(x) = \text{Var}(f(x)|X_{n+1}, Y_{n+1}),
$$
which denotes the variance of the random quantity $f(x)$ after conditioning
on the data $(X_{n+1}, Y_{n+1})$. This can equivalently be thought of as the
variance of $f_n(x)$ (which is already conditional on $(X_n, Y_n)$) after
additionally conditioning on the new acquisition $(x_{n+1}, y_{n+1})$. This
greedy maximum variance approach is intuitive and can produce reasonable results
in certain cases, but has some drawbacks. By focusing on the single point of
maximal variance, this objective is hyper-localized. In general, we hope to
choose a design that reduces the overall uncertainty across the entire design
space in some average sense. The maximum variance criterion does not reflect
this goal, which can result in some undesireable behavior. For example,
optimizing (1) tends to favor points on the boundary of $\mathcal{X}$; while
boundary points commonly exhibit the highest variance due to the fact that the
model is essentially extrapolating (rather than interpolating) in these
exterior regions, their selection is often not optimal as far as reducing
uncertainty across all of $\mathcal{X}$; some nice examples of this are detailed  
[here](https://bookdown.org/rbg/surrogates/chap6.html#chap6sequential).

To capture the notion of wanting to globally reduce uncertainty, we can
define the objective function by replacing the maximum in (1) with some sort
of average.



{% endkatexmm %}
