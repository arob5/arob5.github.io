---
title: Gaussian Measures, Part 2 - The Multivariate Case
subtitle: A fairly deep dive into Gaussian measures in finitely many dimensions. The next step in building up to the infinite-dimensional case.
layout: default
date: 2024-05-19
keywords: GP, Prob-Theory
published: true
---


## Preliminaries
{% katexmm %}
Let $x, y \in \mathbb{R}^n$. Throughout this post,
we write $\langle x, y \rangle = x^\top y$ for the standard
inner product on $\mathbb{R}^n$, and $\lVert x \rVert_2 = \sqrt{\langle x, x \rangle}$
the norm induced by this inner product. We will frequently consider linear
functions of the form $\ell: \mathbb{R}^n \to \mathbb{R}$, and denote the set
of all such functions as $(\mathbb{R}^n)^*$. Every
$\ell \in (\mathbb{R}^n)^*$ can be uniquely represented as an inner product
with some vector $y \in \mathbb{R}^n$. When we wish to make this identification
explicit, we will write $\ell_y$ to denote the linear map given by
$\ell_y(x) = \langle x, y \rangle$. Likewise, if we are working with a generic
linear map $\ell \in (\mathbb{R}^n)^*$, then we will write $y_{\ell} \in \mathbb{R}^n$
to denote the unique vector satisfying $\ell(x) = \langle x, y_{\ell} \rangle$.
We will also loosely refer to $\ell_y(x)$ as a *projection* onto $y$.
Note that if $y$ has unit norm, then this is precisely the magnitude of the
orthogonal projection of $x$ onto $y$.
{% endkatexmm %}

## Sigma Algebra
{% katexmm %}
We recall from the previous post that a univariate Gaussian measure is defined
on the Borel $\sigma$-algebra $\mathcal{B}(\mathbb{R})$. Analogously, we
will define an $n$-dimensional Gaussian measure on the Borel sets
$\mathcal{B}(\mathbb{R}^n)$. There are two reasonable approaches
to define $\mathcal{B}(\mathbb{R}^n)$, and I want to take a moment to highlight
them since the same two options will present themselves when we consider
defining the Borel sets over infinite-dimensional spaces.

### Option 1: Leverage the Standard Topology on $\mathbb{R}^n$
A Borel $\sigma$-algebra can be defined for any space that comes equipped with
a topology; i.e., a collection of open sets. The Borel $\sigma$-algebra is then
defined as the smallest $\sigma$-algebra that contains all of these open sets.
In the present setting, this means
$$
\mathcal{B}(\mathbb{R}^n)
:= \sigma\left\{\mathcal{O} \subseteq \mathbb{R}^n : \mathcal{O} \text{ is open} \right\}, \tag{1}
$$
where $\sigma(\mathcal{S})$ denotes the $\sigma$-algebra generated by a collection
of sets $\mathcal{S}$. A nice perspective on $\mathcal{B}(\mathbb{R}^n)$ is
that it is the smallest $\sigma$-algebra that ensures all continuous
functions $f: \mathbb{R}^n \to \mathbb{R}$ are measurable. We note that this is
*not* a property of Borel $\sigma$-algebras more generally, but one
that does hold in the special case of $\mathbb{R}^n$; see
[this](https://math.stackexchange.com/questions/3952771/borel-sigma-algebra-and-measurability-of-continuous-functions?rq=1) StackExchange post for some details.


### Option 2: Product of One-Dimensional Borel Sets
A second reasonable approach is to try extending what we have already defined
in one dimension, which means simply taking Cartesian products of one-dimensional
Borel sets:
$$
\mathcal{B}(\mathbb{R}^n)
:= \sigma\left\{B_1 \times \cdots \times B_n: B_i \in \mathcal{B}(\mathbb{R}), \ i = 1, \dots, n \right\}. \tag{2}
$$
It turns out that the resulting $\sigma$-algebra agrees with that defined
in option 1, so there is no ambiguity in the notation.
{% endkatexmm %}

## Definition: One-Dimensional Projections
{% katexmm %}
With the $\sigma$-algebra defined, we now consider how to define a Gaussian
measure $\mu$ on the measurable space $(\mathbb{R}^n, \mathcal{B}(\mathbb{R}^n))$.
We will explore a few different equivalent definitions, starting with this:
a measure is Gaussian if all of its one-dimensional projections are
(univariate) Gaussians.
<blockquote>
  <p><strong>Definition.</strong>
  A probability measure $\mu$ defined on the Borel measurable space
  $(\mathbb{R}^n, \mathcal{B}(\mathbb{R}^n))$ is called <strong>Gaussian</strong>
  if, for all linear maps $\ell \in (\mathbb{R}^n)^*$, the pushforward measure
  $\mu \circ \ell^{-1}$ is Gaussian on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$.
  </p>
</blockquote>

As in the univariate setting, we define a random variable $X$ as Gaussian
if its law $X$ is a Gaussian measure. Recall that each linear map $\ell$
can be identified with a unique $y \in \mathbb{R}^n$ such that
$\ell(x) = \langle x, y \rangle$, which we indicate by writing
$\ell_y = \ell$. We thus see that $\mu \circ \ell_{y}^{-1}$ is the
distribution of the random variable $\langle X, y \rangle$. The previous
definition can therefore be re-stated in the language of random variables
as follows: an $n$-dimensional random variable is Gaussian if every linear
combination of the entries of $X$ is univariate Gaussian. More precisely:
<blockquote>
  <p><strong>Definition.</strong>
  Let $(\Omega, \mathcal{A}, \mathbb{P})$ be a a probability space and
  $X: \Omega \to \mathbb{R}^n$ a random vector. Then $X$ is called
  <strong>Gaussian</strong> if $\langle X, y \rangle$ is a univariate Gaussian
  random variable for all $y \in \mathbb{R}^n$.
  </p>
</blockquote>
Notice that by choosing $y := e_j$ (the vector with a $1$ in its $j^{\text{th}}$
entry and zeros everywhere else), then this definition immediately tells us that
a Gaussian random vector has univariate  Gaussian marginal distributions.
That is, if $X = (X_1, \dots, X_n)^\top$ then $X_i$ is univariate Gaussian
for all $i = 1, \dots, n$.
{% endkatexmm %}

## Fourier Transform
{% katexmm %}
Just as in the univariate case, the Fourier transform $\hat{\mu}$ provides
an alternate, equivalent, characterization of Gaussian measures. First, we
recall how such a Fourier transform is defined in the multiple variable
setting.
<blockquote>
  <p><strong>Definition.</strong>
  Let $\mu$ be a measure on $(\mathbb{R}^n, \mathcal{B}(\mathbb{R}^n))$.
  Then the Fourier transform of $\mu$ is defined as
  $$
  \hat{\mu}(y) := \int_{\mathbb{R}^n} e^{i\langle x, y\rangle} \mu(dx),
  \qquad y \in \mathbb{R}^n \tag{3}
  $$
  </p>
</blockquote>
We can alternatively view $\hat{\mu}$ as a function of
$\ell_y \in (\mathbb{R}^n)^*$;
that is,
$$
\ell_y \mapsto \int_{\mathbb{R}^n} e^{i \ell_y(x)} \mu(dx).
$$
Note that this is similar in spirit to the definition of the $n$-dimensional
Gaussian measure, in the sense that the extension from one to multiple
dimensions is acheived by considering one-dimensional linear projections.
This idea will also provide the basis for an extension to infinite dimensions.

With this background established, we can state the following, which gives
an alternate definition of Gaussian measures.
<blockquote>
  <p><strong>Theorem.</strong>
  A probability measure $\mu$ defined on the Borel measurable space
  $(\mathbb{R}^n, \mathcal{B}(\mathbb{R}^n))$ is Gaussian if and only
  if its Fourier transform is of the form
  $$
  \hat{\mu}(y) = \exp\left\{i\langle m, y\rangle - \frac{1}{2}\langle Cy, y\rangle \right\}, \tag{4}
  $$
  for some fixed vector $m \in \mathbb{R}^{n}$ and symmetric, positive
  semi-definite matrix $C \in \mathbb{R}^{n \times n}$.
  </p>
</blockquote>
The proof, which is given in the appendix, also provides the expressions for
the mean and covariance of $\mu$ as a byproduct.
<blockquote>
  <p><strong>Corollary.</strong>
  Let $\mu$ be a Gaussian measure with Fourier transform
  $\hat{\mu}(y) = \exp\left\{\langle y, m\rangle - \frac{1}{2}\langle Cy, y\rangle\right\}$. Then the mean vector and covariance matrix of $\mu$ are
  given by
  \begin{align}
  \int x \mu(dx) &= m \tag{5} \newline
  \int (x-m)(x-m)^\top \mu(dx) &= C.
  \end{align}
  </p>
</blockquote>

{% endkatexmm %}


## Density Function
The one-dimensional projections and Fourier transform provide equivalent
definitions of multivariate Gaussian measures that apply in full generality.
The more familiar notion of the Gaussian density provides a third characterization,
with the caveat that it only pertains to the case that the covariance matrix
$C$ is positive definite.
<blockquote>
  <p><strong>Proposition.</strong>
  Let $\mu$ be a Gaussian measure with mean vector and
  covariance matrix $m$ and $C$, respectively, given as in (5). Then $\mu$ admits
  a Lebesgue density if and only if $C$ is positive definite, in which case
  $$
  \frac{d\mu}{d\lambda}(x) &= \text{det}(2\pi C)^{-1/2}\langle C^{-1}(y-m), y-m\rangle.
  $$
  </p>
</blockquote>

## Transformation of Standard Gaussian Random Variables
In this section we provide yet another characterization of Gaussian measures.
We consider a *generative* perspective, whereby a Gaussian random vector
$X \in \mathbb{R}^n$ arises via a linear transformation of $n$ iid
$\mathcal{N}(0,1)$ random variables.  
<blockquote>
  <p><strong>Proposition.</strong>
  Let $Z_1, \dots, Z_n$ be iid $\mathcal{N}(0, 1)$ random variables stacked into
  the column vector $Z \in \mathbb{R}^n$. Then, for any fixed vector
  $m \in \mathbb{R}^n$ and matrix $A \in \mathbb{R}^{n \times n}$, the random
  variable given by
  $$
  X := m + AZ \tag{6}
  $$
  has a Gaussian distribution $\mathcal{N}(m, AA^\top)$.

  Conversely, let $X \in \mathbb{R}^n$ be a Gaussian random variable. Then
  there exists a vector $m \in \mathbb{R}^n$ and matrix $A \in \mathbb{R}^{n \times n}$
  such that $X = m + AZ$.
  </p>
</blockquote>

{% katexmm %}
Another way to think about this is that we have defined a *transport map*
$T: \mathbb{R}^n \to \mathbb{R}^n$ such that
\begin{align}
T(Z) &= X, &&\text{where } T(z) = m + Az.
\end{align}
That is, we feed in vectors with iid standard Gaussian components, and get out
vectors with distribution $\mathcal{N}(m, AA^\top)$. This is a very practical
way to look at multivariate Gaussians, immediately providing the basis for
a sampling algorithm. Indeed, suppose we want to draw iid samples from
the distribution $\mathcal{N}(m, C)$. Then the above proposition gives us a
way to do so, provided that we can (1) draw univariate $\mathcal{N}(0,1)$
samples; and (2) factorize the matrix $C$ as $C = AA^\top$ for some $A$. This
procedure is summarized in the below corollary.

<blockquote>
  <p><strong>Corollary.</strong>
  The following algorithm produces a sample from the distribution
  $\mathcal{N}(m, C)$. <br>

  1. Draw $n$ iid samples $Z_i \sim \mathcal{N}(0,1)$ and stack them in a column
  vector $Z$.  <br>
  2. Compute a factorization $C = AA^\top$.  <br>
  3. Return $m + AZ$.  <br>

  Repeating steps 1 and 3 will produce independent samples from $\mathcal{N}(m, C)$
  (the matrix factorization need not be re-computed each time).  
  </p>
</blockquote>
As for the factorization, the [Cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition)
is a standard choice when $C$ is positive definite.
When $C$ is only positive semidefinite,
the eigendecomposition provides another option, since
$$
C = UDU^\top = UD^{1/2} D^{1/2} U^\top = (UD^{1/2})(UD^{1/2})^\top
$$
so setting $A := UD^{1/2}$ does the trick. Note that $C$ is positive semidefinite
so $D$ is just a diagonal matrix with nonnegative values on the diagonal.

{% endkatexmm %}


## Covariance Operator

## Conditional Distributions

## Appendix

### Proof of (4): Fourier Transform Characterization
{% katexmm %}
Assume that the probability measure $\mu$ has a Fourier transform given by
$$
\hat{\mu}(y) = \exp\left\{i \langle m, y\rangle - \frac{1}{2}\langle Cy, y\rangle \right\},
$$
for some nonrandom vector $m \in \mathbb{R}^n$ and symmetric positive semidefinite
matrix $C \in \mathbb{R}^{n \times n}$. We must show that the pushforward
$\mu \circ \ell_y^{-1}$ is Gaussian for an arbitrary
$\ell_y \in \left(\mathbb{R}^n\right)^*$. We will do so by invoking the known
form of the Fourier transform for univariate Gaussians. To this end, let
$t \in \mathbb{R}$ and consider
\begin{align}
\mathcal{F}\left(\mu \circ \ell_y^{-1}\right)(t)
&= \int e^{its} \left(\mu \circ \ell_y^{-1} \right)(ds) \newline
&= \int e^{it \ell_y(x)} \mu(dx) \newline
&= \int e^{i \langle ty, x\rangle} \mu(dx) \newline
&= \hat{\mu}(ty) \newline
&= \exp\left(i \langle m, ty\rangle - \frac{1}{2}\langle C(ty), ty\rangle \right) \newline
&= \exp\left(it \langle m, y\rangle - \frac{1}{2}t^2\langle Cy, y\rangle \right),
\end{align}
where the second equality uses the change-of-variables formula, and the final
uses the assumed form of $\hat{\mu}$. Also recall the alternate notation for
the Fourier transform: $\hat{\mu}(y) = \mathcal{F}(\mu)(y)$.
We recognize the final expression above as
the Fourier transform of a univariate Gaussian measure with mean
$\langle y, m\rangle$ and variance $\langle Cy, y\rangle$, evaluated at
frequency $t$. This implies that $\mu \circ \ell_y^{-1}$ is Gaussian. Since
$\ell_y \in \left(\mathbb{R}^n \right)^*$ was arbitrary, it follows by definition
that $\mu$ is Gaussian.

Conversely, assume that $\mu$ is Gaussian. Then, $\mu \circ \ell_y^{-1}$ is
univariate Gaussian for all $\ell_y \in \left(\mathbb{R}^n \right)^*$. We must
show that $\hat{\mu}$ assumes the claimed form. Letting $y \in \mathbb{R}^n$,
we have
\begin{align}
\hat{\mu}(y)
&= \int e^{i \langle y, x\rangle} \mu(dx) \newline
&= \int e^{is} \left(\mu \circ \ell_y^{-1}\right)(ds) \newline
&= \mathcal{F}\left(\mu \circ \ell_y^{-1}\right)(1) \newline
&= \exp\left(i m(y) - \frac{1}{2}\sigma^2(y) \right),
\end{align}
where $m(y)$ and $\sigma^2(y)$ are the mean and variance of $\mu \circ \ell_y^{-1}$,
respectively. The first equality again uses the change-of-variables formula, while
the last expression follows from the assumption that $\mu \circ \ell_y^{-1}$
is Gaussian, and hence must have a Fourier transform of this form. It remains
to verify that $m(y) = \langle y, m\rangle$ and
$\sigma^2(y) = \langle Cy, y\rangle$ to complete the proof. By definition, the  
mean of $\mu \circ \ell_y^{-1}$ is given by
\begin{align}
m(y) &= \int \ell_y(x) \mu(dx) \newline
&= \int \langle y, x\rangle \mu(dx) \newline
&= \left\langle y, \int x \mu(dx) \right\rangle \newline
&=: \langle y, m \rangle,
\end{align}
where we have used the linearity of integration and defined the nonrandom
vector $m := \int x \mu(dx)$. Now, for the variance we have
\begin{align}
\sigma^2(y)
&= \int \left[\ell_y(x) - m(y) \right]^2 \mu(dx) \newline
&= \int \left[\langle y, x\rangle - \langle y, m \rangle \right]^2 \mu(dx) \newline
&= \int \langle y, x-m\rangle^2 \mu(dx) \newline
&= y^\top \left[\int (x-m)(x-m)^\top \mu(dx) \right] y \newline
&=: y^\top C y \newline
&= \langle Cy, y \rangle.
\end{align}
Note that $\sigma^2(y)$ is the expectation of a nonnegative quantity, so
$\langle Cy, y \rangle \geq 0$ for all $y \in \mathbb{R}^n$; i.e.,  
$C$ is positive semidefinite. We have thus shown that
\begin{align}
\hat{\mu}(y) &= \exp\left(\langle y, m\rangle - \frac{1}{2}\langle Cy,y\rangle \right),
\end{align}
with $C$ a positive semidefinite matrix, as required. $\qquad \blacksquare$
{% endkatexmm %}


## References
