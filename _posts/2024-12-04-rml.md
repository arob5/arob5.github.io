---
title: Randomized Maximum Likelihood Sampling
subtitle: Approximate posterior inference and data assimilation applications.
layout: default
date: 2024-12-04
keywords: Bayes, Filtering, Data-Assim
published: true
---

# Setup
{% katexmm %}
In this post, we will consider an approximate posterior inference scheme
tailored to Bayesian models of the form
\begin{align}
y|u &\sim \mathcal{N}(\mathcal{G}(u), \Sigma) \tag{1} \newline
u &\sim \mathcal{N}(m, C),
\end{align}
where $y \in \R^n$ and $u \in \R^d$.
The statistical model here is very simple, consisting of a Gaussian likelihood
and prior. The complexity is baked into the *forward model* $\mathcal{G}$,
which may be nonlinear and computationally demanding to evaluate. The
posterior distribution of (1) is given by
$$
\pi(u) := p(u|y) \propto \exp\left[-J(u)\right] \tag{2}
$$
where $J(u)$ is a nonlinear least squares *cost function* given by
$$
J(u) = \frac{1}{2} \lVert y - \fwd(u)\rVert^2_{\Sigma} + \frac{1}{2}\lVert u-m \rVert^2_{C}. \tag{3}
$$
In (3) we have used the following notation for inner products and norms
weighted by a positive definite matrix $C$:
\begin{align}
\langle u, u^\prime \rangle_C &:= \langle C^{-1}u, u^\prime\rangle = u^\top C^{-1}u^\prime \newline
\lVert u \rVert_C^2 &:= \langle u, u\rangle_C.
\end{align}
Standard methods for optimization or sampling require evaluating the cost
function $J(u)$ sequentially many times, which may be infeasible when the
cost of computing $\fwd(u)$ is very large. In this post, we consider a
method for posterior inference that allows for cost function evaluations
to be computed in parallel. The tradeoff is that the method is an approximation
except when $\fwd$ is linear, and the properties of the approximation are
poorly understood.

# The Basic Algorithm
The basic RML algorithm proceeds by randomly generating an ensemble of
cost functions $\{J^{(i)}\}$, $i = 1, \dots, m$ and then optimizing each
one independently. The ensemble consisting of the optimizers for each of the
$J^{(i)}$ are then interpreted as approximate samples from the posterior
distribution. This procedure is summarized below.
<blockquote>
  <p><strong>Randomized Maximum Likelihood Sampling.</strong> <br>
  Construct an ensemble of $m$ cost functions $\{J^{(i)}\}$, where
  $J^{(i)}$ is defined by sampling
  \begin{align}
  &u^{(i)} \sim \mathcal{N}(m, C), &&y^{(i)} \sim \mathcal{N}(y, \Sigma) \tag{4}
  \end{align}
  and then defining $J^{(i)}: \R^d \to [0,\infty)$ by
  $$
  J^{(i)}(u) := \frac{1}{2} \lVert y^{(i)} - \fwd(u)\rVert^2_{\Sigma} +
  \frac{1}{2}\lVert u-u^{(i)} \rVert^2_{C}. \tag{5}
  $$
  The RML algorithm outputs the ensemble $\{u^{(i)}_{\star}\}$, where
  $$
  u^{(i)}_{\star} := \argmin_{u \in \R^d} J^{(i)}(u). \tag{6}
  $$
  </p>
</blockquote>

Note that the RML method requires solving $m$ nonlinear least square problems
(6), but they are decoupled and hence can be solved in parallel. The cost
functions $J^{(i)}$ are defined by sampling $(u^{(i)}, y^{(i)})$ and then
altering $J(u)$ by replacing $m$ with $u^{(i)}$ and $y$ with $y^{(i)}$. The
method of sampling $(u^{(i)}, y^{(i)})$ may seem a bit peculiar; the $u^{(i)}$
are independent draws from the prior, while $y^{(i)}$ is simply the observed
data $y$ that has been perturbed by the addition of a sampled noise
realization $\epsilon^{(i)} \sim \Gaussian(0, \Sigma)$. Sometimes $y^{(i)}$
is referred to as "simulated data". The following section provides some
justification for replacing $y$ with the simulated data; namely, the RML
produces exact samples from the posterior when $\fwd$ is linear.
{% endkatexmm %}

# Linear Forward Model
{% katexmm %}
The following result states that RML exactly samples the posterior if the
forward model is linear. The proof is given in the appendix.

<blockquote>
  <p><strong>Exactness in linear setting.</strong> <br>
  Suppose that the forward model in (1) is linear; i.e., $\fwd(u) = Gu$, for
  some matrix $G \in \R^{n \times d}$. Then the samples $\{u^{(i)}_{\star}\}$
  returned by the RML algorithm (6) admit the closed-form expression
  $$
  u^{(i)}_{\star} =
  u^{(i)} + CG^\top \left(GCG^\top + \Sigma \right)^{-1}(y^{(i)} - Gu^{(i)}) \tag{7}
  $$
  and satisfy
  $$
  u^{(i)}_{\star} \overset{iid}{\sim} \pi \tag{8}
  $$
</blockquote>

Thus, in the linear case, posterior inference can be cast as a stochastic
optimization problem.

## Ensemble Kalman Filter with Linear Observation Operator
A common example of this linear setting is in the
analysis step of the ensemble Kalman filter (EnKF), under the assumption
of a linear observation operator. In this case, $G$ is the observation
operator, $u$ is the state variable, and (7) is the well-known EnKF update
equation. However, even with a linear observation operator, the exactness
(8) does not hold for the EnKF. The reason is that the prior distribution on the
state variable is rarely Gaussian in the settings in which the EnKF is applied.
Indeed, in this setting the prior ensemble $\{u^{(i)}\}$ is not simply drawn
from a Gaussian prior, but rather constitutes a forecast ensemble output by
a complicated physical model. If the forecast ensemble is well-aporoximated
by a Gaussian, then we might expect the EnKF update to provide a good
approximation to the true posterior.
{% endkatexmm %}

# Nonlinear Forward Model
In the general case of a nonlinear forward model, the RML samples are no longer
guaranteed to be distributed according to the true posterior. Thus, RML is
in general a method for *approximate* inference. We emphasize that
this is the case even if the optimization problems (6) are solved exactly.
However, they are typically not solved exactly, as optimizing the
$J^{(i)}$ also becomes much more difficult in the nonlinear setting. Thus,
there are two sources of error to keep in mind:  

1. In the nonlinear setting, RML samples are approximate, even if the
optimization problems (6) are solved exactly.  
2. In the nonlinear setting, the solutions of the optimization problems (6) are
typically approximated by numerical algorithms. The objective functions
$J^{(i)}$ may have many local minima, and finding a global minimum is often
infeasible. 

# Appendix
{% katexmm %}

## Proof of (7) and (8), exactness in linear setting
Under the assumption of a linear forward model, the RML algorithm reduces to
solving a set of least squares problems
$$
u^{(i)}_{\star} = \argmin_{u \in \R^d} \frac{1}{2} \lVert y^{(i)} - Gu\rVert^2_{\Sigma} +
\frac{1}{2}\lVert u-u^{(i)} \rVert^2_{C}. \tag{A1}
$$
The solution to (A1) is given by
$$
u^{(i)}_{\star} = u^{(i)} + CG^\top \left(GCG^\top + \Sigma \right)^{-1}(y^{(i)} - Gu^{(i)}). \tag{A2}
$$
See this (post)[https://arob5.github.io/blog/2024/07/03/lin-Gauss/] for a
derivation of this fact. Since $u^{(i)}_{\star}$ is a linear function of
the independent Gaussian random variables $u^{(i)}$ and $y^{(i)}$ then it is
itself Gaussian. It thus remains to verify that the first two moments of
$u^{(i)}_{\star}$ agree with those of the true posterior (again, see the linked
post for the true moments). The mean is given by
\begin{align}
\mathbb{E}[u^{(i)}_{\star}]
&= \mathbb{E}[u^{(i)}] + CG^\top \left(GCG^\top + \Sigma \right)^{-1}\left(\mathbb{E}[y^{(i)}] - \mathbb{E}[u^{(i)}]\right) \newline
&= m + CG^\top \left(GCG^\top + \Sigma \right)^{-1}(y - Gm). \tag{A3}
\end{align}
For the covariance, we denote $K := CG^\top \left(GCG^\top + \Sigma \right)^{-1}$,
so that
$$
u^{(i)}_{\star} = Ky^{(i)} + (I-KG)u^{(i)}. \tag{A4}
$$
Noting that $u^{(i)}$ and $y^{(i)}$ are independent, we have
\begin{align}
\text{Cov}[u^{(i)}_{\star}]
&= K\text{Cov}[y^{(i)}]K^\top + (I-KG)\text{Cov}[u^{(i)}] (I-KG)^\top \newline
&= K \Sigma K^\top + (I-KG)C(I-KG)^\top \newline
&= C + K[GCG^\top + \Sigma]K^\top - CG^\top K^\top - KGC \newline
&= C + CG^\top \left(GCG^\top + \Sigma \right)^{-1}[GCG^\top + \Sigma]K^\top -
CG^\top K^\top - KGC \newline \newline
&= C + CG^\top K^\top - CG^\top K^\top - KGC \newline
&= C - KGC \newline
&= C - CG^\top \left(GCG^\top + \Sigma \right)^{-1} GC. \tag{A5}
\end{align}
We recognize (A4) and (A5) as the posterior moments for the linear Gaussian
inverse problem, and thus the proof is complete. $\qquad \blacksquare$
{% endkatexmm %}


# References
1. Data assimilation fundamentals
2. Randomized maximum likelihood based posterior sampling (Ba et al)
3. Randomized maximum likelihood via high-dimensional Bayesian optimization (Breaz and Wilkinson)
4. Metropolized Randomized Maximum Likelihood for sampling from multimodal distributions (Oliver)
