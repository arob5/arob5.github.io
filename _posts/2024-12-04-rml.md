---
title: Randomized Maximum Likelihood Sampling
subtitle: Approximate posterior inference and data assimilation applications.
layout: default
date: 2024-12-04
keywords: Bayes, Filtering, Data-Assim
published: true
---

# Setup
{% katexmm %}
In this post, we will consider an approximate posterior inference scheme
tailored to Bayesian models of the form
\begin{align}
y|u &\sim \mathcal{N}(\mathcal{G}(u), \Sigma) \tag{1} \newline
u &\sim \mathcal{N}(m, C),
\end{align}
where $y \in \R^n$ and $u \in \R^d$.
The statistical model here is very simple, consisting of a Gaussian likelihood
and prior. The complexity is baked into the *forward model* $\mathcal{G}$,
which may be nonlinear and computationally demanding to evaluate. The
posterior distribution of (1) is given by
$$
\pi(u) := p(u|y) \propto \exp\left[-J(u)\right] \tag{2}
$$
where $J(u)$ is a nonlinear least squares *cost function* given by
$$
J(u) = \frac{1}{2} \lVert y - \fwd(u)\rVert^2_{\Sigma} + \frac{1}{2}\lVert u-m \rVert^2_{C}. \tag{3}
$$
In (3) we have used the following notation for inner products and norms
weighted by a positive definite matrix $C$:
\begin{align}
\langle u, u^\prime \rangle_C &:= \langle C^{-1}u, u^\prime\rangle = u^\top C^{-1}u^\prime \newline
\lVert u \rVert_C^2 &:= \langle u, u\rangle_C.
\end{align}
Standard methods for optimization or sampling require evaluating the cost
function $J(u)$ sequentially many times, which may be infeasible when the
cost of computing $\fwd(u)$ is very large. In this post, we consider a
method for posterior inference that allows for cost function evaluations
to be computed in parallel. The tradeoff is that the method is an approximation
except when $\fwd$ is linear, and the properties of the approximation are
poorly understood.

# The Basic Algorithm
The basic RML algorithm proceeds by randomly generating an ensemble of
cost functions $\{J^{(i)}\}$, $i = 1, \dots, m$ and then optimizing each
one independently. The ensemble consisting of the optimizers for each of the
$J^{(i)}$ are then interpreted as approximate samples from the posterior
distribution. This procedure is summarized below.
<blockquote>
  <p><strong>Randomized Maximum Likelihood Sampling.</strong> <br>
  Construct an ensemble of $m$ cost functions $\{J^{(i)}\}$, where
  $J^{(i)}$ is defined by sampling
  \begin{align}
  &u^{(i)} \sim \mathcal{N}(m, C), &&y^{(i)} \sim \mathcal{N}(y, \Sigma) \tag{4}
  \end{align}
  and then defining $J^{(i)}: \R^d \to [0,\infty)$ by
  $$
  J^{(i)}(u) := \frac{1}{2} \lVert y^{(i)} - \fwd(u)\rVert^2_{\Sigma} +
  \frac{1}{2}\lVert u-u^{(i)} \rVert^2_{C}. \tag{5}
  $$
  The RML algorithm outputs the ensemble $\{u^{(i)}_{\star}\}$, where
  $$
  u^{(i)}_{\star} := \argmin_{u \in \R^d} J^{(i)}(u). \tag{6}
  $$
  </p>
</blockquote>

Note that the RML method requires solving $m$ nonlinear least square problems
(6), but they are decoupled and hence can be solved in parallel. The cost
functions $J^{(i)}$ are defined by sampling $(u^{(i)}, y^{(i)})$ and then
altering $J(u)$ by replacing $m$ with $u^{(i)}$ and $y$ with $y^{(i)}$. The
method of sampling $(u^{(i)}, y^{(i)})$ may seem a bit peculiar; the $u^{(i)}$
are independent draws from the prior, while $y^{(i)}$ is simply the observed
data $y$ that has been perturbed by the addition of a sampled noise
realization $\epsilon^{(i)} \sim \Gaussian(0, \Sigma)$. Sometimes $y^{(i)}$
is referred to as "simulated data". The following section provides some
justification for replacing $y$ with the simulated data; namely, the RML
produces exact samples from the posterior when $\fwd$ is linear.
{% endkatexmm %}

# Linear Forward Model
<blockquote>
  <p><strong>Exactness in linear setting.</strong> <br>
  Suppose that the forward model in (1) is linear; i.e., $\fwd(u) = Gu$, for
  some matrix $G \in \R^{n \times d}$. Then the samples $\{u^{(i)}_{\star}\}$
  returned by the RML algorithm (6) satisfy
  $$
  u^{(i)}_{\star} \overset{iid}{\sim} \pi \tag{7}
  $$
</blockquote>

**Proof.**
Under the assumption of a linear forward model, the RML algorithm reduces to
solving a set of least squares problems
$$
u^{(i)}_{\star} = \argmin_{u \in \R^d} \frac{1}{2} \lVert y^{(i)} - Gu\rVert^2_{\Sigma} +
\frac{1}{2}\lVert u-u^{(i)} \rVert^2_{C}. \tag{8}
$$
The solution to (8) is given by
$$
u^{(i)}_{\star} = u^{(i)} + CG^\top \left(GCG^\top + \Sigma \right)^{-1}(y^{(i)} - u^{(i)}). \tag{9}
$$
See this (post)[https://arob5.github.io/blog/2024/07/03/lin-Gauss/] for a derivation
of this fact.



# References
1. Data assimilation fundamentals
2. Randomized maximum likelihood based posterior sampling (Ba et al)
3. Randomized maximum likelihood via high-dimensional Bayesian optimization (Breaz and Wilkinson)
4. Metropolized Randomized Maximum Likelihood for sampling from multimodal distributions (Oliver)
