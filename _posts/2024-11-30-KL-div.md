---
title: The Kullback-Leibler Divergence
subtitle:
layout: default
date: 2024-11-30
keywords: probability, statistics
published: false
---

# Plan:
- KL as special case of F-Divergence (See DA and Inverse Problems ML Perspective)
- Information-theoretic perspective
- Basic properties
- Connection to MLE: see e.g. https://jaketae.github.io/study/kl-mle/ MLE is
with respect to fixed data realization; KL averages over data distribution.
- Forward vs. Backward KL
- Use as objective functions: why one KL direction is more suitable when you
have unnormalized density and one is more suitable when you have samples.
