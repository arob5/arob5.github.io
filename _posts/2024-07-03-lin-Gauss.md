---
title: Linear Gaussian Inverse Problems
subtitle: Derivations and discussion of linear Gaussian inverse problems.
layout: default
date: 2024-07-03
keywords: statistics
published: true
---

This post focuses on Bayesian inverse problems with the following features:
- Linear forward model.
- Additive Gaussian observation noise.
- Gaussian prior distribution.
- Prior independence of the observation noise and prior.  
We refer to such inverse problems as *linear Gaussian*. The typical Bayesian
linear regression model with a Gaussian prior on the coefficients constitutes
a common example of a linear Gaussian inverse problem. The assumptions of
linearity and Gaussianity play quite nicely together, resulting in a closed-form
Gaussian posterior distribution. Moreover, many extensions to nonlinear and/or
non-Gaussian settings rely on methods rooted in our understanding of the linear
Gaussian regime.

# Setup
We consider the following linear Gaussian regression model
\begin{align}
y &= Gu + \epsilon \tag{1} \newline
\epsilon &\sim \mathcal{N}(0, \Sigma) \newline
u &\sim \mathcal{N}(m, C), && u \perp \epsilon
\end{align}
consisting of the *observation* (or *data*) $y \in \mathbb{R}^n$, *parameter* $u \in \mathbb{R}^d$,
*noise* $\epsilon \in \mathbb{R}^n$, and linear *forward model* represented by the
matrix $G \in \mathbb{R}^{n \times d}$. The *observation covariance*
$\Sigma \in \mathbb{R}^{n \times n}$ and *prior covariance* $u \in \mathbb{R}^{d \times d}$
are both fixed positive definite matrices. The vector $m \in \mathbb{R}^d$ is the
*prior mean*. We write $u \perp \epsilon$ to indicate the key assumption that
$u$ and $\epsilon$ are a priori statistically independent. The model (1) can
equivalently be written as
\begin{align}
y|u &\sim \mathcal{N}(Gu, \Sigma) \tag{2} \newline
u &\sim \mathcal{N}(m, C),
\end{align}
which gives the explicit expression for the Gaussian likelihood $p(y|u)$.
The solution of the Bayesian inverse problem is the posterior distribution
$p(u|y)$. We provide two approaches to calculating this distribution below, which
yield different (but equivalent) expressions.

# Computing the Posterior.
## Method 1: Completing the Square
We first tackle the problem directly, using Bayes' theorem and
the matrix analog of completing the square from elementary algebra. Applying
Bayes' theorem to (2) yields
\begin{align}
p(u|y)
&\propto p(y|u)p(u) \newline
&\propto \exp\left(-\frac{1}{2}\left[(y-Gu)^\top \Sigma^{-1} (y-Gu) + (u-m)^\top C^{-1} (u-m) \right] \right) \newline
&\propto \exp\left(-\frac{1}{2}\left[u^\top(G^\top \Sigma^{-1}G + C^{-1})u -
2u^\top(\Sigma^{-1}y + C^{-1}m)\right] \right). \tag{3}
\end{align}
All we have done above is to combine the Gaussian likelihood and prior, dropping
any multiplicative constants that don't depend on $u$, and grouping like terms
in $u$. Note that since (3) is an exponential of a quadratic in $u$, then we
immediately know that the posterior must be Gaussian. It therefore remains to find
the mean $\overline{m}$ and covariance $\overline{C}$. Knowing that (3) is
proportional to a Gaussian density, let's set the term in square brackets equal to  
\begin{align}
(u - \overline{m})^\top \overline{C}^{-1} (u - \overline{m})
= u^\top \overline{C}^{-1}u - 2u^\top \overline{C}^{-1} \overline{m} +
\overline{m}^\top \overline{C}^{-1} \overline{C}^{-1}
\end{align}
and equate like terms to solve for the unknowns $\overline{m}$ and $\overline{C}$.
Doing so, we find that
\begin{align}
\overline{C}^{-1} &= G^\top \Sigma^{-1} G + C^{-1} \newline
\overline{m}^\top \overline{C}^{-1} &= \Sigma^{-1}y + C^{-1}m.
\end{align}
Rearranging these expressions gives the desired mean and covariance equations,
which are summarized in the following result.

<blockquote>
  <p><strong>Proposition.</strong>
  The posterior distribution under the linear Gaussian model (1) is Gaussian
  $u|y \sim \mathcal{N}(\overline{m}, \overline{C})$, with
  \begin{align}
  \overline{m} &= \overline{C}^{-1}\left[\Sigma^{-1}y + C^{-1}m \right] \tag{4} \newline
  \overline{C} &= \left[G^\top \Sigma^{-1} G + C^{-1} \right]^{-1}.
  \end{align}
  </p>
</blockquote>


## Method 2: Joint Gaussian Conditioning
We now present a second method for computing $p(u|y)$. This approach relies on
the observation that the vector $(u, y)^\top \in \mathbb{R}^{d+n}$ has a
joint Gaussian distribution. This follows from the prior independence of $u$
and $\epsilon$, and is formally proved in the appendix. Writing out this joint
Gaussian explicitly gives
\begin{align}
\begin{bmatrix} u \newline y \end{bmatrix}
&\sim \mathcal{N}\left(
\begin{bmatrix} m \newline Gm \end{bmatrix},
\begin{bmatrix} C & CG^\top \newline GC & GCG^\top + \Sigma \end{bmatrix}
\right). \tag{5}
\end{align}
The mean and covariance of $u$ is immediate from (1),
and the remaining quantities are computed as:
\begin{align}
\mathbb{E}[y] &= \mathbb{E}[Gu + \epsilon]
= G\mathbb{E}[u] + \mathbb{E}[\epsilon] = Gm \tag{6} \newline
\text{Cov}[y] &= \text{Cov}[Gu + \epsilon]
= \text{Cov}[Gu] + \text{Cov}[\epsilon] = GCG^\top + \Sigma \tag{7} \newline
\text{Cov}[y, u] &= \text{Cov}[Gu + \epsilon, u]
= \text{Cov}[Gu, u] + \text{Cov}[\epsilon, u]
= GC. \tag{8}
\end{align}
In (6) we use the linearity of expectation and the fact that the noise is zero-mean.
The covariance splits into the sum in (7) due to the independence of $u$ and
$\epsilon$. This independence assumption is similarly leveraged in (8).

The conditional distributions of joint Gaussians are well-known to also be
Gaussian, and can be computed in closed-form. Applying these Gaussian conditioning
identities to (5) provides expressions for the posterior distribution $u|y$,
which is summarized in the following result.
<blockquote>
  <p><strong>Proposition.</strong>
  The posterior distribution under the linear Gaussian model (1) is Gaussian
  $u|y \sim \mathcal{N}(\overline{m}, \overline{C})$, with
  \begin{align}
  \overline{m} &= m + CG^\top [GCG^\top + \Sigma]^{-1}(y - Gm) \tag{9} \newline
  \overline{C} &= C - CG^\top [GCG^\top + \Sigma]^{-1} GC.
  \end{align}
  </p>
</blockquote>

## Equivalence of the Two Approaches
TODO

# Investigating the Posterior Equations

## The Posterior Covariance
A first important observation is that the the posterior covariance
$\overline{C}$ is independent of the data $y$. In this sense, the specific
data realization observed does not affect the uncertainty in the estimation
of $u$. The expression coming from the first derivation (4) tells us that the
posterior *precision* (inverse covariance) $\overline{C}^{-1}$ is the sum of
the prior precision $C^{-1}$ and $G^\top \Sigma^{-1}G$, which
is the observation precision $\Sigma^{-1}$ modified by the forward model. Since
the posterior covariance is the inverse of $G^\top \Sigma^{-1}G + C^{-1}$,
we should verify that this matrix is indeed invertible. First, note that
$\Sigma^{-1}$ and $C^{-1}$ are both positive definite, since the
inverse of positive definite matrices are also positive definite. Thus,
the factorization $\Sigma^{-1} = SS^\top$ exists, which implies
\begin{align}
x^\top [G^\top \Sigma^{-1}G]x
&= x^\top [G^\top SS^\top G]x
= \lVert S^\top Gx \rVert^2_2 \geq 0.
\end{align}
That is, $G^\top \Sigma^{-1}G$ is positive semidefinite. Since the sum of a
positive semidefinite and positive definite matrix is positive definite, then
$G^\top \Sigma^{-1}G + C^{-1}$ is positive definite, and thus invertible.

The covariance expression in (9) provides an alternative perspective.




- TODO: show that $GCG^\top$ is the Hessian of the log-likelihood.
- Variance doesn't depend on data/observation.
- Positive definiteness/invertibility.

# Posterior Predictive Distribution

# Computing the Posterior Equations




# Appendix

## Joint Gaussian Distribution
TODO
