---
title: Linear Gaussian Inverse Problems
subtitle: Derivations and discussion of linear Gaussian inverse problems.
layout: default
date: 2024-07-03
keywords: statistics
published: true
---

This post focuses on Bayesian inverse problems with the following features:
- Linear forward model.
- Additive Gaussian observation noise.
- Gaussian prior distribution.
- Prior independence of the observation noise and prior.  
We refer to such inverse problems as *linear Gaussian*. The typical Bayesian
linear regression model with a Gaussian prior on the coefficients constitutes
a common example of a linear Gaussian inverse problem. The assumptions of
linearity and Gaussianity play quite nicely together, resulting in a closed-form
Gaussian posterior distribution. Moreover, many extensions to nonlinear and/or
non-Gaussian settings rely on methods rooted in our understanding of the linear
Gaussian regime.

# Setup
We consider the following linear Gaussian regression model
\begin{align}
y &= Gu + \epsilon \tag{1} \newline
\epsilon &\sim \mathcal{N}(0, \Sigma) \newline
u &\sim \mathcal{N}(m, C), && u \perp \epsilon
\end{align}
consisting of the *observation* $y \in \mathbb{R}^n$, *parameter* $u \in \mathbb{R}^d$,
*noise* $\epsilon \in \mathbb{R}^n$, and linear *forward model* represented by the
matrix $G \in \mathbb{R}^{n \times d}$. The *observation covariance*
$\Sigma \in \mathbb{R}^{n \times n}$ and *prior covariance* $u \in \mathbb{R}^{d \times d}$
are both fixed positive definite matrices. The vector $m \in \mathbb{R}^d$ is the
*prior mean*. We write $u \perp \epsilon$ to indicate the key assumption that
$u$ and $\epsilon$ are a priori statistically independent. The model (1) can
equivalently be written as
\begin{align}
y|u &\sim \mathcal{N}(Gu, \Sigma) \tag{2} \newline
u &\sim \mathcal{N}(m, C),
\end{align}
which gives the explicit expression for the Gaussian likelihood $p(y|u)$.
The solution of the Bayesian inverse problem is the posterior distribution
$p(u|y)$. We provide two approaches to calculating this distribution below, which
yield different (but equivalent) expressions.

# Computing the Posterior.
## Method 1: Completing the Square

## Method 2: Joint Gaussian Conditioning
We now present a second method for computing $p(u|y)$. This approach relies on
the observation that the vector $(u, y)^\top \in \mathbb{R}^{d+n}$ has a
joint Gaussian distribution. This follows from the prior independence of $u$
and $\epsilon$, and is formally proved in the appendix. Writing out this joint
Gaussian explicitly gives
\begin{align}
\begin{bmatrix} u \newline y \end{bmatrix}
&\sim \mathcal{N}\left(
\begin{bmatrix} m \newline Gm \end{bmatrix},
\begin{bmatrix} C & CG^\top \newline GC & GCG^\top + \Sigma \end{bmatrix}
\right). \tag{3}
\end{align}
The mean and covariance of $u$ is immediate from (1),
and the remaining quantities are computed as:
\begin{align}
\mathbb{E}[y] &= \mathbb{E}[Gu + \epsilon]
= G\mathbb{E}[u] + \mathbb{E}[\epsilon] = Gm \tag{4} \newline
\text{Cov}[y] &= \text{Cov}[Gu + \epsilon]
= \text{Cov}[Gu] + \text{Cov}[\epsilon] = GCG^\top + \Sigma \tag{5} \newline
\text{Cov}[y, u] &= \text{Cov}[Gu + \epsilon, u]
= \text{Cov}[Gu, u] + \text{Cov}[\epsilon, u]
= GC. \tag{6}
\end{align}
In (4) we use the linearity of expectation and the fact that the noise is zero-mean.
The covariance splits into the sum in (5) due to the independence of $u$ and
$\epsilon$. This independence assumption is similarly leveraged in (6).

The conditional distributions of joint Gaussians are well-known to also be
Gaussian, and can be computed in closed-form. Applying these Gaussian conditioning
identities to (3) gives the posterior
\begin{align}
u|y &\sim \mathcal{N}\left(\overline{m}, \overline{C} \right), \tag{7}
\end{align}
where
\begin{align}
\overline{m} &= m + CG^\top [GCG^\top + \Sigma]^{-1}(y - Gm) \tag{8} \newline
\overline{C} &= C - CG^\top [GCG^\top + \Sigma]^{-1} GC.
\end{align}

# Investigating the Posterior Equations.
- TODO: show that $GCG^\top$ is the Hessian of the log-likelihood.
- Variance doesn't depend on data/observation.
- Positive definiteness/invertibility.

# Computing the Posterior Equations.


# Appendix

## Joint Gaussian Distribution
TODO
