---
title: Principal Components Analysis
subtitle: I derive the PCA decomposition from both a minimum reconstruction error and maximum variance perspective. I also discuss a statistical interpretation of PCA.
layout: default
date: 2023-12-15
keywords: PCA, Statistics
published: true
---

# Part 1: Formulating and Solving the PCA Optimization Problem

## Setup and Notation
{% katexmm %}
Suppose that we have data $\mathbf{x}_1, \dots, \mathbf{x}_N \in \mathbb{R}^D$, stacked into the
rows of a matrix $X \in \mathbb{R}^{N \times D}$. Our task is to find a subspace
of smaller dimension $R < D$ such that the projection of the data points onto the
subspace retains as much information as possible. By restricting our attention to
orthonormal bases for the low-dimensional subspace, we reduce the problem to finding
a set of orthonormal basis vectors
\begin{align}
&\mathbf{b}_1, \dots, \mathbf{b}_R \in \mathbb{R}^D,
&&\langle \mathbf{b}_r, \mathbf{b}_s \rangle = \delta\_{r,s}.
\end{align}
Define $B \in \mathbb{R}^{D \times R}$ to be the matrix with $r^{\text{th}}$ column
equal to $\mathbf{b}_r$. The subspace generated by the basis $B$ is given by
$$
\text{span}(B) := \text{span}(\mathbf{b}_1, \dots, \mathbf{b}_R).
$$
Throughout this post I will abuse notation by referring to the matrix $B$ when actually
talking about the set of vectors $\{\mathbf{b}_1, \dots, \mathbf{b}_R\}$. Since there
is no a priori reason to assume that the data is centered, we should also allow for
the subspace to be shifted by some intercept $\mathbf{w}_0 \in \mathbb{R}^D$,
resulting in the affine space
$$
\mathbf{w}_0 + \text{span}(B) = \left\{\mathbf{w}_0 +
\sum_{r=1}^{R} w_r \mathbf{b}_r : w_1, \dots, w_R \in \mathbb{R} \right\}.
$$
Loosely speaking, the task is to find the basis
$B$, intercept $\mathbf{w}_0$, and pointwise weights
$\mathbf{w}_1, \dots, \mathbf{w}_N \in \mathbb{R}^R$ such that
\begin{align}
\mathbf{x}_n &\approx \mathbf{w}_0 + \sum\_{r=1}^{R} (\mathbf{w}_n)_r \mathbf{b}_r &&\forall n=1,\dots,N \newline
&= \mathbf{w}_0 + B\mathbf{w}_n.
\end{align}
To formalize this notion, PCA measures the error in the above approximation using
Euclidean distance, averaged over the $N$ data points. To further simplify notation,
we stack the $\mathbf{w}_n$ in the columns of a matrix $W \in \mathbb{R}^{R \times N}$.
With all of this notation established, we can state that PCA solves the optimization
problem
$$
\text{argmin}_{B, W, \mathbf{w}_0} \sum_{n=1}^{N} \lVert \mathbf{x}_n - (\mathbf{w}_0 + B\mathbf{w}_n) \rVert_2^2, (1)
$$
where the basis $B$ is constrained to be orthonormal.
As we will see, this optimization naturally breaks down into two distinct problems
which can be solved sequentially:
1. Given the basis $B$ and intercept $\mathbf{w}_0$, find the optimal basis coefficients
$\mathbf{w}_n$ corresponding to each data point $\mathbf{x}_n$.
2. Find the optimal basis and intercept.

Part of the popularity of PCA stems from the fact that both problems can be solved in
closed-form. Let us consider both problems in turn.

## Optimizing the Basis Coefficients
Let us first consider $\mathbf{w}_0$ and $B$ to be fixed, meaning that we are fixing
an affine subspace of dimension $R$. We seek to find the optimal way to represent
the data $X$ in this lower-dimensional space. As we will show, the Euclidean objective
used by PCA implies that this problem reduces to straightforward orthogonal projection.
For now, let $\mathbf{x}^c_n := \mathbf{x}_n - \mathbf{w}_0$ denote the centered
data points (we will deal with the intercept shortly). We are thus considering
the problem
$$
\text{argmin}_{W} \sum_{n=1}^{N} \lVert \mathbf{x}^c_n - B\mathbf{w}_n \rVert_2^2 (2)
$$
Observe that $\mathbf{w}_n$ only appears in the $n^{\text{th}}$ term of the sum,
meaning that we can consider each summand independently,  
$$
\text{argmin}_{\mathbf{w}_n} \lVert \mathbf{x}^c_n - B\mathbf{w}_n \rVert_2^2.
$$
In words, we seek the linear combination of the basis vectors $B$ that results
in minimal Euclidean distance from $\mathbf{x}^c_n$; this is a standard orthogonal
projection problem from linear algebra. Since the basis vectors are orthonormal,
the optimal projection coefficients are given by
\begin{align}
&(\mathbf{w}_n)_r = \langle \mathbf{x}_n^c, \mathbf{b}_r \rangle,
&&\mathbf{w}_n = B^\top \mathbf{x}_n^c
\end{align}
which can be written succinctly for all data points by stacking the $\mathbf{w}_n^\top$
as rows in a matrix $W^\top$; i.e.,
$$
W^\top = X^c B,
$$
with $X^c$ denoting the centered data matrix with rows set to the
$(\mathbf{x}^c_n)^\top$.
{% endkatexmm %}

## Optimizing the Basis
In the previous section, we saw that for a fixed basis and intercept, optimizing
the basis weights reduced to an orthogonal projection problem. In this section
we show that for fixed weights, optimizing the basis reduces to solving a sequence
of eigenvalue problems. To be clear, we are now considering the problem
{% katexmm %}
$$
\text{argmin}_{B} \sum_{n=1}^{N} \lVert \mathbf{x}_n - B\mathbf{w}_n \rVert_2^2, \tag{3}
$$
where we are additionally treating $\mathbf{w}_0$ as fixed for the time being.
Thus, I should really be writing $\mathbf{x}_n^c$ in place of $\mathbf{x}_n$ in
(3), but for succinctness I will omit the superscript.

This problem is also referred to as minimizing the *reconstruction error*, since
$\lVert \mathbf{x}_n - \mathbf{\hat{x}}_n \rVert_2 := \lVert \mathbf{x}_n - B\mathbf{w}_n \rVert_2$
is the error between the original
data point $\mathbf{x}_n$ and the $D$-dimensional vector $\mathbf{\hat{x}}_n$ which
can be thought of as an approximation to $\mathbf{x}_n$ that has been
*reconstructed* from its lower-dimensional representation $\mathbf{w}_n$. The key
here is to re-write this objective function so that this optimization problem
takes the form of an eigenvalue problem, which is something that we already know
how to solve (see the appendix, A1).

To start, we extend the orthonormal set $\mathbf{b}_1, \dots, \mathbf{b}_R$
to an orthonormal basis $\mathbf{b}_1, \dots, \mathbf{b}_D$ for $\mathbb{R}^D$.
Now we can write the original data point $\mathbf{x}_n$ and its approximation
$\mathbf{\hat{x}_n}$ with respect to this basis as
\begin{align}
&\mathbf{x}\_n = \sum_{r=1}^{D} \langle \mathbf{x}_n, \mathbf{b}_r \rangle \mathbf{b}_r,
&&\mathbf{\hat{x}}\_n = \sum\_{r=1}^{R} \langle \mathbf{x}_n, \mathbf{b}_r \rangle \mathbf{b}_r
\end{align}

and hence the residual $\mathbf{x}_n - \mathbf{\hat{x}}_n$ is given by
\begin{align}
\mathbf{x}\_n - \mathbf{\hat{x}}\_n &= \sum_{r=R+1}^{D} \langle \mathbf{x}_n, \mathbf{b}_r \rangle \mathbf{b}_r.
\end{align}

Thus, the objective function in (3) can be written as
\begin{align}
\sum_{n=1}^{N} \lVert \mathbf{x}_n - \mathbf{\hat{x}}_n \rVert_2^2
&= \sum\_{n=1}^{N} \bigg\lVert \sum\_{r=R+1}^{D} \langle \mathbf{x}_n, \mathbf{b}_r \rangle \mathbf{b}_r \bigg\rVert_2^2
= \sum\_{n=1}^{N} \sum\_{r=R+1}^{D} \langle \mathbf{x}_n, \mathbf{b}_r \rangle^2 \lVert \mathbf{b}_r \rVert_2^2
= \sum\_{n=1}^{N} \sum\_{r=R+1}^{D} \langle \mathbf{x}_n, \mathbf{b}_r \rangle^2,
\end{align}

where the second and third equalities use the facts that the $\mathbf{b}_r$ are
orthogonal and unit norm, respectively.

We could continue working with this formulation, but at this point it is convenient to
re-write the minimization problem we have been working with as an equivalent maximization
problem. Note that the above residual calculation is of the form
$\mathbf{\hat{e}}_n = \mathbf{x}_n - \mathbf{\hat{x}}_n$ (and summed over $n$).
Since $\mathbf{x}_n$ is fixed, then minimizing the residual (i.e., the
reconstruction error) is equivalent to maximizing $\mathbf{\hat{x}}_n$. More
rigorously, we have

\begin{align}
\text{argmin}_B \sum\_{n=1}^{N} \sum\_{r=R+1}^{D} \langle \mathbf{x}_n, \mathbf{b}_r \rangle^2
&= \text{argmin}_B \sum\_{n=1}^{N}
\left(\sum\_{r=1}^{D} \langle \mathbf{x}_n, \mathbf{b}_r \rangle^2 - \sum\_{r=1}^{R} \langle \mathbf{x}_n, \mathbf{b}_r \rangle^2\right) \newline
&= \text{argmin}_B \sum\_{n=1}^{N}
\left(\lVert \mathbf{x}_n \rVert_2^2 - \sum\_{r=1}^{R} \langle \mathbf{x}_n, \mathbf{b}_r \rangle^2\right) \newline
&= \text{argmax}_B \sum\_{n=1}^{N} \sum\_{r=1}^{R} \langle \mathbf{x}_n, \mathbf{b}_r \rangle^2. \tag{4}
\end{align}

We can now re-write the squared inner product to obtain
\begin{align}
\sum\_{n=1}^{N} \sum\_{r=1}^{D} \langle \mathbf{x}_n, \mathbf{b}_r \rangle^2
&= \sum\_{n=1}^{N} \sum\_{r=1}^{D} \mathbf{b}_r^\top \mathbf{x}_n \mathbf{x}_n^\top \mathbf{b}_r
= \sum\_{r=1}^{D} \mathbf{b}_r^\top \left(\sum\_{n=1}^{N}\mathbf{x}_n \mathbf{x}_n^\top\right) \mathbf{b}_r
= \sum\_{r=1}^{D} \mathbf{b}_r^\top (X^\top X) \mathbf{b}_r^\top,
\end{align}

where the final step uses [this](https://gregorygundersen.com/blog/2020/07/17/matmul/)
fact. We have managed to re-write (3) as
$$
\text{argmax}_{B} \sum_{r=1}^{D} \mathbf{b}_r^\top (X^\top X) \mathbf{b}_r^\top, \tag{5}
$$
where we recall that this is also subject to the constraint that $B$ is orthogonal.

Before proceeding, we note that $X^\top X$ is a positive semi-definite matrix,
whose eigenvalues we denote $\lambda_1, \dots, \lambda_D$, sorted in decreasing
order. Note that the eigenvalues are all non-negative due to the positive
definiteness. Let $\mathbf{e}_1, \dots, \mathbf{e}_D$ denote the respective
eigenvectors, normalized to have unit norm. These vectors are guaranteed to be
orthogonal by the Spectral Theorem.

We now notice in (5) that the objective function has been decomposed into $R$ different
terms, each of which only depends on a single $\mathbf{b}_r$. However, these
do not constitute $R$ independent optimization problems, as they are all coupled
through the orthogonality constraint. We will thus consider solving them in
a recursive fashion, beginning with the first term,
$$
\text{argmax}_{\lVert \mathbf{b}_1 \rVert_2=1} \mathbf{b}_1^\top (X^\top X) \mathbf{b}_1^\top
= \text{argmax}_{\lVert \mathbf{b}_1 \rVert_2=1} \lVert X \mathbf{b}_1 \rVert_2^2.
$$
This is an eigenvalue problem! It is precisely of the form (A4) (see appendix)
and so we apply that result to conclude that the optimal argument is
$\mathbf{b}_1 = \mathbf{e}_1$ with associated optimal value $\lambda_1$ (note
the objective here is the *squared* norm, in contrast to the appendix). Taking
this as the base case, we now proceed inductively. Assume that at the
$r^{\text{th}}$ problem in the sequence, the solution is given by
$(\mathbf{b}_1, \dots, \mathbf{b}_r) = (\mathbf{e}_1, \dots, \mathbf{e}_r)$.
We must show the solution to the $(r+1)^{\text{st}}$ problem is
$\mathbf{e}_{r+1}$. Under the inductive hypothesis, this problem is constrained
so that $\mathbf{b}_{r+1}$ is orthogonal to each of $\mathbf{e}_1, \dots, \mathbf{e}_r$;
i.e., we require $\mathbf{b}_{r+1} \perp \text{span}(\mathbf{e}_1, \dots, \mathbf{e}_r)$.
If we denote $\mathcal{E}_{r} := \text{span}(\mathbf{e}_1, \dots, \mathbf{e}_r)$
and $\mathcal{E}^{\perp}_{r}$ the orthogonal complement of $\mathcal{E}_{r}$,
then a succinct way to write the orthogonality constraint is that $\mathbf{b}_{r+1} \in \mathcal{E}^{\perp}_r$.
The problem can thus be written as
\begin{align}
\text{argmax}\_{\mathbf{b}\_{r+1} \in \mathcal{E}^{\perp}\_{r}, \lVert \mathbf{b}\_{r+1} \rVert_2=1} \lVert X \mathbf{b}\_{r+1} \rVert_2^2,
\end{align}
which is another eigenvalue problem, precisely of the form (A3). Using this
result from the appendix, we conclude that this is solved by
$\mathbf{b}_{r+1} = \mathbf{e}_{r+1}$, with the maximal objective value $\lambda_{r+1}$.
{% endkatexmm %}

## An Alternative Approach: The Eckart-Young Theorem
Armed with the Eckart-Young Theorem, we can derive the same result with much less
work. The key is to re-write the reconstruction error as a a matrix approximation
problem using the Frobenius norm. We have

{% katexmm %}
$$
\sum_{n=1}^{N} \lVert \mathbf{x}_n - B\mathbf{w}_n \rVert_2^2
= \lVert X - WB^\top \rVert_2^2,
$$
where the function $\lVert \cdot \rVert_2$ denotes the Frobenius norm when
passed a matrix. The PCA optimization problem can then be written as the
matrix approximation problem
$$
\text{argmin}_{B, W} \lVert X - WB^\top \rVert_2^2,
$$
where $B$ is constrained to be an orthogonal matrix. This is precisely the problem
considered by the Eckart-Young theorem.
{% endkatexmm %}

# Part 2: Interpreting PCA
1. Maximum variance or minimum reconstruction error  
2. Statistical Interpretation
3. View as regression problem where you simultaneously optimize the design matrix


# Part 3: Computing PCA
1. Eigendecomposition
2. SVD


# Part 4: Using PCA
1. Decorrelating
2. Dimensionality reduction

# Part 5: Application and Code


# Appendix

## Eigenvalue Problems
{% katexmm %}
In this section, I briefly discuss the spectral norm and eigenvalue problems in
finite-dimensional vector spaces, which I utilize above when optimizing the basis
$B$ in the PCA derivation. Consider a matrix $A \in \mathbb{R}^{N \times D}$,
which represents a linear transformation from $\mathbb{R}^{D}$ to $\mathbb{R}^N$.
We define the
**spectral norm** of $A$ as the largest factor by which the map
$A$ can "stretch" a vector $\mathbf{u} \in \mathbb{R}^{D}$,
$$
\lVert A \rVert_2 := \max_{\lVert \mathbf{u} \rVert_2 \neq \boldsymbol{0}}
\frac{\lVert A\mathbf{u} \rVert}{\lVert \mathbf{u} \rVert}.
$$
Using the linearity of $A$, one can show that we need only consider vectors of
unit length; that is,
$$
\lVert A \rVert_2 = \max_{\lVert \mathbf{u}_2 \rVert=1} \lVert A\mathbf{u} \rVert_2. \tag{A1}
$$
Optimization problems of this type are called **eigenvalue problems** for reasons that will
shortly become clear. For the purposes of the PCA derivation, we will require
consideration of a slightly more general eigenvalue problem. To define this problem, first
note that the matrix $A^\top A$ is symmetric, positive semi-definite since
\begin{align}
&(A^\top A)^\top = A^\top (A^\top)^\top = A^\top A,
&\mathbf{u}^\top (A^\top A)\mathbf{u} = \lVert A \mathbf{u} \rVert_2^2 \geq 0.
\end{align}
Thus, by the spectral theorem $A^\top A$ has $D$ orthogonal eigenvectors, which
we will denote by $\mathbf{e}_1, \dots, \mathbf{e}_D$ and assume that they
have been normalized to have unit norm. By positive definiteness the respective
eigenvalues $\lambda_1, \dots, \lambda_D$ (sorted in decreasing order) are
all non-negative. For $d = 1, \dots, D$, let
$\mathcal{E}_d := \text{span}(\mathbf{e}_1, \dots, \mathbf{e}_d)$, with
$\mathcal{E}^{\perp}_d := \text{span}(\mathbf{e}_{d+1}, \dots, \mathbf{e}_D)$
its orthogonal complement. We now consider the eigenvalue problem

\begin{align}
\max_{\mathbf{u} \in \mathcal{E}^{\perp}_d, \lVert \mathbf{u} \rVert_2=1} \lVert A\mathbf{u} \rVert_2. \tag{A2}
\end{align}

We have generalized (A1) by adding an orthogonality constraint. Taking as a
convention $\mathcal{E}_0 := \{\mathbf{0}\}$ we then have
$\mathcal{E}^{\perp}_0 = \mathbb{R}^D$, which means that setting $d = 0$ in
(A2) recovers the original problem (A1) as a special case. We will prove the
following result.  

**Proposition.** Let $A \in \mathbb{R}^{N \times D}$ be a matrix. Let
$\mathbf{e}_1, \dots, \mathbf{e}_D$ denote the orthonormal
eigenbasis of $A^\top A$, with respective eigenvalues
$\lambda_1, \dots, \lambda_D$ sorted in descending
order by magnitude. For $d = 1, \dots, D$ define
$\mathcal{E}_d := \text{span}(\mathbf{e}_1, \dots, \mathbf{e}_d)$, with
$\mathcal{E}^{\perp}_d := \text{span}(\mathbf{e}_{d+1}, \dots, \mathbf{e}_D)$
its orthogonal complement. Then

\begin{align}
\mathbf{e}\_{d+1} = \text{argmax}_{\mathbf{u} \in \mathcal{E}^{\perp}_d, \mathbf{u}=1} \lVert A\mathbf{u} \rVert_2. \tag{A3}
\end{align}

with the maximal value  equal to $\sqrt{\lambda_{d+1}}$. In particular, we have

\begin{align}
\mathbf{e}\_1 = \text{argmax}_{\lVert \mathbf{u} \rVert_2=1} \lVert A\mathbf{u} \rVert_2, \tag{A4}
\end{align}

with maximal value $\sqrt{\lambda_1}$.

**Proof.**
Let $\mathbf{u} \in \mathcal{E}^{\perp}_d$
be an arbitrary vector of unit length. This vector may be represented with
respect to the eigenbasis as
$$
\mathbf{u} = \sum_{r=d+1}^{D} u_r \mathbf{e}_r, \qquad u_r := \langle \mathbf{u}, \mathbf{e}_r \rangle
$$
We will use this representation to show that
1. $\lVert A\mathbf{u} \rVert_2$ is upper bounded by $\sqrt{\lambda_{d+1}}$.
2. The upper bound is achieved by some $\mathbf{u} \in \mathcal{E}^{\perp}_d$,

which together imply the claimed result. We will actually work with the squared
norm instead, which allows us to leverage the inner product. We have
\begin{align}
\lVert A\mathbf{u} \rVert^2_2
= \langle A\mathbf{u}, A\mathbf{u} \rangle
= \langle A^\top A\mathbf{u}, \mathbf{u} \rangle
&= \left\langle A^\top A \sum\_{r=d+1}^{D} u_r \mathbf{e}_r,
\sum\_{r=d+1}^{D} u_r \mathbf{e}_r \right\rangle \newline
&= \left\langle \sum\_{r=d+1}^{D} u_r (A^\top A \mathbf{e}_r),
\sum\_{r=d+1}^{D} u_r \mathbf{e}_r \right\rangle \newline
&= \left\langle \sum\_{r=d+1}^{D} u_r \lambda_r \mathbf{e}_r,
\sum\_{r=d+1}^{D} u_r \mathbf{e}_r \right\rangle,
\end{align}
having used the fact the $\mathbf{e}_r$ are eigenvectors of $A^\top A$.
Now we can take advantage of the fact that the $\mathbf{e}_r$ are orthonormal
to obtain
\begin{align}
\left\langle \sum\_{r=d+1}^{D} u_r \lambda_r \mathbf{e}_r,
\sum\_{r=d+1}^{D} u_r \mathbf{e}_r \right\rangle
= \sum\_{r=d+1}^{D} u_r^2 \lambda_r \lVert \mathbf{e}_r \rVert^2_2
= \sum\_{r=d+1}^{D} u_r^2 \lambda_r
\leq \sum\_{r=d+1}^{D} u_r^2 \lambda\_{d+1}
= \lambda\_{d+1} \lVert \mathbf{u} \rVert_2^2
= \lambda\_{d+1}
\end{align}
where the inequality follows from the fact that the eigenvalues are sorted
in descending order. This verifies the upper bound
$\lVert A\mathbf{u} \rVert_2 \leq \sqrt{\lambda_{d+1}}$. To show that the bound
is achieved, we consider setting $\mathbf{u} = \mathbf{e}_{d+1}$. Then,

\begin{align}
\lVert A\mathbf{e}\_{d+1} \rVert^2_2
= \langle A\mathbf{e}\_{d+1}, A\mathbf{e}\_{d+1} \rangle
= \langle A^\top A\mathbf{e}\_{d+1}, \mathbf{e}\_{d+1} \rangle
= \langle \lambda\_{d+1} \mathbf{e}\_{d+1}, \mathbf{e}\_{d+1} \rangle
= \lambda\_{d+1} \lVert \mathbf{e}\_{d+1} \rVert^2_2
= \lambda\_{d+1},
\end{align}

so we have indeed verified that the equality
$\lVert A\mathbf{u} \rVert_2 = \sqrt{\lambda_{d+1}}$ is achieved for
some unit-norm vector $\mathbf{u} \in \mathcal{E}^{\perp}_{d}$.
The claim is thus proved. $\qquad \blacksquare$

















{% endkatexmm %}
