---
title: An Introduction to Scattered Data Approximation
subtitle: I summarize the first chapter of Holger Wendland's book "Scattered Data Approximation", which I augment with some background on polynomial interpolation and splines.
layout: default
date: 2024-03-24
keywords: Numerical-Analysis, Kernel-Methods
published: true
---

This post provides a summary of the first chapter of Holger Wendland's
excellent book on
[scattered data approximation ](https://www.cambridge.org/core/books/scattered-data-approximation/980EEC9DBC4CAA711D089187818135E3). The introductory chapter nominally provides motivation
and applications for the subsequent theory, but is somewhat of a whirlwind in
itself if you lack significant background in approximation theory. I therefore
try to augment the material in the chapter with some additional content,
which is especially suited for readers coming from more of a statistical
background. The notation I use more or less follows that of Wendland.

# The General Problem
{% katexmm %}
The theory of scattered data interpolation addresses a very general problem,
encompassing both problems of interpolation and smoothing (i.e., regression).
The generic setup considers a dataset $\{(x_j, f_j)\}_{j=1}^{N}$ consisting of
$N$ **sites** (also called **locations**, **knots**, or **design points** in
different settings) $X := \{x_j\}_{j=1}^{N}$ and
corresponding values $f_j$ at each respective site $x_j$. In typical applications,
these values are assumed to be generated by some underlying function
$$
f_j = f(x_j),
$$
perhaps subject to noise
$$
f_j \approx f(x_j),
$$
but this need not be the case. The noiseless and noisy settings naturally
motivate the two primary problems in this domain: **interpolation** and
**approximation**. I will also refer to the latter as a problem of *smoothing*
or *regression*, depending on the context. In the two cases, we seek a function
$s(\cdot)$ that either interpolates
\begin{align}
f_j &= s(x_j), && j = 1, \dots, N
\end{align}
or approximates
\begin{align}
f_j &\approx s(x_j), && j = 1, \dots, N
\end{align}
the data. The name "scattered" comes from the lack of assumptions on the sites
$X$; we will generally treat them generically, without assuming any sort of special
structure. On the other hand, we will require assumptions on the generating
process for the $f_j$, which typically means assuming that the underlying function
$f(x)$ satisfies some notion of smoothness. We will thus consider
the approximation properties of different spaces of smooth functions $S$, with
the goal being to identify the function $s \in S$ which is optimal in some sense.

In order to fully specify a concrete problem, we need to define in what space
the $x_j$ live. Statisticians would probably jump right to $\mathbb{R}^d$, but
in approximation theory there are some fundamental differences between one and
multiple dimensions. Therefore, in the next section we start by discussing the
concrete case of a one-dimensional input space, which will be used to motivate
higher-dimensional generalizations later on.  
{% endkatexmm %}

# The One-Dimensional Case
{% katexmm %}
In this section we will assume that $x_j \in \mathbb{R}$; in particular,
that $x_j \in (a, b)$ for all $j = 1, \dots, N$ for some $a < b$. It will
sometimes be useful to extend the notation as $x_0 := a$ and $x_{N+1} := b$,
in which case we will sometimes refer to $X = \{x_j\}_{j=1}^{N}$ as the
**interior sites** to distinguish from the end points. Note that there are
different conventions for this in the literature, but it is important to remember
that in our case $N$ denotes the number of sites with corresponding output
values $f_j$. The goal in this setting is to find a continuous function
$s: [a,b] \to \mathbb{R}$ which either interpolates or approximates the data.

## Polynomials
Following Wendland, let $\pi_M(\mathbb{R})$ denote the space of polynomials with
dimension at most $M$; i.e., the functions $s: \mathbb{R} \to \mathbb{R}$ which
admit a representation
\begin{align}
s(x) = \alpha_0 + \alpha_1 x + \alpha_2 x^2 + \cdots + \alpha_M x^M
\end{align}
for some coefficients $\alpha_0, \dots, \alpha_M \in \mathbb{R}$. $\pi_M(\mathbb{R})$ is
indeed a *space* in the linear algebra sense; it is a vector space of dimension
$M+1$. The most obvious basis for this space is the monomial basis
$1, x, x^2, \dots, x^M$ but many other bases are commonly used which have
nicer theoretical and computational properties.

## Interpolation with Polynomials
Let's now consider solving the interpolation problem using polynomials; that is,
we will try to find a function $s \in \pi_{M-1}(\mathbb{R})$ which interpolates the
data. I'm considering $M-1$ and not $M$ since the space $\pi_{M-1}(\mathbb{R})$
has dimension $M$, which will be more convenient notationally than having to write
$M+1$ all the time. With this choice of function space, the interpolation requirement is that
\begin{align}
f_j &= \alpha_0 + \alpha_1 x_j + \alpha_2 x_j^2 + \cdots \alpha_M x_j^{M-1}, &&\forall j = 1, \dots, N.
\end{align}  
It is convenient to define
\begin{align}
\varphi(x) &:= (1, x, x^2, \dots, x^{M-1})^\top \in \mathbb{R}^{M} \newline
\alpha &:= (\alpha_0, \alpha_1, \alpha_2, \dots, \alpha_{M-1})^\top \in \mathbb{R}^{M}
\end{align}
so that we can re-write the above as
\begin{align}
f_j &= \alpha_0 + \alpha_1 x_j + \alpha_2 x_j^2 + \cdots \alpha_M x_j^{M-1} = \varphi(x_j)^\top \alpha,
&&\forall j = 1, \dots, N. \tag{1}
\end{align}  
To succinctly write the equalities for all $j$, let $\Phi \in \mathbb{R}^{N \times M}$
be the matrix with $j^{\text{th}}$ row equal to $\varphi(x_j)^\top$. The requirement
(1) is then encoded in the linear system
\begin{align}
y = \Phi \alpha, \tag{2}
\end{align}
where we have additionally defined $y := (f_1, \dots, f_N)^\top \in \mathbb{R}^N$.
The questions of existence and uniqueness can now be answered by appealing to known
properties of linear systems, though it is not in general obvious when (2) will have a solution.
A unique solution exists if and only if $\Phi$ is non-singular, which in particular
requires that $\Phi$ is square.
In the square case, it can be
[shown](https://en.wikipedia.org/wiki/Vandermonde_matrix) that $\Phi$ is non-singular
if and only if all of the $x_j$ are distinct, which is the typical case for interpolation.
Under the assumption that the $x_j$ are distinct, we thus conclude that the
space $\pi_{N-1}(\mathbb{R})$ provides a unique interpolating polynomial, whose
coefficients can be computed by $\hat{\alpha}_{\text{interp}} = \Phi^{-1} y$.
The interpolated value at a new input $\tilde{x} \in (a, b)$ is then given by
\begin{align}
s(\tilde{x}) &= \varphi(\tilde{x})^\top \hat{\alpha}_{\text{interp}}
= \varphi(\tilde{x})^\top \Phi^{-1} y
\end{align}

## Regression with Polynomials
We consider the same setting as above, but now suppose that the $f_j$ correspond
to noisy evaluations of some underlying function $f$,
$$
f_j = f(x_j) + \epsilon_j.
$$
We now consider finding a polynomial $s \in \pi_{M-1}(\mathbb{R})$ that satisfies
$s(x_j) \approx f_j$ for $j = 1, \dots, N$. To make this precise, we attempt
to minimize the average squared error, which yields the ordinary least squares
(OLS) regression problem
\begin{align}
\text{min}\_{s \in \pi_{M-1}(\mathbb{R})} \lVert y - s(X) \rVert_2^2 =
\text{min}\_{\alpha \in \mathbb{R}^M} \lVert y - \Phi \alpha \rVert_2^2, \tag{3}
\end{align}
where $s(X) = (s(x_1), \dots, s(x_N))^\top$.

The optimality condition for this problem is well-known to be given by the
[normal equations](https://fncbook.github.io/fnc/leastsq/normaleqns.html),
\begin{align}
\Phi^\top \Phi \alpha = \Phi^\top y.
\end{align}
If $\Phi$ is of full column rank then $\Phi^\top \Phi$ is invertible, which leads
to the unique least squares solution
\begin{align}
\hat{\alpha}_{\text{OLS}} = (\Phi^\top \Phi)^{-1} \Phi^\top y.
\end{align}
Note that the full column rank condition in particular requires $M \leq N$,
corresponding to the usual regression setting in which the number of observations
exceeds the number of variables. If we consider the special case where $\Phi$ is invertible, the unique OLS
solution reduces to the interpolation solution:
\begin{align}
\hat{\alpha}\_{\text{OLS}} = \Phi^{-1} (\Phi^\top)^{-1} \Phi^\top y = \Phi^{-1}y = \hat{\alpha}\_{\text{interp}}.
\end{align}

Finally, note that a regression prediction (assuming the case where $\hat{\alpha}_{\text{OLS}}$
is unique) at a new location $\tilde{x} \in (a, b)$ is given by
\begin{align}
s(\tilde{x}) &= \phi(\tilde{x})^\top \hat{\alpha}\_{\text{OLS}} = \phi(\tilde{x})^\top (\Phi^\top \Phi)^{-1} \Phi^\top y,
\end{align}  
which again reduces to the interpolation analog when $\Phi$ is invertible.

## Splines
In the previous sections, we uncovered a remarkable property possessed by (one-dimensional)
polynomials: for any set of $N$ distinct sites $X$, the space $\pi_{N-1}(\mathbb{R})$
contains a unique interpolating polynomial. However, even in one dimension this
does not imply that the problem is solved in a practical sense. As the number
of sites grows large, working with very high degree polynomials becomes very
difficult. To prevent the dimensionality from getting out of hand, an alternative
approach is to work in a space $S$ consisting of piecewise polynomials of lower
dimension. A simple example is just linearly interpolating the data, but typically
we want to work with smoother functions. Thus, we might impose constraints such
as requiring the existence of a certain number of derivatives, which requires
ensuring that the polynomials "line up" correctly at the interior knots. We
briefly discuss two popular examples of such spaces below.

### Cubic Splines
We define the space of **cubic splines**, denoted $S_3(X)$, to be the set of
all piecewise cubic polynomials that are also twice continuously differentiable,
$$
S_3(X) := \left\{s \in C^2[a,b] : s|[x_j, x_{j+1}) \in \pi_3(\mathbb{R}), 0 \leq j \leq N \right\}. \tag{5}
$$
We write $s|c, d)$ for the restriction of the function $s$ to the interval
$[c, d)$. Also recall that by definition $x_0 = a$ and $x_{N+1}=b$.
Note that, unlike the polynomial spaces considered above, the space of
cubic splines is defined with respect to the specific set of locations $X$.

Cubic splines are indeed a vector space, which follows from the fact that
$C^2[a,b]$ and $\pi_3(\mathbb{R})$ are vector spaces. The dimension of $S_3(X)$
can be established by furnishing a basis for the space, but a back-of-the-envelope
calculation can give you a good guess. Indeed, note that the interior sites
define $N+1$ intervals, on each of which contains cubic polynomials. Since
$\pi_{3}(\mathbb{R})$ has dimension $4$ then this yields $4(N+1)$ degrees of
freedom. But in neglecting the $C^2[a,b]$ constraint we have over-counted.
This constraint requires that the function values, first derivatives, and
second derivatives match at each of the $N$ interior nodes, thus removing
$3N$ degrees of freedom, for a total count of $4(N+1) - 3N = N + 4$. This is
indeed the correct dimension of $S_3(X)$, and a basis for this space is given by
$$
\{(x - x_j)^3_+\}_{j=1}^{N} \cup \{1, x, x^2, x^3\},
$$
using the notation $x_+ := \max(x, 0)$. In fact, replacing $\{1, x, x^2, x^3\}$
with any basis of $\pi_3(\mathbb{R})$ will do the trick. Cubic splines are sometimes
described as the simplest splines which look smooth to the human eye.

As far as regression and interpolation, with a basis for $S_3(X)$ in hand we can
proceed very similarly to the previous two sections; in this case we now have
$\varphi(x) \in \mathbb{R}^{N+4}$ and $\Phi \in \mathbb{R}^{N \times (N+4)}$. 

{% endkatexmm %}
