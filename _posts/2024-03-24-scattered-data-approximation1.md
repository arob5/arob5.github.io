---
title: An Introduction to Scattered Data Approximation
subtitle: I summarize the first chapter of Holger Wendland's book "Scattered Data Approximation", which I augment with some background on polynomial interpolation and splines.
layout: default
date: 2024-03-24
keywords: Numerical-Analysis, Kernel-Methods
published: true
---

This post provides a summary of the first chapter of Holger Wendland's
excellent book on
[scattered data approximation ](https://www.cambridge.org/core/books/scattered-data-approximation/980EEC9DBC4CAA711D089187818135E3). The introductory chapter nominally provides motivation
and applications for the subsequent theory, but is somewhat of a whirlwind in
itself if you lack significant background in approximation theory. I therefore
try to augment the material in the chapter with some additional content,
which is especially suited for readers coming from more of a statistical
background. The notation I use more or less follows that of Wendland.

# The General Problem
{% katexmm %}
The theory of scattered data interpolation addresses a very general problem,
encompassing both problems of interpolation and smoothing (i.e., regression).
The generic setup considers a dataset $\{(x_j, f_j)\}_{j=1}^{N}$ consisting of
$N$ **sites** (also called **locations**, **knots**, or **design points** in
different settings) $X := \{x_j\}_{j=1}^{N}$ and
corresponding values $f_j$ at each respective site $x_j$. In typical applications,
these values are assumed to be generated by some underlying function
$$
f_j = f(x_j),
$$
perhaps subject to noise
$$
f_j \approx f(x_j),
$$
but this need not be the case. The noiseless and noisy settings naturally
motivate the two primary problems in this domain: **interpolation** and
**approximation**. I will also refer to the latter as a problem of *smoothing*
or *regression*, depending on the context. In the two cases, we seek a function
$s(\cdot)$ that either interpolates
\begin{align}
f_j &= s(x_j), && j = 1, \dots, N
\end{align}
or approximates
\begin{align}
f_j &\approx s(x_j), && j = 1, \dots, N
\end{align}
the data. The name "scattered" comes from the lack of assumptions on the sites
$X$; we will generally treat them generically, without assuming any sort of special
structure. On the other hand, we will require assumptions on the generating
process for the $f_j$, which typically means assuming that the underlying function
$f(x)$ satisfies some notion of smoothness. We will thus consider
the approximation properties of different spaces of smooth functions $S$, with
the goal being to identify the function $s \in S$ which is optimal in some sense.

In order to fully specify a concrete problem, we need to define in what space
the $x_j$ live. Statisticians would probably jump right to $\mathbb{R}^d$, but
in approximation theory there are some fundamental differences between one and
multiple dimensions. Therefore, in the next section we start by discussing the
concrete case of a one-dimensional input space, which will be used to motivate
higher-dimensional generalizations later on.  
{% endkatexmm %}

# The One-Dimensional Case
{% katexmm %}
In this section we will assume that $x_j \in \mathbb{R}$; in particular,
that $x_j \in (a, b)$ for all $j = 1, \dots, N$ for some $a < b$. It will
sometimes be useful to extend the notation as $x_0 := a$ and $x_{N+1} := b$,
in which case we will sometimes refer to $X = \{x_j\}_{j=1}^{N}$ as the
**interior sites** to distinguish from the end points. Note that there are
different conventions for this in the literature, but it is important to remember
that in our case $N$ denotes the number of sites with corresponding output
values $f_j$. The goal in this setting is to find a continuous function
$s: [a,b] \to \mathbb{R}$ which either interpolates or approximates the data.

## Polynomials
Following Wendland, let $\pi_M(\mathbb{R})$ denote the space of polynomials with
dimension at most $M$; i.e., the functions $s: \mathbb{R} \to \mathbb{R}$ which
admit a representation
\begin{align}
s(x) = \alpha_0 + \alpha_1 x + \alpha_2 x^2 + \cdots + \alpha_M x^M
\end{align}
for some coefficients $\alpha_0, \dots, \alpha_M \in \mathbb{R}$. $\pi_M(\mathbb{R})$ is
indeed a *space* in the linear algebra sense; it is a vector space of dimension
$M+1$. The most obvious basis for this space is the monomial basis
$1, x, x^2, \dots, x^M$ but many other bases are commonly used which have
nicer theoretical and computational properties.

## Interpolation with Polynomials
Let's now consider solving the interpolation problem using polynomials; that is,
we will try to find a function $s \in \pi_{M-1}(\mathbb{R})$ which interpolates the
data. I'm considering $M-1$ and not $M$ since the space $\pi_{M-1}(\mathbb{R})$
has dimension $M$, which will be more convenient notationally than having to write
$M+1$ all the time. With this choice of function space, the interpolation requirement is that
\begin{align}
f_j &= \alpha_0 + \alpha_1 x_j + \alpha_2 x_j^2 + \cdots \alpha_M x_j^{M-1}, &&\forall j = 1, \dots, N.
\end{align}  
It is convenient to define
\begin{align}
\varphi(x) &:= (1, x, x^2, \dots, x^{M-1})^\top \in \mathbb{R}^{M} \newline
\alpha &:= (\alpha_0, \alpha_1, \alpha_2, \dots, \alpha_{M-1})^\top \in \mathbb{R}^{M}
\end{align}
so that we can re-write the above as
\begin{align}
f_j &= \alpha_0 + \alpha_1 x_j + \alpha_2 x_j^2 + \cdots \alpha_M x_j^{M-1} = \varphi(x_j)^\top \alpha,
&&\forall j = 1, \dots, N. \tag{1}
\end{align}  
To succinctly write the equalities for all $j$, let $\Phi \in \mathbb{R}^{N \times M}$
be the matrix with $j^{\text{th}}$ row equal to $\varphi(x_j)^\top$. The requirement
(1) is then encoded in the linear system
\begin{align}
y = \Phi \alpha, \tag{2}
\end{align}
where we have additionally defined $y := (f_1, \dots, f_N)^\top \in \mathbb{R}^N$.
The questions of existence and uniqueness can now be answered by appealing to known
properties of linear systems. For an interpolating polynomial $s \in \pi_{M-1}(\mathbb{R})$
to exist, a necessary condition is that $M \geq N$. A unique solution exists if
and only if $\Phi$ is non-singular, which in particular requires that $\Phi$
is square. This means that the polynomial space $\pi_{N-1}(\mathbb{R})$ provides
a unique interpolating polynomial.


## Regression with Polynomials





{% endkatexmm %}
