---
title: An Introduction to Scattered Data Approximation
subtitle: I summarize the first chapter of Holger Wendland's book "Scattered Data Approximation", which I augment with some background on polynomial interpolation and splines.
layout: default
date: 2024-03-24
keywords: Numerical-Analysis, Kernel-Methods
published: true
---

This post provides a summary of the first chapter of Holger Wendland's
excellent book on
[scattered data approximation ](https://www.cambridge.org/core/books/scattered-data-approximation/980EEC9DBC4CAA711D089187818135E3). The introductory chapter nominally provides motivation
and applications for the subsequent theory, but is somewhat of a whirlwind in
itself if you lack significant background in approximation theory. I therefore
try to augment the material in the chapter with some additional content,
which is especially suited for readers coming from more of a statistical
background. The notation I use more or less follows that of Wendland.

# The General Problem
{% katexmm %}
The theory of scattered data interpolation addresses a very general problem,
encompassing both problems of interpolation and smoothing (i.e., regression).
The generic setup considers a dataset $\{(x_j, f_j)\}_{j=1}^{N}$ consisting of
$N$ **sites** (also called **locations**, **knots**, or **design points** in
different settings) $X := \{x_j\}_{j=1}^{N}$ and
corresponding values $f_j$ at each respective site $x_j$. In typical applications,
these values are assumed to be generated by some underlying function
$$
f_j = f(x_j),
$$
perhaps subject to noise
$$
f_j \approx f(x_j),
$$
but this need not be the case. The noiseless and noisy settings naturally
motivate the two primary problems in this domain: **interpolation** and
**approximation**. I will also refer to the latter as a problem of *smoothing*
or *regression*, depending on the context. In the two cases, we seek a function
$s(\cdot)$ that either interpolates
\begin{align}
f_j &= s(x_j), && j = 1, \dots, N
\end{align}
or approximates
\begin{align}
f_j &\approx s(x_j), && j = 1, \dots, N
\end{align}
the data. The name "scattered" comes from the lack of assumptions on the sites
$X$; we will generally treat them generically, without assuming any sort of special
structure. On the other hand, we will require assumptions on the generating
process for the $f_j$, which typically means assuming that the underlying function
$f(x)$ satisfies some notion of smoothness. We will thus consider
the approximation properties of different spaces of smooth functions $S$, with
the goal being to identify the function $s \in S$ which is optimal in some sense.

In order to fully specify a concrete problem, we need to define in what space
the $x_j$ live. Statisticians would probably jump right to $\mathbb{R}^d$, but
in approximation theory there are some fundamental differences between one and
multiple dimensions. Therefore, in the next section we start by discussing the
concrete case of a one-dimensional input space, which will be used to motivate
higher-dimensional generalizations later on.  
{% endkatexmm %}

# The One-Dimensional Case
{% katexmm %}
In this section we will assume that $x_j \in \mathbb{R}$; in particular,
that $x_j \in (a, b)$ for all $j = 1, \dots, N$ for some $a < b$. It will
sometimes be useful to extend the notation as $x_0 := a$ and $x_{N+1} := b$,
in which case we will sometimes refer to $X = \{x_j\}_{j=1}^{N}$ as the
**interior sites** to distinguish from the end points. Note that there are
different conventions for this in the literature, but it is important to remember
that in our case $N$ denotes the number of sites with corresponding output
values $f_j$. The goal in this setting is to find a continuous function
$s: [a,b] \to \mathbb{R}$ which either interpolates or approximates the data.

## Polynomials
Following Wendland, let $\pi_M(\mathbb{R})$ denote the space of polynomials with
dimension at most $M$; i.e., the functions $s: \mathbb{R} \to \mathbb{R}$ which
admit a representation
\begin{align}
s(x) = a_0 + a_1 x + a_2 x^2 + \cdots + a_M x^M
\end{align}
for some coefficients $a_0, \dots, a_M \in \mathbb{R}$. $\pi_M(\mathbb{R})$ is
indeed a *space* in the linear algebra sense; it is a vector space of dimension
$M+1$. The most obvious basis for this space is the monomial basis
$1, x, x^2, \dots, x^M$ but many other bases are commonly used which have
nicer theoretical and computational properties.

## Interpolation with Polynomials 

## Regression with Polynomials





{% endkatexmm %}
