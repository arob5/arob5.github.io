[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andrew G. Roberts",
    "section": "",
    "text": "The Reversed Cholesky Decomposition\n\n\nCholesky-like decomposition with upper-triangular matrices.\n\n\n\nLinear-Algebra\n\n\n\n\n\n\n\n\n\nJan 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nRegularized Least Squares with Singular Prior\n\n\nSolving the regularized least squares optimization problem when the prior covariance matrix is not positive definite.\n\n\n\nStatistics\n\n\nData-Assimilation\n\n\nOptimization\n\n\nInverse-Problem\n\n\n\n\n\n\n\n\n\nDec 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNonlinear Least Squares\n\n\nGauss-Newton, Levenberg-Marquardt\n\n\n\nStatistics\n\n\nOptimization\n\n\nInverse-Problem\n\n\n\n\n\n\n\n\n\nNov 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Gaussian Process Priors and Hyperparameter Estimation\n\n\nA deep dive into hyperparameter specifications for GP mean and covariance functions, including both frequentist and Bayesian methods for hyperparameter estimation.\n\n\n\nStatistics\n\n\nGaussian-Process\n\n\nkernel-methods\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDeriving the Metropolis-Hastings Update from the Transition Kernel\n\n\n\n\n\n\nMCMC\n\n\nProbability\n\n\nComputational Statistics\n\n\n\n\n\n\n\n\n\nDec 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPrincipal Components Analysis\n\n\nA deep dive into PCA.\n\n\n\nStatistics\n\n\nLinear-Algebra\n\n\n\n\n\n\n\n\n\nDec 15, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/MH-update-from-kernel.html",
    "href": "posts/MH-update-from-kernel.html",
    "title": "Deriving the Metropolis-Hastings Update from the Transition Kernel",
    "section": "",
    "text": "The Metropolis-Hastings (MH) Markov Chain Monte Carlo (MCMC) method is typically introduced in the form of a practical algorithm. In a more theoretically-oriented context, one might prove that the algorithm defines a Markov chain and derive the associated transition (i.e. probability) kernel. I have found it insightful to also work through the derivations in the reverse order; given only the transition kernel, how could one derive the well-known MH update? In other words, how can you simulate the Markov chain implied by the transition kernel? In this post, I work through the required derivations."
  },
  {
    "objectID": "posts/MH-update-from-kernel.html#setup-and-notation",
    "href": "posts/MH-update-from-kernel.html#setup-and-notation",
    "title": "Deriving the Metropolis-Hastings Update from the Transition Kernel",
    "section": "Setup and notation",
    "text": "Setup and notation\nWe consider drawing samples from a target probability distribution \\(\\mu\\) supported on a state space \\(\\mathcal{X} \\subseteq \\mathbb{R}^D\\) with Borel sigma algebra \\(\\mathcal{B}\\). Let \\(\\pi: \\mathcal{X} \\to [0,\\infty]\\) denote the Lebesgue density of \\(\\mu\\), i.e. \\[\n\\mu(A) = \\int_A \\mu(d\\mathbf{x}) = \\int_A \\pi(\\mathbf{x}) d\\mathbf{x}, \\qquad \\forall A \\in \\mathcal{B}.\n\\] Let \\(Q: \\mathcal{X} \\times \\mathcal{B} \\to [0,1]\\) denote the proposal kernel for the MH algorithm, with \\(q(\\mathbf{x}, \\cdot)\\) the Lebesgue density of the measure \\(Q(\\mathbf{x}, \\cdot)\\). For current state \\(\\mathbf{x} \\in \\mathcal{X}\\) and proposal \\(\\mathbf{y} \\sim Q(\\mathbf{x}, \\cdot)\\) we recall the MH acceptance probability \\[\n\\alpha(\\mathbf{x}, \\mathbf{y})\n=\\min\\left(1, \\frac{\\pi(\\mathbf{y})q(\\mathbf{y},\\mathbf{x})}{\\pi(\\mathbf{x})q(\\mathbf{x},\\mathbf{y})} \\right).\n\\] Throughout this post I will let \\(A \\in \\mathcal{B}\\) denote an arbitrary Borel set. The transition kernel \\(P:\\mathcal{X} \\times \\mathcal{B} \\to [0,1]\\) implied by the MH algorithm is then given by \\[\\begin{align}\nP(\\mathbf{x},A)\n= \\int_A q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y} + \\delta_{\\mathbf{x}}(A) \\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})[1-\\alpha(\\mathbf{x},\\mathbf{y})] d\\mathbf{y} \\tag{1}\n\\end{align}\\] where \\(\\delta_{\\mathbf{x}}(A) := \\mathbf{1}(\\mathbf{x} \\in A)\\) denotes the Dirac measure. The first term in the kernel is the probability of accepting a proposal in the set \\(A\\), while the second accounts for the probability of rejection in the case that the current state \\(\\mathbf{x}\\) is already in \\(A\\). I will denote the overall probability of acceptance by \\[\n\\overline{a}(\\mathbf{x})\n:= \\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y}. \\tag{2}\n\\]"
  },
  {
    "objectID": "posts/MH-update-from-kernel.html#mixture-of-kernels",
    "href": "posts/MH-update-from-kernel.html#mixture-of-kernels",
    "title": "Deriving the Metropolis-Hastings Update from the Transition Kernel",
    "section": "Mixture of Kernels",
    "text": "Mixture of Kernels\nSuppose we don’t already know the MH algorithm, and are given the task of writing an algorithm to draw a sample from the distribution \\(P(\\mathbf{x},\\cdot)\\) defined in (1). A reasonable place to start is to try writing \\(P(\\mathbf{x},\\cdot)\\) as a mixture of kernels from which we already know how to sample. Let’s first try this idea, attempting to write \\[\nP(\\mathbf{x},A) = wP_1(\\mathbf{x},A) + (1-w)P_2(\\mathbf{x},A), \\qquad w \\in [0,1], \\tag{3}\n\\] with \\(P_1\\), \\(P_2\\) transition kernels we already know how to sample from. If we are able to do this, then we could easily sample from \\(P(\\mathbf{x},\\cdot)\\) via the following simple algorithm: 1. Select \\(P_1\\) with probability \\(w\\), else select \\(P_2\\). 2. Sample from the selected kernel.\nTo this end, let’s manipulate (1) to write it in the form of a kernel mixture. We have \\[\\begin{align}\nP(\\mathbf{x},A)\n&= \\int_A q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y} + \\delta_{\\mathbf{x}}(A) \\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})[1-\\alpha(\\mathbf{x},\\mathbf{y})] d\\mathbf{y} \\newline\n&= \\overline{a}(\\mathbf{x}) \\int_A \\frac{q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})}{\\overline{a}(\\mathbf{x})}d\\mathbf{y} + \\left[1-\\overline{a}(\\mathbf{x})\\right] \\delta_{\\mathbf{x}}(A) \\tag{4}\n\\end{align}\\] which is a kernel mixture of the form (3) with \\[\\begin{align}\nw = \\overline{a}(\\mathbf{x}), \\qquad P_1(\\mathbf{x},A)=\\int_A \\frac{q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})}{\\overline{a}(\\mathbf{x})}d\\mathbf{y},\n\\qquad P_2(\\mathbf{x},A)=\\delta_{\\mathbf{x}}(A).\n\\end{align}\\] All we did here was to multiply and divide by the acceptance probability \\(\\overline{a}(\\mathbf{x})\\) in the first term (under typical assumptions on \\(Q\\) this will be non-zero when \\(\\mathbf{y}\\) is in the support of \\(\\pi\\)) and to rearrange the second term using the fact that \\(q(\\mathbf{x},\\cdot)\\) is a probability density; hence, \\[\n\\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y} = 1.\n\\] Note that the mixture weight \\(\\overline{a}(\\mathbf{x})\\) is the overall acceptance probability, and is thus in \\([0,1]\\) as required. Moreover, \\(P_2\\) is the Dirac measure centered at \\(\\mathbf{x}\\) and is thus a valid kernel. To check that \\(P_1(\\mathbf{x},\\cdot)\\) is a valid probability measure, we recall the form of \\(\\overline{a}(\\mathbf{x})\\) from (2) to verify that \\[\\begin{align}\nP_1(\\mathbf{x},A)\n&= \\int_{\\mathcal{X}} \\frac{q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})}{\\overline{a}(\\mathbf{x})}d\\mathbf{y} \\newline\n&= \\frac{\\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y}}{\\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y}} \\newline\n&= 1.\n\\end{align}\\] The other required properties (countable additivity and non-negativity) are similarly verified. Thus, \\(P_1(\\mathbf{x},\\cdot)\\) is a probability measure with Lebesgue density proportional to \\(q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})\\); the overall acceptance probability \\(\\overline{a}(\\mathbf{x})\\) is the normalizing constant for this density.\nThis representation of the MH kernel as a mixture distribution is conceptually useful, but it does not directly help us determine a sampling algorithm. Indeed, we cannot implement the simple mixture sampling algorithm described above since (i.) computing the mixture weight \\(\\overline{a}(\\mathbf{x})\\) requires evaluating an intractable integral, and (ii.) we don’t know how to directly sample from the probability distribution with density proportional to \\(q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})\\). While this approach seems to be a dead end from a practical point of view, we should keep in mind that the MH algorithm derived below does sample from the mixture (4), but does so in a way that avoids having to compute the mixture weight or to directly sample from \\(P_1(\\mathbf{x},\\cdot)\\)."
  },
  {
    "objectID": "posts/MH-update-from-kernel.html#marginalized-mixture-of-kernels",
    "href": "posts/MH-update-from-kernel.html#marginalized-mixture-of-kernels",
    "title": "Deriving the Metropolis-Hastings Update from the Transition Kernel",
    "section": "Marginalized Mixture of Kernels",
    "text": "Marginalized Mixture of Kernels\nIn the previous section, we feigned ignorance of the MH algorithm in order to approach the problem of simulating (1) as a generic sampling problem. We found that \\(P\\) can indeed be written as a mixture of kernels, but the problem of sampling from the resulting mixture was also intractable. To take a step in the right direction, it is useful to cheat a little bit and recall some of the mechanics of the MH algorithm. The proposal \\(\\mathbf{y} \\sim Q(\\mathbf{x},\\cdot)\\) is accepted with probability \\(\\alpha(\\mathbf{x},\\mathbf{y})\\); if rejected, the next state is set to the current state \\(\\mathbf{x}\\). Thus, it seems that we should be looking for a mixture of kernels of the form \\[\n\\alpha(\\mathbf{x},\\mathbf{y})\\delta_{\\mathbf{y}}(\\cdot) + [1-\\alpha(\\mathbf{x},\\mathbf{y})]\\delta_{\\mathbf{x}}(\\cdot). \\tag{5}\n\\] Of course, this can’t represent the whole picture since the mixture weight in (5) depends on \\(\\mathbf{y}\\) and the proposal kernel \\(Q\\) is completely missing from the expression. The key insight is that the MH kernel \\(P(\\mathbf{x},\\cdot)\\) can be viewed as the expectation of (5) averaged with respect to \\(Q(\\mathbf{x},\\cdot)\\); i.e., \\(P(\\mathbf{x},\\cdot)\\) is derived by marginalizing the mixture (5) with respect to \\(\\mathbf{y} \\sim Q(\\mathbf{x},\\cdot)\\). To show this, we return to the original expression (1) for the MH kernel. We have \\[\\begin{align}\nP(\\mathbf{x},A)\n&= \\int_A q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y} + \\delta_{\\mathbf{x}}(A) \\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})[1-\\alpha(\\mathbf{x},\\mathbf{y})] d\\mathbf{y} \\newline\n&= \\int_{\\mathcal{X}} \\alpha(\\mathbf{x},\\mathbf{y}) \\mathbf{1}(\\mathbf{y} \\in A) q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y} +  \\int_{\\mathcal{X}} [1-\\alpha(\\mathbf{x},\\mathbf{y})] \\delta_{\\mathbf{x}}(A) q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y} \\newline\n&= \\int_{\\mathcal{X}} \\alpha(\\mathbf{x},\\mathbf{y}) \\delta_{\\mathbf{y}}(A) q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y} +  \\int_{\\mathcal{X}} [1-\\alpha(\\mathbf{x},\\mathbf{y})] \\delta_{\\mathbf{x}}(A) q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y} \\newline\n&= \\int_{\\mathcal{X}} \\left[\\alpha(\\mathbf{x},\\mathbf{y})\\delta_{\\mathbf{y}}(A) + [1-\\alpha(\\mathbf{x},\\mathbf{y})] \\delta_{\\mathbf{x}}(A) \\right] q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y}, \\tag{6}\n\\end{align}\\] which is precisely the mixture (5) averaged (in \\(\\mathbf{y}\\)) with respect to the weights \\(q(\\mathbf{x},\\mathbf{y})\\). We now have three different representations of the MH transition kernel \\(P\\): (1) is the most natural to derive when starting from the MH algorithm, (4) represents \\(P\\) as a mixture of two distributions, and (6) represents \\(P\\) as a marginalized mixture of two distributions. It is this final representation which proves useful for developing a practical simulation algorithm.\nAll that remains is to recall how to sample from a marginalized distribution. First note that \\(\\mathbf{x}\\) is essentially a fixed parameter in the integral (6); the averaging is done with respect to \\(\\mathbf{y}\\). Now, if we condition on a fixed \\(\\mathbf{y}\\) as well, then the expression \\[\n\\alpha(\\mathbf{x},\\mathbf{y})\\delta_{\\mathbf{y}}(A) + [1-\\alpha(\\mathbf{x},\\mathbf{y})] \\delta_{\\mathbf{x}}(A)\n\\] is simply a mixture of two distributions, which we know how to sample from. Thus, a sample can be drawn from \\(P(\\mathbf{x},\\cdot)\\) via the following algorithm: 1. Sample \\(\\mathbf{y} \\sim Q(\\mathbf{x}, \\cdot)\\). 2. Conditional on \\(\\mathbf{y}\\), sample from \\(\\alpha(\\mathbf{x},\\mathbf{y})\\delta_{\\mathbf{y}} + [1-\\alpha(\\mathbf{x},\\mathbf{y})] \\delta_{\\mathbf{x}}\\).\nFor the second step, the mixture can be sampled from using the simple algorithm discussed in the previous section: randomly select one of the two kernels with probabilities equal to their mixture weights, then return a sample from the selected kernel. Since sampling from the Dirac measure \\(\\delta_{\\mathbf{x}}\\) simply means returning the value \\(\\mathbf{x}\\) (and similarly for \\(\\delta_{\\mathbf{y}}\\)) then this step will simply return \\(\\mathbf{x}\\) or \\(\\mathbf{y}\\) according to their respective probabilities \\(\\alpha(\\mathbf{x},\\mathbf{y})\\) and \\(1-\\alpha(\\mathbf{x},\\mathbf{y})\\). This is precisely the MH accept-reject mechanism!\nIt might be helpful to make this more concrete by letting \\(\\mathbf{X}_k\\) denote the value of the MCMC algorithm at iteration \\(k\\) and letting \\(\\mathbf{Y}|\\mathbf{X}_k \\sim Q(\\mathbf{X}_k, \\cdot)\\) be the random variable representing the proposal. Then the above mixture corresponds to the probability \\(\\mathbb{P}\\left(\\mathbf{X}_{k+1} \\in A | \\mathbf{X}_k=\\mathbf{x}, \\mathbf{Y}=\\mathbf{y}\\right)\\) so we can re-write (6) as \\[\nP(\\mathbf{x},A)\n= \\int_{\\mathcal{X}} \\mathbb{P}\\left(\\mathbf{X}_{k+1} \\in A | \\mathbf{X}_k=\\mathbf{x}, \\mathbf{Y}=\\mathbf{y}\\right) q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y}.\n\\] Once we condition on \\(\\mathbf{X}_k\\) and \\(\\mathbf{Y}\\), the only remaining randomness in the probability above is coming from the selection of one of the two kernels."
  },
  {
    "objectID": "posts/MH-update-from-kernel.html#conclusion",
    "href": "posts/MH-update-from-kernel.html#conclusion",
    "title": "Deriving the Metropolis-Hastings Update from the Transition Kernel",
    "section": "Conclusion",
    "text": "Conclusion\nWhile all of this might appear to be overcomplicating the very simple MH algorithm, I have found it a quite worthwhile exercise to contemplate some different perspectives on the method, as well as to get some practice manipulating expressions involving probability kernels and thinking through sampling schemes. The MH transition kernel (1) can easily be derived by thinking through the mechanics of the MH algorithm. In this post, I showed in (4) how the kernel can be re-written as mixture of two distributions and in (6) as a marginalized mixture of two distributions. It is this final expression which provides the basis for a tractable simulation algorithm."
  },
  {
    "objectID": "posts/pca.html",
    "href": "posts/pca.html",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "Suppose that we have data \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_N \\in \\mathbb{R}^D\\), stacked into the rows of a matrix \\(X \\in \\mathbb{R}^{N \\times D}\\). Our task is to find a subspace of smaller dimension \\(R &lt; D\\) such that the projection of the data points onto the subspace retains as much information as possible. By restricting our attention to orthonormal bases for the low-dimensional subspace, we reduce the problem to finding a set of orthonormal basis vectors \\[\n\\begin{align}\n&\\mathbf{b}_1, \\dots, \\mathbf{b}_R \\in \\mathbb{R}^D,\n&&\\langle \\mathbf{b}_r, \\mathbf{b}_s \\rangle = \\delta_{r,s}.\n\\end{align}\n\\] Define \\(B \\in \\mathbb{R}^{D \\times R}\\) to be the matrix with \\(r^{\\text{th}}\\) column equal to \\(\\mathbf{b}_r\\). The subspace generated by the basis \\(B\\) is given by \\[\n\\text{span}(B) := \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R).\n\\] Throughout this post I will abuse notation by referring to the matrix \\(B\\) when actually talking about the set of vectors \\(\\{\\mathbf{b}_1, \\dots, \\mathbf{b}_R\\}\\). Since there is no a priori reason to assume that the data is centered, we should also allow for the subspace to be shifted by some intercept \\(\\mathbf{w}_0 \\in \\mathbb{R}^D\\), resulting in the affine space \\[\n\\mathbf{w}_0 + \\text{span}(B) = \\left\\{\\mathbf{w}_0 +\n\\sum_{r=1}^{R} w_r \\mathbf{b}_r : w_1, \\dots, w_R \\in \\mathbb{R} \\right\\}.\n\\] Loosely speaking, the task is to find the basis \\(B\\), intercept \\(\\mathbf{w}_0\\), and pointwise weights \\(\\mathbf{w}_1, \\dots, \\mathbf{w}_N \\in \\mathbb{R}^R\\) such that \\[\n\\begin{align}\n\\mathbf{x}_n &\\approx \\mathbf{w}_0 + \\sum_{r=1}^{R} (\\mathbf{w}_n)_r \\mathbf{b}_r &&\\forall n=1,\\dots,N \\\\\n&= \\mathbf{w}_0 + B\\mathbf{w}_n.\n\\end{align}\n\\] To formalize this notion, PCA measures the error in the above approximation using Euclidean distance, averaged over the \\(N\\) data points. To further simplify notation, we stack the \\(\\mathbf{w}_n\\) in the columns of a matrix \\(W \\in \\mathbb{R}^{R \\times N}\\). With all of this notation established, we can state that PCA solves the optimization problem \\[\n\\text{argmin}_{B, W, \\mathbf{w}_0} \\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - (\\mathbf{w}_0 + B\\mathbf{w}_n) \\rVert_2^2, \\tag{1}\n\\] where the basis \\(B\\) is constrained to be orthonormal. As we will see, this optimization naturally breaks down into two distinct problems which can be solved sequentially: 1. Given the basis \\(B\\) and intercept \\(\\mathbf{w}_0\\), find the optimal basis coefficients \\(\\mathbf{w}_n\\) corresponding to each data point \\(\\mathbf{x}_n\\). 2. Find the optimal basis and intercept.\nPart of the popularity of PCA stems from the fact that both problems can be solved in closed-form. Let us consider both problems in turn.\n\n\n\nLet us first consider \\(\\mathbf{w}_0\\) and \\(B\\) to be fixed, meaning that we are fixing an affine subspace of dimension \\(R\\). We seek to find the optimal way to represent the data \\(X\\) in this lower-dimensional space. As we will show, the Euclidean objective used by PCA implies that this problem reduces to straightforward orthogonal projection. For now, let \\(\\mathbf{x}^c_n := \\mathbf{x}_n - \\mathbf{w}_0\\) denote the centered data points (we will deal with the intercept shortly). We are thus considering the problem \\[\n\\text{argmin}_{W} \\sum_{n=1}^{N} \\lVert \\mathbf{x}^c_n - B\\mathbf{w}_n \\rVert_2^2 \\tag{2}\n\\] Observe that \\(\\mathbf{w}_n\\) only appears in the \\(n^{\\text{th}}\\) term of the sum, meaning that we can consider each summand independently, \\[\n\\text{argmin}_{\\mathbf{w}_n} \\lVert \\mathbf{x}^c_n - B\\mathbf{w}_n \\rVert_2^2.\n\\] In words, we seek the linear combination of the basis vectors \\(B\\) that results in minimal Euclidean distance from \\(\\mathbf{x}^c_n\\); this is a standard orthogonal projection problem from linear algebra. Since the basis vectors are orthonormal, the optimal projection coefficients are given by \\[\n\\begin{align}\n&(\\mathbf{w}_n)_r = \\langle \\mathbf{x}_n^c, \\mathbf{b}_r \\rangle,\n&&\\mathbf{w}_n = B^\\top \\mathbf{x}_n^c\n\\end{align}\n\\] which can be written succinctly for all data points by stacking the \\(\\mathbf{w}_n^\\top\\) as rows in a matrix \\(W\\); i.e., \\[\nW := X^c B,\n\\] with \\(X^c\\) denoting the centered data matrix with rows set to the \\((\\mathbf{x}^c_n)^\\top\\).\n\n\n\nIn the previous section, we saw that for a fixed basis and intercept, optimizing the basis weights reduced to an orthogonal projection problem. In this section we show that with the weights fixed at their optimal values, optimizing the basis reduces to solving a sequence of eigenvalue problems. To be clear, we are now considering the problem \\[\n\\text{argmin}_{B} \\sum_{n=1}^{N} \\lVert \\mathbf{x}^c_n - B\\mathbf{w}^*_n \\rVert_2^2, \\tag{3}\n\\] where the \\(\\mathbf{w}^*_n\\) are now fixed at the optimal values satisfying (2); i.e., \\(\\mathbf{w}^*_n = B^\\top \\mathbf{x}^c_n\\). However, in the derivations below we will just write \\(\\mathbf{w}_n = \\mathbf{w}^*_n\\) to keep the notation lighter. Note that we are still treating \\(\\mathbf{w}_0\\) as fixed for the time being. We will make another notational simplification in this section by writing \\(\\mathbf{x}_n = \\mathbf{x}_n^c\\). Just keep in mind that throughout this section, \\(\\mathbf{x}_n\\) should be interpreted as \\(\\mathbf{x}_n - \\mathbf{w}_0\\).\nThis problem is also referred to as minimizing the reconstruction error, since \\(\\lVert \\mathbf{x}_n - \\mathbf{\\hat{x}}_n \\rVert_2 := \\lVert \\mathbf{x}_n - B\\mathbf{w}_n \\rVert_2\\) is the error between the original data point \\(\\mathbf{x}_n\\) and the \\(D\\)-dimensional vector \\(\\mathbf{\\hat{x}}_n\\) which can be thought of as an approximation to \\(\\mathbf{x}_n\\) that has been reconstructed from its lower-dimensional representation \\(\\mathbf{w}_n\\). The key here is to re-write this objective function so that this optimization problem takes the form of an eigenvalue problem, which is something that we already know how to solve (see the appendix, A1).\nTo start, we extend the orthonormal set \\(\\mathbf{b}_1, \\dots, \\mathbf{b}_R\\) to an orthonormal basis \\(\\mathbf{b}_1, \\dots, \\mathbf{b}_D\\) for \\(\\mathbb{R}^D\\). Now we can write the original data point \\(\\mathbf{x}_n\\) and its approximation \\(\\mathbf{\\hat{x}}_n\\) with respect to this basis as \\[\n\\begin{align}\n&\\mathbf{x}\\_n = \\sum_{r=1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r,\n&&\\mathbf{\\hat{x}}\\_n = \\sum\\_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r\n\\end{align}\n\\]\nand hence the residual \\(\\mathbf{x}_n - \\mathbf{\\hat{x}}_n\\) is given by \\[\n\\begin{align}\n\\mathbf{x}_n - \\mathbf{\\hat{x}}_n &= \\sum_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r.\n\\end{align}\n\\]\nThus, the objective function in (3) can be written as \\[\n\\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{\\hat{x}}_n \\rVert_2^2\n= \\sum\\_{n=1}^{N} \\bigg\\lVert \\sum\\_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r \\bigg\\rVert_2^2\n= \\sum\\_{n=1}^{N} \\sum\\_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2 \\lVert \\mathbf{b}_r \\rVert_2^2\n= \\sum\\_{n=1}^{N} \\sum\\_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2,\n\\]\nwhere the second and third equalities use the facts that the \\(\\mathbf{b}_r\\) are orthogonal and of unit norm, respectively.\nWe could continue working with this formulation, but at this point it is convenient to re-write the minimization problem we have been working with as an equivalent maximization problem. Note that the above residual calculation is of the form \\(\\mathbf{\\hat{e}}_n = \\mathbf{x}_n - \\mathbf{\\hat{x}}_n\\) (and summed over \\(n\\)). Since \\(\\mathbf{x}_n\\) is fixed, then minimizing the residual (i.e., the reconstruction error) is equivalent to maximizing \\(\\mathbf{\\hat{x}}_n\\). More rigorously, we have\n\\[\n\\begin{align}\n\\text{argmin}_B \\sum\\_{n=1}^{N} \\sum\\_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\n&= \\text{argmin}_B \\sum\\_{n=1}^{N}\n\\left(\\sum\\_{r=1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2 - \\sum\\_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\\right) \\newline\n&= \\text{argmin}_B \\sum\\_{n=1}^{N}\n\\left(\\lVert \\mathbf{x}_n \\rVert_2^2 - \\sum\\_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\\right) \\newline\n&= \\text{argmax}_B \\sum\\_{n=1}^{N} \\sum\\_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2. \\tag{4}\n\\end{align}\n\\]\nWe can now re-write the squared inner product to obtain \\[\\begin{align}\n\\sum\\_{n=1}^{N} \\sum\\_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\n&= \\sum\\_{n=1}^{N} \\sum\\_{r=1}^{R} \\mathbf{b}_r^\\top \\mathbf{x}_n \\mathbf{x}_n^\\top \\mathbf{b}_r\n= \\sum\\_{r=1}^{R} \\mathbf{b}_r^\\top \\left(\\sum\\_{n=1}^{N}\\mathbf{x}_n \\mathbf{x}_n^\\top\\right) \\mathbf{b}_r\n= \\sum\\_{r=1}^{R} \\mathbf{b}_r^\\top (X^\\top X) \\mathbf{b}_r^\\top,\n\\end{align}\\]\nwhere the final step uses this fact. We have managed to re-write (3) as \\[\n\\text{argmax}_{B} \\sum_{r=1}^{D} \\mathbf{b}_r^\\top (X^\\top X) \\mathbf{b}_r^\\top, \\tag{5}\n\\] where we recall that this is also subject to the constraint that \\(B\\) is orthogonal.\nBefore proceeding, we note that \\(X^\\top X\\) is a positive semi-definite matrix, whose eigenvalues we denote \\(\\lambda_1, \\dots, \\lambda_D\\), sorted in decreasing order. Note that the eigenvalues are all non-negative due to the positive definiteness. Let \\(\\mathbf{e}_1, \\dots, \\mathbf{e}_D\\) denote the respective eigenvectors, normalized to have unit norm. These vectors are guaranteed to be orthogonal by the Spectral Theorem.\nWe now notice in (5) that the objective function has been decomposed into \\(R\\) different terms, each of which only depends on a single \\(\\mathbf{b}_r\\). However, these do not constitute \\(R\\) independent optimization problems, as they are all coupled through the orthogonality constraint. We will thus consider solving them in a recursive fashion, beginning with the first term, \\[\n\\text{argmax}_{\\lVert \\mathbf{b}_1 \\rVert_2=1} \\mathbf{b}_1^\\top (X^\\top X) \\mathbf{b}_1^\\top\n= \\text{argmax}_{\\lVert \\mathbf{b}_1 \\rVert_2=1} \\lVert X \\mathbf{b}_1 \\rVert_2^2.\n\\] This is an eigenvalue problem! It is precisely of the form (A4) (see appendix) and so we apply that result to conclude that the optimal argument is \\(\\mathbf{b}_1 = \\mathbf{e}_1\\) with associated optimal value \\(\\lambda_1\\) (note the objective here is the squared norm, in contrast to the statement in the appendix). Taking this as the base case, we now proceed inductively. Assume that at the \\(r^{\\text{th}}\\) problem in the sequence, the solution is given by \\((\\mathbf{b}_1, \\dots, \\mathbf{b}_r) = (\\mathbf{e}_1, \\dots, \\mathbf{e}_r)\\). We must show the solution to the \\((r+1)^{\\text{st}}\\) problem is \\(\\mathbf{e}_{r+1}\\). Under the inductive hypothesis, this problem is constrained so that \\(\\mathbf{b}_{r+1}\\) is orthogonal to each of \\(\\mathbf{e}_1, \\dots, \\mathbf{e}_r\\); i.e., we require \\(\\mathbf{b}_{r+1} \\perp \\text{span}(\\mathbf{e}_1, \\dots, \\mathbf{e}_r)\\). If we denote \\(\\mathcal{E}_{r} := \\text{span}(\\mathbf{e}_1, \\dots, \\mathbf{e}_r)\\) and \\(\\mathcal{E}^{\\perp}_{r}\\) the orthogonal complement of \\(\\mathcal{E}_{r}\\), then a succinct way to write the orthogonality constraint is that \\(\\mathbf{b}_{r+1} \\in \\mathcal{E}^{\\perp}_r\\). The problem can thus be written as \\[\\begin{align}\n\\text{argmax}\\_{\\mathbf{b}\\_{r+1} \\in \\mathcal{E}^{\\perp}\\_{r}, \\lVert \\mathbf{b}\\_{r+1} \\rVert_2=1} \\lVert X \\mathbf{b}\\_{r+1} \\rVert_2^2, \\tag{6}\n\\end{align}\\] which is another eigenvalue problem, precisely of the form (A3). Using this result from the appendix, we conclude that this is solved by \\(\\mathbf{b}_{r+1} = \\mathbf{e}_{r+1}\\), with the maximal objective value \\(\\lambda_{r+1}\\).\nThat was a lot, so before moving on let’s briefly summarize. First of all, recall that I have been abusing notation by writing \\(\\mathbf{x}_n\\) where I should be writing \\(\\mathbf{x}_n^c = \\mathbf{x}_n - \\mathbf{w}_0\\). In summarizing the result here I will make this correction. Here we have considered the problem of finding the optimal orthonormal basis \\(B\\), for any fixed \\(\\mathbf{w}_0 \\in \\mathbb{R}^D\\), but with the \\(\\mathbf{w}_n\\) set to their optimal values satisfying (2); i.e., \\(\\mathbf{w}_n = B^\\top \\mathbf{x}^c_n\\). Given this, we showed that the reconstruction error (5) is minimized by setting \\(B\\) equal to the matrix with columns given by the dominant \\(R\\) (normalized) eigenvectors of \\((X^c)^\\top X^c\\). We arrived at this solution by showing that the error minimization problem (5) could be viewed as a sequence of \\(R\\) eigenvalue problems.\n\n\n\nThe last ingredient we are missing to solve (1) is the optimal value of \\(\\mathbf{w}_0\\), which has henceforth been viewed as fixed in the above derivations. At first glance, this problem might seem like somewhat of an afterthought, but there are some subtleties that are worth exploring here.\nThe problem we are now considering is \\[\n\\text{argmin}_{\\mathbf{w}_0} \\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{w}_0 - B\\mathbf{w}^*_n \\rVert_2^2, \\tag{7}\n\\] with \\(\\mathbf{w}^*_n\\) denoting the optimal weights \\(\\mathbf{w}^*_n = B^\\top \\mathbf{x}_n\\) derived above (these derivations will go through with any orthonormal basis \\(B\\)). Plugging in this expression for \\(\\mathbf{w}^*_n\\) gives \\[\n\\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{w}_0 - B\\mathbf{w}^*_n \\rVert_2^2\n= \\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{w}_0 - BB^\\top \\mathbf{x}_n \\rVert_2^2\n= \\sum_{n=1}^{N} \\lVert (I - BB^\\top)(\\mathbf{x}_n - \\mathbf{w}_0) \\rVert_2^2.\n\\] Computing the gradient of this expression with respect to \\(\\mathbf{w}_0\\) and setting it equal to zero yields the optimality condition \\[\n\\sum_{n=1}^{N} (I - BB^\\top)(\\mathbf{x}_n - \\mathbf{w}_0) = 0,\n\\] where we have used the fact that \\((I - BB^\\top)^2 = (I - BB^\\top)\\) (since \\(I - BB^\\top\\) is a projection matrix; see appendix). By linearity we then have \\[\n\\sum_{n=1}^{N} (I - BB^\\top)(\\mathbf{x}_n - \\mathbf{w}_0)\n= (I - BB^\\top) \\sum_{n=1}^{N} (\\mathbf{x}_n - \\mathbf{w}_0)\n= (I - BB^\\top)(N \\bar{\\mathbf{x}} - N\\mathbf{w}_0), \\tag{7}\n\\] where we have defined \\[\n\\bar{\\mathbf{x}} := \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}_n,\n\\] the empirical mean of the data. Since \\(\\mathbf{w}_0\\) is optimal when (8) is equal to zero, this leads to the condition \\[\n(I - BB^\\top)(\\bar{\\mathbf{x}} - \\mathbf{w}_0) = 0,\n\\] or equivalently, \\[\n\\bar{\\mathbf{x}} - \\mathbf{w}_0 \\in \\text{Null}(I - BB^\\top).\n\\] Noting that the null space is non-trivial, since \\(\\text{Null}(I - BB^\\top) = \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R)\\) (again, see the appendix section on projection matrices), we then conclude that there are infinitely many optimal solutions! Using the basis \\(\\mathbf{b}_1, \\dots, \\mathbf{b}_R\\) for the null space, we can characterize the set of optimal \\(\\mathbf{w}_0\\) as those satisfying \\[\n\\bar{\\mathbf{x}} - \\mathbf{w}_0 \\in \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R),\n\\] or, more explicitly, all vectors within the affine space \\[\n\\mathbf{w}_0 \\in \\bar{\\mathbf{x}} + \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R). \\tag{9}\n\\] While we have an infinity of optimal solutions we could choose from, the obvious choice \\(\\mathbf{w}_0^* := \\bar{\\mathbf{x}}\\) stands out. Indeed, this is the choice that is essentially always made in practice, so much so that many PCA tutorials will begin by assuming that the data points have all been centered by subtracting off their empirical mean. I find it more insightful to include the intercept as a variable in the PCA optimization problem, and then show that the choice to set it equal to \\(\\bar{\\mathbf{x}}\\) is actually justified.\nMoreover, it is quite interesting that the mean is not actually the unique optimal choice here. Why is this? The characterization (9) says that we can add any vector lying in the span of the orthonormal basis to \\(\\bar{\\mathbf{x}}\\) and still maintain optimality. So the key requirement is that, after shifting the data by subtracting off \\(\\mathbf{w}_0\\), the resulting shifted points must “lie along” the lower-dimensional subspace \\(\\text{span}(B)\\). Since, \\(\\text{span}(B)\\) defines a hyperplane, the data must lie somewhere along this plane; from the perspective of the optimization problem, it doesn’t matter whether it lies around the origin or somewhere very far away, so long as it is clustered around this plane. A picture is worth a thousand words here, and I will try to add one once I have time.\nFinally, note that the specific choice of \\(\\bar{\\mathbf{x}}\\) has various other practical benefits. It leads to projections that are clustered around the origin, thus keeping numbers relatively small. It also leads to a nice statistical interpretation of the eigenvalue problems discussed in the previous subsection; e.g. the basis vector \\(\\mathbf{b}_1\\) can be viewed as the direction along which the empirical variance of the projected data is maximized. This maximum variance perspective is discussed in more detail below."
  },
  {
    "objectID": "posts/pca.html#setup-and-notation",
    "href": "posts/pca.html#setup-and-notation",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "Suppose that we have data \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_N \\in \\mathbb{R}^D\\), stacked into the rows of a matrix \\(X \\in \\mathbb{R}^{N \\times D}\\). Our task is to find a subspace of smaller dimension \\(R &lt; D\\) such that the projection of the data points onto the subspace retains as much information as possible. By restricting our attention to orthonormal bases for the low-dimensional subspace, we reduce the problem to finding a set of orthonormal basis vectors \\[\n\\begin{align}\n&\\mathbf{b}_1, \\dots, \\mathbf{b}_R \\in \\mathbb{R}^D,\n&&\\langle \\mathbf{b}_r, \\mathbf{b}_s \\rangle = \\delta_{r,s}.\n\\end{align}\n\\] Define \\(B \\in \\mathbb{R}^{D \\times R}\\) to be the matrix with \\(r^{\\text{th}}\\) column equal to \\(\\mathbf{b}_r\\). The subspace generated by the basis \\(B\\) is given by \\[\n\\text{span}(B) := \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R).\n\\] Throughout this post I will abuse notation by referring to the matrix \\(B\\) when actually talking about the set of vectors \\(\\{\\mathbf{b}_1, \\dots, \\mathbf{b}_R\\}\\). Since there is no a priori reason to assume that the data is centered, we should also allow for the subspace to be shifted by some intercept \\(\\mathbf{w}_0 \\in \\mathbb{R}^D\\), resulting in the affine space \\[\n\\mathbf{w}_0 + \\text{span}(B) = \\left\\{\\mathbf{w}_0 +\n\\sum_{r=1}^{R} w_r \\mathbf{b}_r : w_1, \\dots, w_R \\in \\mathbb{R} \\right\\}.\n\\] Loosely speaking, the task is to find the basis \\(B\\), intercept \\(\\mathbf{w}_0\\), and pointwise weights \\(\\mathbf{w}_1, \\dots, \\mathbf{w}_N \\in \\mathbb{R}^R\\) such that \\[\n\\begin{align}\n\\mathbf{x}_n &\\approx \\mathbf{w}_0 + \\sum_{r=1}^{R} (\\mathbf{w}_n)_r \\mathbf{b}_r &&\\forall n=1,\\dots,N \\\\\n&= \\mathbf{w}_0 + B\\mathbf{w}_n.\n\\end{align}\n\\] To formalize this notion, PCA measures the error in the above approximation using Euclidean distance, averaged over the \\(N\\) data points. To further simplify notation, we stack the \\(\\mathbf{w}_n\\) in the columns of a matrix \\(W \\in \\mathbb{R}^{R \\times N}\\). With all of this notation established, we can state that PCA solves the optimization problem \\[\n\\text{argmin}_{B, W, \\mathbf{w}_0} \\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - (\\mathbf{w}_0 + B\\mathbf{w}_n) \\rVert_2^2, \\tag{1}\n\\] where the basis \\(B\\) is constrained to be orthonormal. As we will see, this optimization naturally breaks down into two distinct problems which can be solved sequentially: 1. Given the basis \\(B\\) and intercept \\(\\mathbf{w}_0\\), find the optimal basis coefficients \\(\\mathbf{w}_n\\) corresponding to each data point \\(\\mathbf{x}_n\\). 2. Find the optimal basis and intercept.\nPart of the popularity of PCA stems from the fact that both problems can be solved in closed-form. Let us consider both problems in turn."
  },
  {
    "objectID": "posts/pca.html#optimizing-the-basis-coefficients",
    "href": "posts/pca.html#optimizing-the-basis-coefficients",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "Let us first consider \\(\\mathbf{w}_0\\) and \\(B\\) to be fixed, meaning that we are fixing an affine subspace of dimension \\(R\\). We seek to find the optimal way to represent the data \\(X\\) in this lower-dimensional space. As we will show, the Euclidean objective used by PCA implies that this problem reduces to straightforward orthogonal projection. For now, let \\(\\mathbf{x}^c_n := \\mathbf{x}_n - \\mathbf{w}_0\\) denote the centered data points (we will deal with the intercept shortly). We are thus considering the problem \\[\n\\text{argmin}_{W} \\sum_{n=1}^{N} \\lVert \\mathbf{x}^c_n - B\\mathbf{w}_n \\rVert_2^2 \\tag{2}\n\\] Observe that \\(\\mathbf{w}_n\\) only appears in the \\(n^{\\text{th}}\\) term of the sum, meaning that we can consider each summand independently, \\[\n\\text{argmin}_{\\mathbf{w}_n} \\lVert \\mathbf{x}^c_n - B\\mathbf{w}_n \\rVert_2^2.\n\\] In words, we seek the linear combination of the basis vectors \\(B\\) that results in minimal Euclidean distance from \\(\\mathbf{x}^c_n\\); this is a standard orthogonal projection problem from linear algebra. Since the basis vectors are orthonormal, the optimal projection coefficients are given by \\[\n\\begin{align}\n&(\\mathbf{w}_n)_r = \\langle \\mathbf{x}_n^c, \\mathbf{b}_r \\rangle,\n&&\\mathbf{w}_n = B^\\top \\mathbf{x}_n^c\n\\end{align}\n\\] which can be written succinctly for all data points by stacking the \\(\\mathbf{w}_n^\\top\\) as rows in a matrix \\(W\\); i.e., \\[\nW := X^c B,\n\\] with \\(X^c\\) denoting the centered data matrix with rows set to the \\((\\mathbf{x}^c_n)^\\top\\)."
  },
  {
    "objectID": "posts/pca.html#optimizing-the-basis",
    "href": "posts/pca.html#optimizing-the-basis",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "In the previous section, we saw that for a fixed basis and intercept, optimizing the basis weights reduced to an orthogonal projection problem. In this section we show that with the weights fixed at their optimal values, optimizing the basis reduces to solving a sequence of eigenvalue problems. To be clear, we are now considering the problem \\[\n\\text{argmin}_{B} \\sum_{n=1}^{N} \\lVert \\mathbf{x}^c_n - B\\mathbf{w}^*_n \\rVert_2^2, \\tag{3}\n\\] where the \\(\\mathbf{w}^*_n\\) are now fixed at the optimal values satisfying (2); i.e., \\(\\mathbf{w}^*_n = B^\\top \\mathbf{x}^c_n\\). However, in the derivations below we will just write \\(\\mathbf{w}_n = \\mathbf{w}^*_n\\) to keep the notation lighter. Note that we are still treating \\(\\mathbf{w}_0\\) as fixed for the time being. We will make another notational simplification in this section by writing \\(\\mathbf{x}_n = \\mathbf{x}_n^c\\). Just keep in mind that throughout this section, \\(\\mathbf{x}_n\\) should be interpreted as \\(\\mathbf{x}_n - \\mathbf{w}_0\\).\nThis problem is also referred to as minimizing the reconstruction error, since \\(\\lVert \\mathbf{x}_n - \\mathbf{\\hat{x}}_n \\rVert_2 := \\lVert \\mathbf{x}_n - B\\mathbf{w}_n \\rVert_2\\) is the error between the original data point \\(\\mathbf{x}_n\\) and the \\(D\\)-dimensional vector \\(\\mathbf{\\hat{x}}_n\\) which can be thought of as an approximation to \\(\\mathbf{x}_n\\) that has been reconstructed from its lower-dimensional representation \\(\\mathbf{w}_n\\). The key here is to re-write this objective function so that this optimization problem takes the form of an eigenvalue problem, which is something that we already know how to solve (see the appendix, A1).\nTo start, we extend the orthonormal set \\(\\mathbf{b}_1, \\dots, \\mathbf{b}_R\\) to an orthonormal basis \\(\\mathbf{b}_1, \\dots, \\mathbf{b}_D\\) for \\(\\mathbb{R}^D\\). Now we can write the original data point \\(\\mathbf{x}_n\\) and its approximation \\(\\mathbf{\\hat{x}}_n\\) with respect to this basis as \\[\n\\begin{align}\n&\\mathbf{x}\\_n = \\sum_{r=1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r,\n&&\\mathbf{\\hat{x}}\\_n = \\sum\\_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r\n\\end{align}\n\\]\nand hence the residual \\(\\mathbf{x}_n - \\mathbf{\\hat{x}}_n\\) is given by \\[\n\\begin{align}\n\\mathbf{x}_n - \\mathbf{\\hat{x}}_n &= \\sum_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r.\n\\end{align}\n\\]\nThus, the objective function in (3) can be written as \\[\n\\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{\\hat{x}}_n \\rVert_2^2\n= \\sum\\_{n=1}^{N} \\bigg\\lVert \\sum\\_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r \\bigg\\rVert_2^2\n= \\sum\\_{n=1}^{N} \\sum\\_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2 \\lVert \\mathbf{b}_r \\rVert_2^2\n= \\sum\\_{n=1}^{N} \\sum\\_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2,\n\\]\nwhere the second and third equalities use the facts that the \\(\\mathbf{b}_r\\) are orthogonal and of unit norm, respectively.\nWe could continue working with this formulation, but at this point it is convenient to re-write the minimization problem we have been working with as an equivalent maximization problem. Note that the above residual calculation is of the form \\(\\mathbf{\\hat{e}}_n = \\mathbf{x}_n - \\mathbf{\\hat{x}}_n\\) (and summed over \\(n\\)). Since \\(\\mathbf{x}_n\\) is fixed, then minimizing the residual (i.e., the reconstruction error) is equivalent to maximizing \\(\\mathbf{\\hat{x}}_n\\). More rigorously, we have\n\\[\n\\begin{align}\n\\text{argmin}_B \\sum\\_{n=1}^{N} \\sum\\_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\n&= \\text{argmin}_B \\sum\\_{n=1}^{N}\n\\left(\\sum\\_{r=1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2 - \\sum\\_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\\right) \\newline\n&= \\text{argmin}_B \\sum\\_{n=1}^{N}\n\\left(\\lVert \\mathbf{x}_n \\rVert_2^2 - \\sum\\_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\\right) \\newline\n&= \\text{argmax}_B \\sum\\_{n=1}^{N} \\sum\\_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2. \\tag{4}\n\\end{align}\n\\]\nWe can now re-write the squared inner product to obtain \\[\\begin{align}\n\\sum\\_{n=1}^{N} \\sum\\_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\n&= \\sum\\_{n=1}^{N} \\sum\\_{r=1}^{R} \\mathbf{b}_r^\\top \\mathbf{x}_n \\mathbf{x}_n^\\top \\mathbf{b}_r\n= \\sum\\_{r=1}^{R} \\mathbf{b}_r^\\top \\left(\\sum\\_{n=1}^{N}\\mathbf{x}_n \\mathbf{x}_n^\\top\\right) \\mathbf{b}_r\n= \\sum\\_{r=1}^{R} \\mathbf{b}_r^\\top (X^\\top X) \\mathbf{b}_r^\\top,\n\\end{align}\\]\nwhere the final step uses this fact. We have managed to re-write (3) as \\[\n\\text{argmax}_{B} \\sum_{r=1}^{D} \\mathbf{b}_r^\\top (X^\\top X) \\mathbf{b}_r^\\top, \\tag{5}\n\\] where we recall that this is also subject to the constraint that \\(B\\) is orthogonal.\nBefore proceeding, we note that \\(X^\\top X\\) is a positive semi-definite matrix, whose eigenvalues we denote \\(\\lambda_1, \\dots, \\lambda_D\\), sorted in decreasing order. Note that the eigenvalues are all non-negative due to the positive definiteness. Let \\(\\mathbf{e}_1, \\dots, \\mathbf{e}_D\\) denote the respective eigenvectors, normalized to have unit norm. These vectors are guaranteed to be orthogonal by the Spectral Theorem.\nWe now notice in (5) that the objective function has been decomposed into \\(R\\) different terms, each of which only depends on a single \\(\\mathbf{b}_r\\). However, these do not constitute \\(R\\) independent optimization problems, as they are all coupled through the orthogonality constraint. We will thus consider solving them in a recursive fashion, beginning with the first term, \\[\n\\text{argmax}_{\\lVert \\mathbf{b}_1 \\rVert_2=1} \\mathbf{b}_1^\\top (X^\\top X) \\mathbf{b}_1^\\top\n= \\text{argmax}_{\\lVert \\mathbf{b}_1 \\rVert_2=1} \\lVert X \\mathbf{b}_1 \\rVert_2^2.\n\\] This is an eigenvalue problem! It is precisely of the form (A4) (see appendix) and so we apply that result to conclude that the optimal argument is \\(\\mathbf{b}_1 = \\mathbf{e}_1\\) with associated optimal value \\(\\lambda_1\\) (note the objective here is the squared norm, in contrast to the statement in the appendix). Taking this as the base case, we now proceed inductively. Assume that at the \\(r^{\\text{th}}\\) problem in the sequence, the solution is given by \\((\\mathbf{b}_1, \\dots, \\mathbf{b}_r) = (\\mathbf{e}_1, \\dots, \\mathbf{e}_r)\\). We must show the solution to the \\((r+1)^{\\text{st}}\\) problem is \\(\\mathbf{e}_{r+1}\\). Under the inductive hypothesis, this problem is constrained so that \\(\\mathbf{b}_{r+1}\\) is orthogonal to each of \\(\\mathbf{e}_1, \\dots, \\mathbf{e}_r\\); i.e., we require \\(\\mathbf{b}_{r+1} \\perp \\text{span}(\\mathbf{e}_1, \\dots, \\mathbf{e}_r)\\). If we denote \\(\\mathcal{E}_{r} := \\text{span}(\\mathbf{e}_1, \\dots, \\mathbf{e}_r)\\) and \\(\\mathcal{E}^{\\perp}_{r}\\) the orthogonal complement of \\(\\mathcal{E}_{r}\\), then a succinct way to write the orthogonality constraint is that \\(\\mathbf{b}_{r+1} \\in \\mathcal{E}^{\\perp}_r\\). The problem can thus be written as \\[\\begin{align}\n\\text{argmax}\\_{\\mathbf{b}\\_{r+1} \\in \\mathcal{E}^{\\perp}\\_{r}, \\lVert \\mathbf{b}\\_{r+1} \\rVert_2=1} \\lVert X \\mathbf{b}\\_{r+1} \\rVert_2^2, \\tag{6}\n\\end{align}\\] which is another eigenvalue problem, precisely of the form (A3). Using this result from the appendix, we conclude that this is solved by \\(\\mathbf{b}_{r+1} = \\mathbf{e}_{r+1}\\), with the maximal objective value \\(\\lambda_{r+1}\\).\nThat was a lot, so before moving on let’s briefly summarize. First of all, recall that I have been abusing notation by writing \\(\\mathbf{x}_n\\) where I should be writing \\(\\mathbf{x}_n^c = \\mathbf{x}_n - \\mathbf{w}_0\\). In summarizing the result here I will make this correction. Here we have considered the problem of finding the optimal orthonormal basis \\(B\\), for any fixed \\(\\mathbf{w}_0 \\in \\mathbb{R}^D\\), but with the \\(\\mathbf{w}_n\\) set to their optimal values satisfying (2); i.e., \\(\\mathbf{w}_n = B^\\top \\mathbf{x}^c_n\\). Given this, we showed that the reconstruction error (5) is minimized by setting \\(B\\) equal to the matrix with columns given by the dominant \\(R\\) (normalized) eigenvectors of \\((X^c)^\\top X^c\\). We arrived at this solution by showing that the error minimization problem (5) could be viewed as a sequence of \\(R\\) eigenvalue problems."
  },
  {
    "objectID": "posts/pca.html#optimizing-the-intercept",
    "href": "posts/pca.html#optimizing-the-intercept",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "The last ingredient we are missing to solve (1) is the optimal value of \\(\\mathbf{w}_0\\), which has henceforth been viewed as fixed in the above derivations. At first glance, this problem might seem like somewhat of an afterthought, but there are some subtleties that are worth exploring here.\nThe problem we are now considering is \\[\n\\text{argmin}_{\\mathbf{w}_0} \\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{w}_0 - B\\mathbf{w}^*_n \\rVert_2^2, \\tag{7}\n\\] with \\(\\mathbf{w}^*_n\\) denoting the optimal weights \\(\\mathbf{w}^*_n = B^\\top \\mathbf{x}_n\\) derived above (these derivations will go through with any orthonormal basis \\(B\\)). Plugging in this expression for \\(\\mathbf{w}^*_n\\) gives \\[\n\\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{w}_0 - B\\mathbf{w}^*_n \\rVert_2^2\n= \\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{w}_0 - BB^\\top \\mathbf{x}_n \\rVert_2^2\n= \\sum_{n=1}^{N} \\lVert (I - BB^\\top)(\\mathbf{x}_n - \\mathbf{w}_0) \\rVert_2^2.\n\\] Computing the gradient of this expression with respect to \\(\\mathbf{w}_0\\) and setting it equal to zero yields the optimality condition \\[\n\\sum_{n=1}^{N} (I - BB^\\top)(\\mathbf{x}_n - \\mathbf{w}_0) = 0,\n\\] where we have used the fact that \\((I - BB^\\top)^2 = (I - BB^\\top)\\) (since \\(I - BB^\\top\\) is a projection matrix; see appendix). By linearity we then have \\[\n\\sum_{n=1}^{N} (I - BB^\\top)(\\mathbf{x}_n - \\mathbf{w}_0)\n= (I - BB^\\top) \\sum_{n=1}^{N} (\\mathbf{x}_n - \\mathbf{w}_0)\n= (I - BB^\\top)(N \\bar{\\mathbf{x}} - N\\mathbf{w}_0), \\tag{7}\n\\] where we have defined \\[\n\\bar{\\mathbf{x}} := \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}_n,\n\\] the empirical mean of the data. Since \\(\\mathbf{w}_0\\) is optimal when (8) is equal to zero, this leads to the condition \\[\n(I - BB^\\top)(\\bar{\\mathbf{x}} - \\mathbf{w}_0) = 0,\n\\] or equivalently, \\[\n\\bar{\\mathbf{x}} - \\mathbf{w}_0 \\in \\text{Null}(I - BB^\\top).\n\\] Noting that the null space is non-trivial, since \\(\\text{Null}(I - BB^\\top) = \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R)\\) (again, see the appendix section on projection matrices), we then conclude that there are infinitely many optimal solutions! Using the basis \\(\\mathbf{b}_1, \\dots, \\mathbf{b}_R\\) for the null space, we can characterize the set of optimal \\(\\mathbf{w}_0\\) as those satisfying \\[\n\\bar{\\mathbf{x}} - \\mathbf{w}_0 \\in \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R),\n\\] or, more explicitly, all vectors within the affine space \\[\n\\mathbf{w}_0 \\in \\bar{\\mathbf{x}} + \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R). \\tag{9}\n\\] While we have an infinity of optimal solutions we could choose from, the obvious choice \\(\\mathbf{w}_0^* := \\bar{\\mathbf{x}}\\) stands out. Indeed, this is the choice that is essentially always made in practice, so much so that many PCA tutorials will begin by assuming that the data points have all been centered by subtracting off their empirical mean. I find it more insightful to include the intercept as a variable in the PCA optimization problem, and then show that the choice to set it equal to \\(\\bar{\\mathbf{x}}\\) is actually justified.\nMoreover, it is quite interesting that the mean is not actually the unique optimal choice here. Why is this? The characterization (9) says that we can add any vector lying in the span of the orthonormal basis to \\(\\bar{\\mathbf{x}}\\) and still maintain optimality. So the key requirement is that, after shifting the data by subtracting off \\(\\mathbf{w}_0\\), the resulting shifted points must “lie along” the lower-dimensional subspace \\(\\text{span}(B)\\). Since, \\(\\text{span}(B)\\) defines a hyperplane, the data must lie somewhere along this plane; from the perspective of the optimization problem, it doesn’t matter whether it lies around the origin or somewhere very far away, so long as it is clustered around this plane. A picture is worth a thousand words here, and I will try to add one once I have time.\nFinally, note that the specific choice of \\(\\bar{\\mathbf{x}}\\) has various other practical benefits. It leads to projections that are clustered around the origin, thus keeping numbers relatively small. It also leads to a nice statistical interpretation of the eigenvalue problems discussed in the previous subsection; e.g. the basis vector \\(\\mathbf{b}_1\\) can be viewed as the direction along which the empirical variance of the projected data is maximized. This maximum variance perspective is discussed in more detail below."
  },
  {
    "objectID": "posts/pca.html#minimum-error-or-maximum-variance",
    "href": "posts/pca.html#minimum-error-or-maximum-variance",
    "title": "Principal Components Analysis",
    "section": "Minimum Error or Maximum Variance?",
    "text": "Minimum Error or Maximum Variance?\nWhile the derivations in the preceding section are somewhat lengthy, recall that this was all in the pursuit of solving the optimization problem (1). In words, we derived the best \\(R\\)-dimensional affine subspace to represent the data \\(X\\), where “best” is defined as minimizing the average Euclidean error between the data points and their projections onto the subspace. We showed that this error minimization problem could be re-written as a sequence of \\(R\\) maximization problems of the form (6). We now show that these maximization problems have a very nice statistical interpretation.\n\nSample covariance of the \\(\\mathbf{x}_n\\)\nWe first recall that the empirical covariance matrix of the data points \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\) is defined to be \\[\n\\hat{C} := \\frac{1}{N-1} \\sum_{n=1}^{N} (\\mathbf{x}_n - \\bar{\\mathbf{x}}) (\\mathbf{x}_n - \\bar{\\mathbf{x}})^\\top,\n\\] which can be re-written as \\[\n\\hat{C} = \\frac{1}{N-1} \\sum_{n=1}^{N} \\mathbf{x}^c_n (\\mathbf{x}^c_n)^\\top = \\frac{1}{N-1} X^c (X^c)^\\top, \\tag{10}\n\\] where the superscript c indicates that the observations have been centered by subtracting off their empirical mean.\nRecall that solving the maximization problems (6) revealed that the optimal basis vectors are given by the dominant eigenvectors of the matrix \\(X^c (X^c)^\\top\\), which is the (unscaled) covariance (10)! The scaling factor does not affect the optimal basis vectors, it simply scales the objective function. Specifically, \\[\\begin{align}\n\\text{argmax}\\_{\\mathbf{b}\\_{r+1} \\in \\mathcal{E}^{\\perp}\\_{r}, \\lVert \\mathbf{b}\\_{r+1} \\rVert_2=1}\n\\left(\\mathbf{b}\\_{r+1}^\\top X^c (X^c)^\\top \\mathbf{b}\\_{r+1}\\right)\n= \\text{argmax}\\_{\\mathbf{b}\\_{r+1} \\in \\mathcal{E}^{\\perp}\\_{r}, \\lVert \\mathbf{b}\\_{r+1} \\rVert_2=1}\n\\left(\\mathbf{b}\\_{r+1}^\\top \\hat{C} \\mathbf{b}\\_{r+1}\\right). \\tag{11}\n\\end{align}\\] We haven’t changed anything from (6) here, other than noting that a re-scaling of the objective function allows us involve \\(\\hat{C}\\) in the expression.\n\n\nSample covariance of the \\(\\mathbf{w}_n\\)\nGiven that the sample covariance matrix of the data \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_N \\in \\mathbb{R}^D\\) is given by \\(\\hat{C}\\), it is natural to also consider the empirical covariance of \\(\\mathbf{w}_1, \\dots, \\mathbf{w}_N \\in \\mathbb{R}^R\\). We begin by computing the sample mean \\[\n\\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{w}_n\n= \\frac{1}{N} \\sum_{n=1}^{N} B\\mathbf{x}^c_n\n= B \\left[\\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}^c_n \\right] = 0,\n\\] using the fact that the empirical mean of the centered data points is \\(0\\). Recalling that the row vectors \\(\\mathbf{w}_n^\\top\\) are stored in the rows of the matrix \\(W\\), it follows that the empirical covariance matrix of \\(\\mathbf{w}_1, \\dots, \\mathbf{w}_N\\) is given by \\[\n\\hat{C}_w := \\frac{1}{N-1} W^\\top W, \\tag{12}\n\\] which follows from the calculation (10) with \\(W\\) in place of \\(X^c\\). Since \\(W = XB\\), we have \\[\n\\hat{C}_w = \\frac{1}{N-1} (XB)^\\top (XB) = B^\\top \\hat{C} B,\n\\] which allows us to write the covariance of the \\(\\mathbf{w}_n\\) as a function of the covariance of the \\(\\mathbf{x}_n\\). We can say even more since we know that \\(B\\) is given by \\(V_R\\), the truncated set of eigenvectors obtained from the eigendecomposition \\(X^\\top X = V \\Lambda V^\\top\\). We thus have \\[\n\\hat{C}_w\n= \\frac{1}{N-1} B^\\top (X^\\top X) B\n= \\frac{1}{N-1} V_R^\\top (X^\\top X) V_R\n= \\frac{1}{N-1} V_R^\\top V_R \\Lambda_R\n= \\frac{1}{N-1} \\Lambda_R,  \\tag{13}\n\\] where \\((X^\\top X) V_R = V_R \\Lambda_R\\) follows from the fact that the columns of \\(V_R\\) store the first \\(R\\) eigenvectors of \\(X^\\top X\\). The conclusion here is that \\(\\hat{C}_w\\) is diagonal, with variances equal to the eigenvalues of \\(\\hat{C}\\). In other words, PCA computes a change-of-basis that diagonalizes the empirical covariance of the data.\n\n\nPCA as Variance Maximization\nWe now return to the goal of providing a statistical interpretation of the objective function \\(\\mathbf{b}_{r}^\\top \\hat{C} \\mathbf{b}_r\\) in (11). Given the derivation of \\(\\hat{C}_w\\) in (13), we see the empirical variance of \\((\\mathbf{w}_1)_r, \\dots, (\\mathbf{w}_N)_r\\) (i.e., the values in the \\(r^{\\text{th}}\\) column of \\(W\\)) is equal to \\[\n[\\hat{C}_w]_{rr} = \\mathbf{b}_r^\\top \\hat{C} \\mathbf{b}_r,\n\\] which is precisely the objective being maximized. To interpret this quantity more clearly, we consider the projection of \\(\\mathbf{x}_n^c\\) onto the span of \\(\\mathbf{b}_r\\), \\[\n\\text{proj}_{\\mathbf{b}_r} \\mathbf{x}^c_n\n:= \\langle \\mathbf{x}^c_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r\n= (\\mathbf{w}_n)_r \\mathbf{b}_r,\n\\] which implies that \\((\\mathbf{w}_n)_r\\) is the magnitude of the projection. Therefore, we conclude that \\(\\mathbf{b}_r^\\top \\hat{C} \\mathbf{b}_r\\) is the sample variance of the magnitude of the projections onto the subspace \\(\\text{span}(\\mathbf{b}_r)\\); loosely speaking, the variance of the projection along the \\(r^{\\text{th}}\\) basis vector. Combining all of these equivalent expressions yields the chain of equalities, \\[\n\\text{Tr}(\\hat{C}_w)\n= \\frac{1}{N-1} \\sum_{r=1}^{R} \\lambda_r\n= \\sum_{n=1}^{N} \\sum_{r=1}^{R} W_{nr}^2.\n= \\sum_{r=1}^{R} \\mathbf{b}_r^\\top \\hat{C} \\mathbf{b}_r \\tag{15}\n\\] The trace \\(\\text{Tr}(\\hat{C}_w)\\) thus represents the total variance of the projection, summed over all of the basis vectors. The total variance is equivalently given by the sum of the eigenvalues of \\(\\hat{C}\\) or by the squared Frobenius norm \\(\\lVert W \\rVert_F^2\\).\nThe final term in (15) provides an alternative interpretation of the objective function in (5); namely, that PCA seeks the basis \\(B\\) which results in the maximal projected total variance. The resulting sequence of constrained problems, as in (11), are interpreted similarly. In particular, \\[\n\\text{argmax}_{\\mathbf{b}_{r+1} \\in \\mathcal{E}^{\\perp}_{r}, \\lVert \\mathbf{b}_{r+1} \\rVert_2=1}\n\\left(\\mathbf{b}_{r+1}^\\top \\hat{C} \\mathbf{b}_{r+1}\\right)\n\\] can be viewed as seeking the direction along which the variance of the projections is maximized, subject to the constraint that the direction be orthogonal to the previous \\(r\\) directions. The optimal solution is the direction corresponding to the \\((r+1)^{\\text{st}}\\) eigenvector of the empirical covariance \\(\\hat{C}\\), and the resulting maximal variance in this direction is given by the associated eigenvalue."
  },
  {
    "objectID": "posts/pca.html#the-singular-value-decomposition",
    "href": "posts/pca.html#the-singular-value-decomposition",
    "title": "Principal Components Analysis",
    "section": "The Singular Value Decomposition",
    "text": "The Singular Value Decomposition"
  },
  {
    "objectID": "posts/pca.html#a-matrix-approximation-problem-the-eckart-young-theorem",
    "href": "posts/pca.html#a-matrix-approximation-problem-the-eckart-young-theorem",
    "title": "Principal Components Analysis",
    "section": "A Matrix Approximation Problem: The Eckart-Young Theorem",
    "text": "A Matrix Approximation Problem: The Eckart-Young Theorem\nIgnoring the intercept (or assuming that the \\(\\mathbf{x}_n\\) have already been centered), we can re-write the reconstruction error as \\[\n\\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - B\\mathbf{w}_n \\rVert_2^2\n= \\lVert X - WB^\\top \\rVert_F^2,\n\\] where \\(\\lVert \\cdot \\rVert_F\\) denotes the Frobenius norm. The PCA optimization problem can then be written as the matrix approximation problem \\[\n\\text{argmin}_{B, W} \\lVert X - WB^\\top \\rVert_F^2, \\tag{10}\n\\] where \\(B\\) is constrained to be an orthogonal matrix. We can equivalently write this as \\[\\begin{align}\n&\\text{argmin}_{\\hat{X} \\in \\mathcal{M}} \\lVert X - \\hat{X} \\rVert_F^2,\n&&\\mathcal{M} = \\\\{\\hat{X} \\in \\mathbb{R}^{N \\times D} : \\hat{X}=WB^\\top, B \\in \\mathbb{R}^{D \\times R} \\text{ is orthogonal}\\\\}, \\tag{11}\n\\end{align}\\] which makes it even more clear that PCA can generically be viewed as the problem of approximating the data matrix \\(X\\) with another matrix \\(\\hat{X}\\) that is constrained to lie in a subset \\(\\mathcal{M}\\) of all \\(N \\times D\\) matrices. We can phrase this even more succinctly by noticing that \\(\\mathcal{M}\\) is precisely the set of all \\(N \\times D\\) matrices with rank at most \\(R\\). Indeed, if \\(\\hat{X} \\in \\mathcal{M}\\) then \\(\\text{rank}(\\hat{X}) = \\text{rank}(W B^\\top) \\leq R\\) since \\(W\\) has \\(R\\) columns and thus the rank of this matrix product cannot exceed \\(R\\). Conversely, if we let \\(\\hat{X}\\) be an arbirary \\(N \\times D\\) matrix of rank \\(r \\leq R\\) then \\(\\hat{X}\\) can be expanded using the compact SVD as \\[\n\\hat{X} = U\\tilde{\\Sigma}V^\\top = (U\\tilde{\\Sigma})V^\\top,\n\\] where \\(U \\in \\mathbb{R}^{N \\times r}\\), \\(V \\in \\mathbb{R}^{D \\times r}\\) are orthogonal and \\(\\tilde{\\Sigma} \\in \\mathbb{R}^{r \\times r}\\) is diagonal. By setting \\(W := U\\tilde{\\Sigma}\\) and \\(B := V\\) we have almost written things in the form required by (11), but (11) restricts \\(B\\) to be orthogonal with exactly \\(R\\) columns. Thus, we can simply extend \\(V\\) to have \\(R\\) orthonormal columns by appending columns \\(B = [V|\\mathbf{v}_{r+1}, \\dots, \\mathbf{v}_{R}]\\) (this is justified by Gram-Schmidt) and then set \\(W = [U\\tilde{\\Sigma}|\\boldsymbol{0}]\\). Thus, \\(\\hat{X} = (U\\tilde{\\Sigma})V^\\top = WB^\\top\\) with \\(B\\) now of the required form.\nWe have thus verified that (11) can equivalently be written as the low-rank matrix approximation problem \\[\\begin{align}\n&\\text{argmin}_{\\hat{X} \\in \\mathcal{M}} \\lVert X - \\hat{X} \\rVert_F^2,\n&&\\mathcal{M} = \\\\{\\hat{X} \\in \\mathbb{R}^{N \\times D} : \\text{rank}(\\hat{X}) \\leq R \\\\}. \\tag{12}\n\\end{align}\\]\nThis is precisely the problem considered by the celebrated Eckart-Young theorem. The theorem concludes that the optimal solution to (12) is given by the truncated SVD \\(X = U_R \\Sigma_R V_R^\\top\\), which is precisely the PCA solution computed using SVD as discussed in the previous section."
  },
  {
    "objectID": "posts/pca.html#regression-with-an-optimal-basis",
    "href": "posts/pca.html#regression-with-an-optimal-basis",
    "title": "Principal Components Analysis",
    "section": "Regression with an optimal basis",
    "text": "Regression with an optimal basis"
  },
  {
    "objectID": "posts/pca.html#statisticalprobabalistic-perspectives",
    "href": "posts/pca.html#statisticalprobabalistic-perspectives",
    "title": "Principal Components Analysis",
    "section": "Statistical/Probabalistic Perspectives",
    "text": "Statistical/Probabalistic Perspectives\nThere are various ways we might attach a statistical or probabilistic perspective to PCA - we briefly discuss a few of them here. Throughout this section we will take \\[\n\\text{argmin}_{W,B} \\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - B\\mathbf{w}_n \\rVert_2^2  \\tag{13}\n\\] as the jumping off point for a probabilistic generalization; that is, we are implicitly assuming the data is centered so that we can ignore the intercept. Note that, as always, \\(B\\) is constrained to be orthogonal.\n\nA Special Case of the Karhunen-Loeve Expansion\nWe start by noting that the objective function in (13) looks like a sample average of the quantities \\(\\lVert \\mathbf{x}_n - B\\mathbf{w}_n \\rVert_2^2\\). It is therefore natural to consider some underlying true expectation that this sample average is approximating. To this end, let us view \\(\\mathbf{x} = \\mathbf{x}(\\omega)\\) and \\(\\mathbf{w} = \\mathbf{w}(\\omega)\\) as random vectors, defined over some probability space \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\). We can then consider \\[\n\\text{argmin}_{\\mathbf{w},B} \\mathbb{E} \\lVert \\mathbf{x} - B\\mathbf{w} \\rVert_2^2\n= \\text{argmin}_{\\mathbf{w},B} \\int_{\\Omega} \\left\\lVert \\mathbf{x}(\\omega) - \\sum_{r=1}^{R} \\mathbf{w}_r(\\omega) \\mathbf{b}_r \\right\\rVert_2^2 \\mathbb{P}(d\\omega), \\tag{14}\n\\] which can be viewed as the population analog of the sample approximation in (13). We note that the second expression above shows that the low-rank approximation of the random vector \\(\\mathbf{x}\\) takes the form of a linear combination of (non-random) basis vectors, with random coefficients.\nWith \\(B\\) fixed, the optimization in \\(\\mathbf{w}\\) is just as easy as before. Indeed, for any fixed \\(\\omega \\in \\Omega\\), \\[\n\\text{argmin}_{\\mathbf{w}(\\omega)} \\lVert \\mathbf{x}(\\omega) - B\\mathbf{w}(\\omega) \\rVert_2^2 = B^\\top \\mathbf{x}(\\omega),\n\\] using the same exact orthogonal projection reasoning as before. We have thus optimized for \\(\\mathbf{w}(\\omega)\\) on an \\(\\omega\\)-by-\\(\\omega\\) basis. The same result thus holds in expectation: \\[\n\\text{argmin}_{\\mathbf{w}} \\mathbb{E} \\lVert \\mathbf{x} - B\\mathbf{w} \\rVert_2^2 = B^\\top \\mathbf{x}.\n\\] It is important to note that we have found the optimal random vector \\(\\mathbf{w}\\), which was shown to be a linear transformation of the random vector \\(\\mathbf{x}\\).\nOptimizing for \\(B\\) with the optimal \\(\\mathbf{w}\\) fixed also follows similarly from previous derivations. Using some of the results already derived above (see the section on optimizing the basis), we have\n\\[\\begin{align}\n\\text{argmin}\\_{B} \\mathbb{E}\\lVert \\mathbf{x} - B\\mathbf{w} \\rVert_2^2\n&= \\text{argmin}\\_{B} \\mathbb{E} \\sum\\_{r=R+1}^{D} \\langle \\mathbf{x}, \\mathbf{b}_r \\rangle^2 \\newline\n&= \\text{argmin}\\_{B} \\mathbb{E} \\left[\\lVert \\mathbf{x} \\rVert_2^2 - \\sum\\_{r=1}^{R} \\langle \\mathbf{x}, \\mathbf{b}_r \\rangle^2 \\right] \\newline\n&= \\text{argmax}\\_{B} \\mathbb{E} \\sum\\_{r=1}^{R} \\langle \\mathbf{x}, \\mathbf{b}_r \\rangle^2 \\newline\n&= \\text{argmax}\\_{B} \\mathbb{E} \\sum\\_{r=1}^{R} \\mathbf{b}_r^\\top \\mathbf{x}\\mathbf{x}^\\top \\mathbf{b}_r \\newline\n&= \\text{argmax}\\_{B} \\sum\\_{r=1}^{R} \\mathbf{b}_r^\\top \\mathbb{E}\\left[\\mathbf{x}\\mathbf{x}^\\top\\right] \\mathbf{b}_r \\newline\n&= \\text{argmax}\\_{B} \\sum\\_{r=1}^{R} \\mathbf{b}_r^\\top C \\mathbf{b}_r,\n\\end{align}\\]\nwhere \\(C := \\text{Cov}[\\mathbf{x}]\\). Here we have used the centering assumption \\(\\mathbb{E}[\\mathbf{x}] = 0\\), as well as the implicit assumption that the random vector \\(\\mathbf{x}\\) has a well-defined covariance matrix. At this point, the derivations go through exactly as before, with \\(C\\) replacing the empirical covariance \\(\\hat{C}\\); that is, the optimal basis \\(B\\) is obtained by extracting the first \\(R\\) columns of \\(V\\), where \\(C = V\\Lambda V^\\top\\). Unsurprisingly, the results that held for the sample covariance hold analogously in this setting. For example, since \\(\\mathbf{x}\\) has zero mean then \\[\n\\mathbb{E}[\\mathbf{w}_r] = \\mathbb{E}[\\mathbf{v}_r^\\top \\mathbf{x}] = 0.\n\\] Moreover, \\[\n\\text{Cov}[\\mathbf{w}_r, \\mathbf{w}_s]\n= \\text{Cov}[\\mathbf{v}_r^\\top \\mathbf{x}, \\mathbf{v}_s^\\top \\mathbf{x}]\n= \\mathbf{v}_r^\\top C \\mathbf{v}_s\n= \\mathbf{v}_r^\\top V \\Lambda V^\\top \\mathbf{v}_s\n= \\lambda_r 1[r = s],\n\\] which says that the weights \\(\\mathbf{w}_r\\) are uncorrelated, with variance equal to their respective eigenvalues. It is common, therefore, to normalize these random variables to have unit variance \\[\n\\mathbf{\\tilde{w}}_r := \\frac{1}{\\sqrt{\\lambda_r}} \\mathbf{w}_r,\n\\] which means the optimal rank \\(R\\) approximation to the random vector \\(\\mathbf{x}\\) may be expressed as \\[\n\\mathbf{\\hat{x}}(\\omega) := \\sum_{r=1}^{R} \\sqrt{\\lambda_r} \\mathbf{\\tilde{w}}_r(\\omega) \\mathbf{v}_r,\n\\] where we have included the \\(\\omega\\) argument to emphasize which quantities are random here. The following result summarizes our findings, extending to the case with non-zero mean.\n\n\nProposition. Let \\(\\mathbf{x}\\) be a square-integrable \\(D\\)-dimensional random vector defined on \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\). Among all such random vectors constrained to take values in an \\(R\\)-dimensional subspace of \\(\\mathbb{R}^D\\), the random vector \\[\n  \\mathbf{\\hat{x}}(\\omega) = \\mathbb{E}[\\mathbf{x}] + \\sum_{r=1}^{R} \\sqrt{\\lambda_r} \\mathbf{\\tilde{w}}_r(\\omega) \\mathbf{v}_r, \\tag{15}\n  \\] provides an optimal approximation to \\(\\mathbf{x}\\), in the expected Euclidean distance sense (14). Moreover, the random weights \\(\\mathbf{\\tilde{w}}_r\\) are uncorrelated, have zero mean and unit variance.\n\n\nNote that random vectors can conceptually be thought of as stochastic processes with finite-dimensional index sets. A similar decomposition to (15) can be constructed for more general stochastic processes with potentially uncountable index sets, with the modification that the sum in (15) will require a countably infinite number of terms. This result is generally known as the Karhunen-Loeve expansion. Thus, from this perspective PCA can be thought of as the special finite-dimensional case of the Karhunen-Loeve expansion.\n\nGaussian Random Vectors\nThe above derivations did not make any assumptions regarding the distribution of \\(\\mathbf{x}\\), just that its covariance exists. Consequently, the main result (15) does not give the distribution of the weights \\(\\mathbf{\\tilde{w}}_r\\), only that they are uncorrelated, have zero mean, and unit variance. If we add the distributional assumption \\(\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{m}, C)\\) then we are able to say more. Indeed, recalling that the (unscaled) weights are given by projections \\(\\mathbf{w}_r = \\langle \\mathbf{x} - \\mathbf{m}, \\mathbf{v}_r \\rangle\\), then we find that \\[\n\\mathbf{w} = V^\\top (\\mathbf{x} - \\mathbf{m}) \\sim \\mathcal{N}(0, V^\\top C V).\n\\] The key point here are that the \\(\\mathbf{w}_r\\) are jointly Gaussian, and hence their uncorrelatedness implies that they are in fact independent. The weights inherit Gaussianity from \\(\\mathbf{x}\\). Thus, under this stronger assumption we are able to characterize the distribution of the weights exactly.\n\n\n\nMaximum Likelihood Estimation with Gaussian Noise\n\n\nProbabilistic PCA\nhttps://stats.stackexchange.com/questions/190308/why-does-probabilistic-pca-use-gaussian-prior-over-latent-variables"
  },
  {
    "objectID": "posts/pca.html#eigenvalue-problems",
    "href": "posts/pca.html#eigenvalue-problems",
    "title": "Principal Components Analysis",
    "section": "Eigenvalue Problems",
    "text": "Eigenvalue Problems\nIn this section, I briefly discuss the spectral norm and eigenvalue problems in finite-dimensional vector spaces, which I utilize above when optimizing the basis \\(B\\) in the PCA derivation. Consider a matrix \\(A \\in \\mathbb{R}^{N \\times D}\\), which represents a linear transformation from \\(\\mathbb{R}^{D}\\) to \\(\\mathbb{R}^N\\). We define the spectral norm of \\(A\\) as the largest factor by which the map \\(A\\) can “stretch” a vector \\(\\mathbf{u} \\in \\mathbb{R}^{D}\\), \\[\n\\lVert A \\rVert_2 := \\max_{\\lVert \\mathbf{u} \\rVert_2 \\neq \\boldsymbol{0}}\n\\frac{\\lVert A\\mathbf{u} \\rVert_2}{\\lVert \\mathbf{u} \\rVert_2}.\n\\] Using the linearity of \\(A\\), one can show that we need only consider vectors of unit length; that is, \\[\n\\lVert A \\rVert_2 = \\max_{\\lVert \\mathbf{u}_2 \\rVert=1} \\lVert A\\mathbf{u} \\rVert_2. \\tag{A1}\n\\] Optimization problems of this type are called eigenvalue problems for reasons that will shortly become clear. For the purposes of the PCA derivation, we will require consideration of a slightly more general eigenvalue problem. To define this problem, first note that the matrix \\(A^\\top A\\) is symmetric, positive semi-definite since \\[\\begin{align}\n&(A^\\top A)^\\top = A^\\top (A^\\top)^\\top = A^\\top A,\n&\\mathbf{u}^\\top (A^\\top A)\\mathbf{u} = \\lVert A \\mathbf{u} \\rVert_2^2 \\geq 0.\n\\end{align}\\] Thus, by the spectral theorem \\(A^\\top A\\) has \\(D\\) orthogonal eigenvectors, which we will denote by \\(\\mathbf{e}_1, \\dots, \\mathbf{e}_D\\) and assume that they have been normalized to have unit norm. By positive definiteness the respective eigenvalues \\(\\lambda_1, \\dots, \\lambda_D\\) (sorted in decreasing order) are all non-negative. For \\(d = 1, \\dots, D\\), let \\(\\mathcal{E}_d := \\text{span}(\\mathbf{e}_1, \\dots, \\mathbf{e}_d)\\), with \\(\\mathcal{E}^{\\perp}_d := \\text{span}(\\mathbf{e}_{d+1}, \\dots, \\mathbf{e}_D)\\) its orthogonal complement. We now consider the eigenvalue problem\n\\[\\begin{align}\n\\max_{\\mathbf{u} \\in \\mathcal{E}^{\\perp}_d, \\lVert \\mathbf{u} \\rVert_2=1} \\lVert A\\mathbf{u} \\rVert_2. \\tag{A2}\n\\end{align}\\]\nWe have generalized (A1) by adding an orthogonality constraint. Taking as a convention \\(\\mathcal{E}_0 := \\{\\mathbf{0}\\}\\) we then have \\(\\mathcal{E}^{\\perp}_0 = \\mathbb{R}^D\\), which means that setting \\(d = 0\\) in (A2) recovers the original problem (A1) as a special case. We will prove the following result.\n\n\nProposition. Let \\(A \\in \\mathbb{R}^{N \\times D}\\) be a matrix. Let \\(\\mathbf{e}_1, \\dots, \\mathbf{e}_D\\) denote the orthonormal eigenbasis of \\(A^\\top A\\), with respective eigenvalues \\(\\lambda_1, \\dots, \\lambda_D\\) sorted in descending order by magnitude. For \\(d = 1, \\dots, D\\) define \\(\\mathcal{E}_d := \\text{span}(\\mathbf{e}_1, \\dots, \\mathbf{e}_d)\\), with \\(\\mathcal{E}^{\\perp}_d := \\text{span}(\\mathbf{e}_{d+1}, \\dots, \\mathbf{e}_D)\\) its orthogonal complement. Then \\[\n  \\mathbf{e}_{d+1} = \\text{argmax}_{\\mathbf{u} \\in \\mathcal{E}^{\\perp}_d, \\mathbf{u}=1} \\lVert A\\mathbf{u} \\rVert_2. \\tag{A3}\n  \\] with the maximal value equal to \\(\\sqrt{\\lambda_{d+1}}\\). In particular, we have \\[\n  \\mathbf{e}_1 = \\text{argmax}_{\\lVert \\mathbf{u} \\rVert_2=1} \\lVert A\\mathbf{u} \\rVert_2, \\tag{A4}\n  \\] with maximal value \\(\\sqrt{\\lambda_1}\\).\n\n\nProof. Let \\(\\mathbf{u} \\in \\mathcal{E}^{\\perp}_d\\) be an arbitrary vector of unit length. This vector may be represented with respect to the eigenbasis as \\[\n\\mathbf{u} = \\sum_{r=d+1}^{D} u_r \\mathbf{e}_r, \\qquad u_r := \\langle \\mathbf{u}, \\mathbf{e}_r \\rangle\n\\] We will use this representation to show that 1. \\(\\lVert A\\mathbf{u} \\rVert_2\\) is upper bounded by \\(\\sqrt{\\lambda_{d+1}}\\). 2. The upper bound is achieved by some \\(\\mathbf{u} \\in \\mathcal{E}^{\\perp}_d\\),\nwhich together imply the claimed result. We will actually work with the squared norm instead, which allows us to leverage the inner product. We have \\[\\begin{align}\n\\lVert A\\mathbf{u} \\rVert^2_2\n= \\langle A\\mathbf{u}, A\\mathbf{u} \\rangle\n= \\langle A^\\top A\\mathbf{u}, \\mathbf{u} \\rangle\n&= \\left\\langle A^\\top A \\sum\\_{r=d+1}^{D} u_r \\mathbf{e}_r,\n\\sum\\_{r=d+1}^{D} u_r \\mathbf{e}_r \\right\\rangle \\newline\n&= \\left\\langle \\sum\\_{r=d+1}^{D} u_r (A^\\top A \\mathbf{e}_r),\n\\sum\\_{r=d+1}^{D} u_r \\mathbf{e}_r \\right\\rangle \\newline\n&= \\left\\langle \\sum\\_{r=d+1}^{D} u_r \\lambda_r \\mathbf{e}_r,\n\\sum\\_{r=d+1}^{D} u_r \\mathbf{e}_r \\right\\rangle,\n\\end{align}\\] having used the fact the \\(\\mathbf{e}_r\\) are eigenvectors of \\(A^\\top A\\). Now we can take advantage of the fact that the \\(\\mathbf{e}_r\\) are orthonormal to obtain \\[\\begin{align}\n\\left\\langle \\sum\\_{r=d+1}^{D} u_r \\lambda_r \\mathbf{e}_r,\n\\sum\\_{r=d+1}^{D} u_r \\mathbf{e}_r \\right\\rangle\n= \\sum\\_{r=d+1}^{D} u_r^2 \\lambda_r \\lVert \\mathbf{e}_r \\rVert^2_2\n= \\sum\\_{r=d+1}^{D} u_r^2 \\lambda_r\n\\leq \\sum\\_{r=d+1}^{D} u_r^2 \\lambda\\_{d+1}\n= \\lambda\\_{d+1} \\lVert \\mathbf{u} \\rVert_2^2\n= \\lambda\\_{d+1}\n\\end{align}\\] where the inequality follows from the fact that the eigenvalues are sorted in descending order. This verifies the upper bound \\(\\lVert A\\mathbf{u} \\rVert_2 \\leq \\sqrt{\\lambda_{d+1}}\\). To show that the bound is achieved, we consider setting \\(\\mathbf{u} = \\mathbf{e}_{d+1}\\). Then,\n\\[\\begin{align}\n\\lVert A\\mathbf{e}\\_{d+1} \\rVert^2_2\n= \\langle A\\mathbf{e}\\_{d+1}, A\\mathbf{e}\\_{d+1} \\rangle\n= \\langle A^\\top A\\mathbf{e}\\_{d+1}, \\mathbf{e}\\_{d+1} \\rangle\n= \\langle \\lambda\\_{d+1} \\mathbf{e}\\_{d+1}, \\mathbf{e}\\_{d+1} \\rangle\n= \\lambda\\_{d+1} \\lVert \\mathbf{e}\\_{d+1} \\rVert^2_2\n= \\lambda\\_{d+1},\n\\end{align}\\]\nso we have indeed verified that the equality \\(\\lVert A\\mathbf{u} \\rVert_2 = \\sqrt{\\lambda_{d+1}}\\) is achieved for some unit-norm vector \\(\\mathbf{u} \\in \\mathcal{E}^{\\perp}_{d}\\). The claim is thus proved. \\(\\qquad \\blacksquare\\)"
  },
  {
    "objectID": "posts/pca.html#projections",
    "href": "posts/pca.html#projections",
    "title": "Principal Components Analysis",
    "section": "Projections",
    "text": "Projections"
  },
  {
    "objectID": "posts/pca.html#truncated-svd-and-eigendecomposition",
    "href": "posts/pca.html#truncated-svd-and-eigendecomposition",
    "title": "Principal Components Analysis",
    "section": "Truncated SVD and Eigendecomposition",
    "text": "Truncated SVD and Eigendecomposition"
  },
  {
    "objectID": "posts/gp-specifications.html",
    "href": "posts/gp-specifications.html",
    "title": "Introduction to Gaussian Process Priors and Hyperparameter Estimation",
    "section": "",
    "text": "Gaussian processes (GP) are widely utilized across various fields, each with their own preferences, terminology, and conventions. Some notable domains that make significant use of GPs include - Spatial statistics (kriging) - Design and analysis of computer experiments (emulator/surrogate modeling) - Bayesian optimization - Machine learning\nEven if you’re a GP expert in one of these domains, these differences can make navigating the GP literature in other domains a bit tricky. The goal of this post is to summarize common approaches for specifying GP distributions, and emphasize conventions and assumptions that tend to differ across fields. By “specifying GP distributions”, what I am really talking about here is parameterizing the mean and covariance functions that define the GP. While GPs are non-parametric models in a certain sense, specifying and learning the hyperparameters making up the mean and covariance functions is a crucial step to successful GP applications. I will discuss popular parameterizations for these functions, and different algorithms for learning these parameter values from data. In the spirit of drawing connections across different domains, I will try my best to borrow terminology from different fields, and will draw attention to synonymous terms by using boldface."
  },
  {
    "objectID": "posts/gp-specifications.html#background",
    "href": "posts/gp-specifications.html#background",
    "title": "Introduction to Gaussian Process Priors and Hyperparameter Estimation",
    "section": "Background",
    "text": "Background\n\nGaussian Processes\nGaussian processes (GPs) define a probability distribution over a space of functions in such a way that they can be viewed as a generalization of Gaussian random vectors. Just as Gaussian vectors are defined by their mean vector and covariance matrix, GPs are defined by a mean and covariance function. We will interchangeably refer to the latter as either the covariance function or kernel.\nWe will consider GPs defined over a space of functions of the form \\(f: \\mathcal{X} \\to \\mathbb{R}\\), where \\(\\mathcal{X} \\subseteq \\mathbb{R}^d\\). We will refer to elements \\(x \\in \\mathcal{X}\\) as inputs or locations and the images \\(f(x) \\in \\mathbb{R}\\) as outputs or responses. If the use of the word “locations” seems odd, note that in spatial statistical settings, the inputs \\(x\\) are often geographic coordinates. We will denote the mean and covariance function defining the GP by \\(\\mu: \\mathcal{X} \\to \\mathbb{R}\\) and \\(k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\), respectively. The mean function is essentially unrestricted, but the covariance function \\(k(\\cdot, \\cdot)\\) must be a valid positive definite kernel. If \\(f(\\cdot)\\) is a GP with mean function \\(\\mu(\\cdot)\\) and kernel \\(k(\\cdot, \\cdot)\\) we will denote this by \\[\n\\begin{align}\nf \\sim \\mathcal{GP}(\\mu, k). \\tag{1}\n\\end{align}\n\\]\nThe defining property of GPs is that their finite-dimensional distributions are Gaussian; that is, for an arbitrary finite set of \\(n\\) inputs \\(X := \\{x_1, \\dots, x_N\\} \\subset \\mathcal{X}\\), the vector \\(f(X) \\in \\mathbb{R}^n\\) is distributed as \\[\n\\begin{align}\nf(X) \\sim \\mathcal{N}(\\mu(X), k(X, X)). \\tag{2}\n\\end{align}\n\\] We are vectorizing notation here so that \\([f(X)]_i := f(x_i)\\), \\([\\mu(X)]_i := \\mu(x_i)\\), and \\([k(X, X)]_{i,j} := k(x_i, x_j)\\). When the two input sets to the kernel are equal, we lighten notation by writing \\(k(X) := k(X, X)\\). Now suppose we have two sets of inputs \\(X\\) and \\(\\tilde{X}\\), containing \\(n\\) and \\(m\\) inputs, respectively. The defining property (2) then implies\n\\[\n\\begin{align}\n\\begin{bmatrix} f(\\tilde{X}) \\newline f(X) \\end{bmatrix}\n&\\sim \\mathcal{N}\\left(\n  \\begin{bmatrix} \\mu(\\tilde{X}) \\newline \\mu(X) \\end{bmatrix},\n  \\begin{bmatrix}\n  k(\\tilde{X}) & k(\\tilde{X}, X) \\newline\n  k(X, \\tilde{X}) & k(X)\n  \\end{bmatrix}\n\\right). \\tag{3}\n\\end{align}\n\\]\nThe Gaussian joint distribution (3) implies that the conditional distributions are also Gaussian. In particular, the distribution of \\(f(\\tilde{X})|f(X)\\) can be obtained by applying the well-known Gaussian conditioning identities:\n\\[\n\\begin{align}\nf(\\tilde{X})|f(X) &\\sim \\mathcal{N}(\\hat{\\mu}(\\tilde{X}), \\hat{k}(\\tilde{X})), \\tag{4} \\newline\n\\hat{\\mu}(\\tilde{X}) &:= \\mu(\\tilde{X}) + k(\\tilde{X}, X)k(X)^{-1} [f(X) - \\mu(X)] \\newline\n\\hat{k}(\\tilde{X}) &:= k(\\tilde{X}) - k(\\tilde{X}, X)k(X)^{-1} k(X, \\tilde{X}).\n\\end{align}\n\\] The fact that the result (4) holds for arbitrary finite sets of inputs \\(\\tilde{X}\\) implies that the conditional \\(f | f(X)\\) is also a GP, with mean and covariance functions \\(\\hat{\\mu}(\\cdot)\\) and \\(\\hat{k}(\\cdot, \\cdot)\\) defined by (4). On a terminology note, the \\(n \\times n\\) matrix \\(k(X)\\) is often called the kernel matrix. This is the matrix containing the kernel evaluations at the set of \\(n\\) observed locations.\n\n\nRegression with GPs\nOne common application of GPs is their use as a flexible nonlinear regression model. Let’s consider the basic regression setup with observed data pairs \\((x_1, y_1), \\dots, (x_n, y_n)\\). We assume that the \\(y_i\\) are noisy observations of some underlying latent function output \\(f(x_i)\\). The GP regression model arises by placing a GP prior distribution on the latent function \\(f\\). We thus consider the regression model \\[\n\\begin{align}\ny(x) &= f(x) + \\epsilon(x) \\tag{5} \\newline\nf &\\sim \\mathcal{GP}(\\mu, k) \\newline\n\\epsilon(x) &\\overset{iid}{\\sim} \\mathcal{N}(0, \\sigma^2),\n\\end{align}\n\\] where we have assumed a simple additive Gaussian noise model. This assumption is quite common in the GP regression setting due to the fact that it results in closed-form conditional distributions, similar to (4). We will assume the error model (5) throughout this post, but note that there are many other possibilities if one is willing to abandon closed-form posterior inference.\nThe solution of the regression problem is given by the distribution of \\(f(\\cdot)|y(X)\\) or \\(y(\\cdot)|y(X)\\), where \\(y(X)\\) is the \\(n\\)-dimensional vector of observed responses. The first distribution is the posterior on the latent function \\(f\\), while the second incorporates the observation noise as well. Both distributions can be derived in the same way, so we focus on the second. Letting \\(\\tilde{X}\\) denote a set of \\(m\\) inputs at which we would like to predict the response, consider the joint distribution \\[\n\\begin{align}\n\\begin{bmatrix} y(\\tilde{X}) \\newline y(X) \\end{bmatrix}\n&\\sim \\mathcal{N}\\left(\n  \\begin{bmatrix} \\mu(\\tilde{X}) \\newline \\mu(X) \\end{bmatrix},\n  \\begin{bmatrix}\n  k(\\tilde{X}) + \\sigma^2 I_m & k(\\tilde{X}, X) \\newline\n  k(X, \\tilde{X}) & k(X) + \\sigma^2 I_n\n  \\end{bmatrix}\n\\right). \\tag{6}\n\\end{align}\n\\] This is quite similar to (3), but now takes into account the noise term \\(\\epsilon\\). This does not affect the mean vector since \\(\\epsilon\\) is mean-zero; nor does it affect the off-diagonal elements of the covariance matrix since \\(\\epsilon\\) and \\(f\\) were assumed independent. Applying the Gaussian conditioning identities (4) yields the posterior distribution \\[\n\\begin{align}\ny(\\tilde{X})|y(X) &\\sim \\mathcal{N}(\\hat{\\mu}(\\tilde{X}), \\hat{k}(\\tilde{X})), \\tag{7} \\newline\n\\hat{\\mu}(\\tilde{X}) &:= \\mu(\\tilde{X}) + k(\\tilde{X}, X)[k(X) + \\sigma^2 I_n]^{-1} [f(X) - \\mu(X)] \\newline\n\\hat{k}(\\tilde{X}) &:= \\sigma^2 I_m + k(\\tilde{X}) - k(\\tilde{X}, X)[k(X) + \\sigma^2 I_n]^{-1} k(X, \\tilde{X}).\n\\end{align}\n\\] We will refer to (7) as the GP posterior, predictive, or generically conditional, distribution. We observe that these equations are identical to (4), modulo the appearance of \\(\\sigma^2\\) in the predictive mean and covariance equations. The distribution \\(f(\\tilde{X})|y(X)\\) is identical to (7), except that the \\(\\sigma^2 I_m\\) is removed in the predictive covariance. Again, this reflects the subtle distinction between doing inference on the latent function \\(f\\) versus on the observation process \\(y\\).\n\n\nNoise, Nuggets, and Jitters\nObserve that this whole regression procedure is only slightly different from the noiseless GP setting explored in the previous section (thanks to the Gaussian likelihood assumption). Indeed, the conditional distribution of \\(f(\\tilde{X})|y(X)\\) is derived from \\(f(\\tilde{X})|f(X)\\) by simply replacing \\(k(X)\\) with \\(k(X) + \\sigma^2 I_n\\) (obtaining the distribution \\(y(\\tilde{X})|y(X)\\) requires the one additional step of adding \\(\\sigma^2 I_m\\) to the predictive covariance). In other words, we have simply applied standard GP conditioning using the modified kernel matrix \\[\n\\begin{align}\nC(X) := k(X) + \\sigma^2 I_n. \\tag{8}\n\\end{align}\n\\] We thus might reasonably wonder if the model (5) admits an alternative equivalent representation by defining a GP directly on the observation process \\(y\\). Defining such a model would require defining a kernel \\(c: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\) that is consistent with (8). This route is fraught with difficulties and subtleties, which I will do my best to describe clearly here. At first glance, it seems like the right choice is \\[\n\\begin{align}\nc(x, x^\\prime) := k(x, x^\\prime) + \\sigma^2 \\delta(x, x^\\prime), \\tag{9}\n\\end{align}\n\\] where \\(\\delta(x, x^\\prime) := 1[x = x^\\prime]\\) is sometimes called the stationary white noise kernel. Why isn’t this quite right? Notice in (9) that \\(\\sigma^2\\) is added whenever the inputs \\(x = x^\\prime\\) are equal. However, suppose we observe multiple independent realizations of the process at the same inputs \\(X\\). In the regression model (9) the errors \\(\\epsilon(x)\\) are independent across these realizations, even at the same locations. However, this will not hold true in the model under (9), since \\(\\delta(x, x^\\prime)\\) only sees the values of the inputs, and has no sense of distinction across realizations. We might try to fix this by writing something like \\[\n\\begin{align}\nc(x_i, x_j) := k(x_i, x_j) + \\sigma^2 \\delta_{ij}, \\tag{10}\n\\end{align}\n\\] where the Delta function now depends on the labels \\(i, j\\) instead of the values of the inputs. In the spatial statistics literature, it is not uncommon to see a covariance function defined like (10), but this is basically a notational hack. A kernel is a function of two inputs from \\(\\mathcal{X}\\) - we can’t have it also depending on some side information like the labels \\(i, j\\). At the end of the day, (9) and (10) are attempts to incorporate some concept of white noise inside the kernel itself, rather than via a hierarchical model like (5). I would just stick with the hierarchical model, which is easily rigorously defined and much more intuitive.\nNonetheless, one should not be surprised if expressions like (10) pop up, especially in the spatial statistics literature. Spatial statisticians refer to the noise term \\(\\epsilon(x)\\) as the nugget, and \\(\\sigma^2\\) the nugget variance (sometimes these terms are conflated). In this context, instead of representing observation noise, \\(\\sigma^2\\) is often thought of as representing some unresolved small-scale randomness in the spatial field itself. If you imagine sampling a field to determine the concentration of some mineral across space, then you would hope that repeated measurements (taken around the same time) would yield the same values. Naturally, they may not, and the introduction of the nugget is one way to account for this.\nWhile this discussion may seem needlessly abstract, we recall that the effect of incorporating the noise term (however you want to interpret it) is to simply replace the kernel matrix \\(k(X)\\) with the new matrix \\(c(X) = k(X) + \\sigma^2 I_n\\). Confusingly, there is one more reason (having nothing to do with observation error or nuggets) that people use a matrix of the form \\(c(X)\\) in place of \\(k(X)\\): numerical stability. Indeed, even though \\(k(X)\\) is theoretically positive definite, in practice its numerical instantiation may fail to have this property. A simple approach to deal with this is to add a small, fixed constant \\(\\sigma^2\\) to the diagonal of the kernel matrix. In this context, \\(\\sigma^2\\) is often called the jitter. While computationally its effect is the same as the nugget, note that its introduction is motivated very differently. The jitter is not stemming from some sort of random white noise; it is purely a computational hack to improve the conditioning of the kernel matrix. Check out this thread for some entertaining debates on the use of the nugget and jitter concepts.\n\n\nParameterized Means and Kernels\nEverything we have discussed this far assumes fixed mean and covariance functions. In practice, suitable choices for these quantities are not typically known. Thus, the usual approach is to specify some parametric families \\(\\mu = \\mu_{\\psi}\\) and \\(k = k_{\\phi}\\) and learn their parameters from data. The parameters \\(\\psi\\) and \\(\\phi\\) are often referred to as hyperparameters, since they are not the primary parameters of interest in the GP regression model. Recalling from (5) that the GP acts as a prior distribution on the latent function, we see that \\(\\psi\\) and \\(\\phi\\) control the specification of this prior distribution. In addition to \\(\\psi\\) and \\(\\phi\\), the parameter \\(\\sigma^2\\) is also typically not known. I will not wade back into the previous section’s debate in arguing whether this should be classified as a “hyperparameter” or not. In any case, let’s let \\(\\theta := \\{\\psi, \\phi, \\sigma^2 \\}\\) denote the full set of (hyper)parameters that must be learned from data.\n\nMean Functions\nThe machine learning community commonly uses the simplest possible form for the mean function: \\(\\mu(x) \\equiv 0\\). This zero-mean assumption is less restrictive than it seems, since GPs mainly derive their expressivity from the kernel. A slight generalization is to allow a constant, non-zero mean \\(\\mu(x) \\equiv \\beta_0\\), where \\(\\beta_0 \\in \\mathbb{R}\\). However, constant (including zero-mean) GP priors can have some undesirable properties; e.g., in the context of extrapolation. Sometimes one wants more flexibility, and in these cases it is quite common to consider some sort of linear regression model \\[\n\\begin{align}\n\\mu(x) = h(x)^\\top \\beta, \\tag{11}\n\\end{align}\n\\] where \\(h: \\mathcal{X} \\to \\mathbb{R}^p\\) is some feature map and \\(\\beta \\in \\mathbb{R}^p\\) the associated coefficient vector. For example, \\(h(x) = [1, x^\\top]^\\top\\) would yield a standard linear model, and \\(h(x) = [1, x_1, \\dots, x_d, x_1^2, \\dots, x_d^2]^\\top\\) would allow for a quadratic trend.\n\n\nKernels\nThe positive definite restriction makes defining valid covariance functions much more difficult than defining mean functions. Thus, one typically falls back on one of a few popular choices of known parametric kernel families (though note that kernels can be combined in various ways to give a large variety of options). While the goal of this post is not to explore specific kernels, in order to have a concrete example in mind consider the following parameterization: \\[\n\\begin{align}\nk(x, \\tilde{x}) = \\alpha^2 \\sum_{j=1}^{d} \\left(-\\frac{\\lvert x^{j} - \\tilde{x}^j \\rvert}{\\ell^j}\\right)^2.\n\\tag{12}\n\\end{align}\n\\] Note that I’m using superscripts to index vector entries here. This kernel goes by many names, including exponentiated quadratic, squared exponential, Gaussian, radial basis function, and automatic relevance determination. The parameter \\(\\alpha^2\\) is sometimes called the marginal variance, or just the scale parameter. The parameters \\(\\ell^1, \\dots, \\ell^d\\) are often called lengthscale, smoothness, or range parameters, since they control the smoothness of the GP realizations along each coordinate direction. Other popular kernels (e.g., Matérn) have analogous parameters controlling similar features. Note that in this example we have \\(\\phi = \\{\\alpha^2, \\ell^1, \\dots, \\ell^d \\}\\). Also note that people choose to parameterize the Gaussian kernel in many different ways; for example, it’s not uncommon to see a \\(1/2\\) factor included inside the exponential to make the kernel align with the typical parameterization of the Gaussian probability density function. Knowing which parameterization you’re working with is important for interpreting the hyperparameters, specifying bounds, defining priors, etc.\nIt is quite common in the spatial statistics (and sometimes the computer experiments) literature to see kernels written like \\(\\alpha^2 k(\\cdot, \\cdot)\\); in these cases \\(k(\\cdot, \\cdot)\\) typically represents a correlation function, which becomes the covariance function after multiplying by the marginal variance \\(\\alpha^2\\). There is an advantage in decomposing the kernel this way when it comes to estimating the hyperparameters, which we will discuss shortly.\n\n\n\nThe GP (Marginal) Likelihood Function\nLet’s first recall the GP regression model (5) \\[\n\\begin{align}\ny(x) &= f(x) + \\epsilon(x) \\newline\nf &\\sim \\mathcal{GP}(\\mu_{\\psi}, k_{\\phi}) \\newline\n\\epsilon &\\overset{iid}{\\sim} \\mathcal{N}(0, \\sigma^2),\n\\end{align}\n\\] where we have now explicitly added the dependence on \\(\\psi\\) and \\(\\phi\\). This model is defined for any \\(x \\in \\mathcal{X}\\). However, when estimating hyperparameters, we will naturally be restricting the model to \\(X\\), the finite set of locations at which we actually have observations. Restricting to \\(X\\) reduces the above model to the standard (finite-dimensional) Bayesian regression model \\[\n\\begin{align}\ny(X)|f(X), \\theta &\\sim \\mathcal{N}(f(X), \\sigma^2 I_n) \\tag{13} \\newline\nf(X)|\\theta &\\sim \\mathcal{N}(\\mu_{\\psi}(X), k_{\\phi}(X)).\n\\end{align}\n\\] We could consider completing the Bayesian specification by defining a prior on \\(\\theta\\), but we’ll hold off on this for now. Notice that the model (13) defines a joint distribution over \\([y(X), f(X)] | \\theta\\), with \\(y(X)|f(X), \\theta\\) representing the likelihood of the observations at the observed input locations \\(X\\). At present everything is conditional on a fixed \\(\\theta\\). Now, if we marginalize the likelihood \\(y(X)|f(X), \\theta\\) with respect to \\(f(X)\\) then we obtain the distribution \\(y(X) | \\theta\\). This is often called the marginal likelihood, due to the fact that \\(f\\) was marginalized out. In particular, the distribution \\(y(X) | \\theta\\) has implicitly marginalized the function values \\(f(\\tilde{x})\\) at all location \\(\\tilde{x}\\) other than \\(X\\). This same logic and terminology applies in the noiseless setting with \\(\\sigma^2 = 0\\), in which case the marginal likelihood is given by \\(f(X) | \\theta\\). In the noiseless setting we are marginalizing both over the unobserved function values and the noise \\(\\epsilon\\). Thanks to all the Gaussian assumptions here, the marginal likelihood is available in closed-form. One could approach the derivation using (13) as the starting point, but it’s much easier to consider the model written out using random variables, \\[\n\\begin{align}\ny(X) &= f(X) + \\epsilon(X).\n\\end{align}\n\\] Since \\(f(X)\\) and \\(\\epsilon(X)\\) are independent Gaussians, then their sum is also Gaussian with mean and covariance given by \\[\n\\begin{align}\n\\mathbb{E}[y(X)|\\theta]\n&= \\mathbb{E}[f(X)|\\theta] + \\mathbb{E}[\\epsilon(X)|\\theta] = \\mu_{\\psi}(X) \\newline\n\\text{Cov}[y(X)|\\theta]\n&= \\text{Cov}[f(X)|\\theta] + \\text{Cov}[\\epsilon(X)|\\theta]\n= k_{\\phi}(X) + \\sigma^2 I_n.\n\\end{align}\n\\] We have thus found that \\[\n\\begin{align}\ny(X)|\\theta \\sim \\mathcal{N}\\left(\\mu_{\\psi}(X), C_{\\phi, \\sigma^2}(X)\\right), \\tag{14}\n\\end{align}\n\\] recalling the definition \\(C_{\\phi, \\sigma^2}(X) := k_{\\phi}(X) + \\sigma^2 I_n\\). We will let \\(\\mathcal{L}(\\theta)\\) denote the log density of this Gaussian distribution; i.e. the log marginal likelihood: \\[\n\\begin{align}\n\\mathcal{L}(\\theta)\n&:= -\\frac{1}{2} \\log \\text{det}\\left(2\\pi C_{\\phi, \\sigma^2}(X) \\right) -\n\\frac{1}{2} (y(X) - \\mu_{\\psi}(X))^\\top C_{\\phi, \\sigma^2}(X)^{-1} (y(X) - \\mu_{\\psi}(X)) \\tag{15}\n\\end{align}\n\\] The function \\(\\mathcal{L}(\\theta)\\) plays a central role in the typical approach to hyperparameter optimization, as we will explore below. Also note that the above derivations also apply to the noiseless setting (i.e., \\(y(X) = f(X)\\)) by setting \\(\\sigma^2 = 0\\). In this case, the marginal likelihood is simply the GP distribution restricted to the inputs \\(X\\).\nI have henceforth been a bit verbose with the notation in (15) to make very explicit the dependence on the inputs \\(X\\) and the hyperparameters. To lighten notation a bit, we define \\(y_n := y(X)\\), \\(\\mu_{\\psi} := \\mu_{\\psi}(X)\\), and \\(C_{\\phi, \\sigma^2} := C_{\\phi, \\sigma^2}(X)\\), allowing us to rewrite (15) as \\[\n\\begin{align}\n\\mathcal{L}(\\theta)\n&:= -\\frac{1}{2} \\log \\text{det}\\left(2\\pi C_{\\phi, \\sigma^2} \\right) -\n\\frac{1}{2} (y_n - \\mu_{\\psi})^\\top C_{\\phi, \\sigma^2}^{-1} (y_n - \\mu_{\\psi}). \\tag{16}\n\\end{align}\n\\] We have simply suppressed the explicit dependence on \\(X\\) in the notation."
  },
  {
    "objectID": "posts/gp-specifications.html#maximum-marginal-likelihood-or-empirical-bayes",
    "href": "posts/gp-specifications.html#maximum-marginal-likelihood-or-empirical-bayes",
    "title": "Introduction to Gaussian Process Priors and Hyperparameter Estimation",
    "section": "Maximum Marginal Likelihood, or Empirical Bayes",
    "text": "Maximum Marginal Likelihood, or Empirical Bayes\nRecall that (16) gives the expression for the log marginal likelihood \\(\\mathcal{L}(\\theta)\\), which is just the log density of \\(y(X)|\\theta\\) viewed as a function of \\(\\theta\\). A natural approach is to set the hyperparameters \\(\\theta\\) to their values that maximize \\(\\mathcal{L}(\\theta)\\): \\[\n\\begin{align}\n\\hat{\\theta} := \\text{argmax} \\ \\mathcal{L}(\\theta). \\tag{17}\n\\end{align}\n\\] At first glance, the Gaussian form of \\(\\mathcal{L}(\\theta)\\) might look quite friendly to closed-form optimization. After all, maximum likelihood estimates of the mean and covariance of Gaussian vectors are indeed available in closed-form. However, upon closer inspection notice that the covariance is not being directly optimized; we are optimizing \\(\\phi\\), and the covariance \\(C_{\\phi, \\sigma^2}\\) is a nonlinear function of this parameter. Thus, in general some sort of iterative numerical scheme is is used for the optimization. Typically, gradient-based approaches are preferred, meaning we must be able to calculate quantities like \\(\\frac{\\partial}{\\partial \\phi} C_{\\phi, \\sigma^2}\\). The exact gradient calculations will thus depend on the choice of kernel; specifics on kernels and optimization schemes are not the focus of this post. We will instead focus on the high level ideas here. The general approach to GP regression that we have outlined so far can be summarized as: 1. Solve the optimization problem (17) and fix the hyperparameters at their optimized values \\(\\hat{\\theta}\\). The hyperparameters will be fixed from this point onward. 2. Use the GP predictive equations (7) to perform inference at a set of locations of interest \\(\\tilde{X}\\).\nOne might object to the fact that we are estimating the hyperparameters from data, and then neglecting the uncertainty in \\(\\hat{\\theta}\\) during the prediction step. It is true that this uncertainty is being ignored, but it is also very computationally convenient to do so. We will discuss alternatives later on, but this simple approach is probably the most commonly used in practice today. One way to think about this strategy is in an empirical Bayes context; that is, we can view this approach as an approximation to a fully Bayesian hierarchical model, which would involve equipping the hyperparameters with their own priors. Instead of marginalizing the hyperparameters, we instead fix them at their most likely values with respect to the observed data. We are using the data to “fine tune” the GP prior distribution. In the literature you will see this general hyperparameter optimization strategy referred to as either empirical Bayes, maximum marginal likelihood, or even just maximum likelihood."
  },
  {
    "objectID": "posts/gp-specifications.html#special-case-closed-form-solutions-mean-function",
    "href": "posts/gp-specifications.html#special-case-closed-form-solutions-mean-function",
    "title": "Introduction to Gaussian Process Priors and Hyperparameter Estimation",
    "section": "Special Case Closed-Form Solutions: Mean Function",
    "text": "Special Case Closed-Form Solutions: Mean Function\nAs mentioned above, in general the maximization of \\(\\mathcal{L}(\\theta)\\) requires numerical methods. However, in certain cases elements of \\(\\theta\\) can be optimized in closed-form, meaning that numerical optimization may only be required for a subset of the hyperparameters. We start by considering closed form optimizers for the parameters defining the mean functions.\n\nConstant Mean: Plug-In MLE\nWith the choice of constant mean \\(\\mu_{\\psi}(x) \\equiv \\beta_0\\) the log marginal likelihood becomes\n\\[\n\\begin{align}\n\\mathcal{L}(\\theta)\n&:= -\\frac{1}{2} \\log \\text{det}\\left(2\\pi C_{\\phi, \\sigma^2} \\right) -\n\\frac{1}{2} (y_n - \\beta_0 1_n)^\\top C_{\\phi, \\sigma^2}(X)^{-1} (y_n - \\beta_0 1_n),\n\\end{align}\n\\]\nwith \\(1_n \\in \\mathbb{R}^n\\) denoting a vector of ones. We now consider optimizing \\(\\mathcal{L}(\\theta)\\) as a function of \\(\\beta_0\\) only. The partial derivative with respect to the constant mean equals \\[\n\\begin{align}\n\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\beta_0}\n&= y_n^\\top C_{\\phi, \\sigma^2}^{-1}1_n - \\beta_0 1_n^\\top C_{\\phi, \\sigma^2}^{-1} 1_n.    \\tag{18}\n\\end{align}\n\\] Setting (18) equal to zero and solving for \\(\\beta_0\\) gives the optimum \\[\n\\begin{align}\n\\hat{\\beta}_0(\\phi, \\sigma^2) = \\frac{y_n^\\top C_{\\phi, \\sigma^2}^{-1} 1_n}{1_n^\\top C_{\\phi, \\sigma^2}^{-1} 1_n}. \\tag{19}\n\\end{align}\n\\] Notice that \\(\\hat{\\beta}_0(\\phi, \\sigma^2)\\) depends on the values of the other hyperparameters \\(\\phi\\) and \\(\\sigma^2\\). Therefore, while this does not give us the outright value for the mean, we can plug \\(\\hat{\\beta}_0(\\phi, \\sigma^2)\\) in place of \\(\\beta_0\\) in the marginal likelihood. This yields the profile likelihood (aka the concentrated likelihood), which is no longer a function of \\(\\beta_0\\) and hence the dimensionality of the subsequent numerical optimization problem has been reduced.\n\n\nLinear Model Coefficients: Plug-In MLE\nLet’s try to do the same thing with the mean function \\(\\mu_{\\psi}(x) = h(x)^\\top \\beta\\). The constant mean function is actually just a special case of this more general setting, but its common enough that it warranted its own section. If we denote by \\(H \\in \\mathbb{R}^{n \\times p}\\) the feature matrix with rows equal to \\(h(x_i)^\\top\\), \\(i = 1, \\dots, n\\) then the marginal likelihood becomes \\[\n\\begin{align}\n\\mathcal{L}(\\theta)\n&:= -\\frac{1}{2} \\log \\text{det}\\left(2\\pi C_{\\phi, \\sigma^2} \\right) -\n\\frac{1}{2} (y_n - H\\beta)^\\top C_{\\phi, \\sigma^2}^{-1} (y_n - H\\beta), \\tag{20}\n\\end{align}\n\\]\nwith gradient \\[\n\\begin{align}\n\\nabla_{\\beta} \\mathcal{L}(\\theta)\n&= H^\\top C_{\\phi, \\sigma^2}^{-1}y_n - (H^\\top C_{\\phi, \\sigma^2}^{-1} H)\\beta. \\tag{21}\n\\end{align}\n\\] Setting the gradient equal to zero and solving for \\(\\beta\\) yields the optimality condition \\[\n\\begin{align}\n\\left(H^\\top C_{\\phi, \\sigma^2}^{-1} H\\right)\\hat{\\beta} &= H^\\top C_{\\phi, \\sigma^2}^{-1}y_n. \\tag{22}\n\\end{align}\n\\] A unique solution for \\(\\hat{\\beta}\\) thus exists when \\(H^\\top C_{\\phi, \\sigma^2}^{-1} H\\) is invertible. When does this happen? First note that this matrix is positive semidefinite, since \\[\n\\begin{align}\n\\beta^\\top \\left(H^\\top C_{\\phi, \\sigma^2}^{-1} H\\right) \\beta\n&= \\beta^\\top (H^\\top [LL^\\top]^{-1} H) \\beta\n= \\lVert L^{-1} H\\beta \\rVert_2^2 \\geq 0,\n\\end{align}\n\\] where we have used the fact that \\(C_{\\phi, \\sigma^2}\\) is positive definite and hence admits a decomposition \\(LL^\\top\\). The matrix \\(H^\\top C_{\\phi, \\sigma^2}^{-1} H\\) is thus positive definite when \\(L^{-1}H\\) has linearly independent columns; i.e., when it is full rank. We already know that \\(L^{-1}\\) is full rank. If we assume that \\(H\\) is also full rank and \\(n \\geq p\\) then we can conclude that \\(L^{-1}H\\) is full rank; see this post for a quick proof. Thus, under these assumptions we conclude that \\(H^\\top C_{\\phi, \\sigma^2}^{-1} H\\) is invertible and so \\[\n\\begin{align}\n\\hat{\\beta}(\\phi, \\sigma^2) &= \\left(H^\\top C_{\\phi, \\sigma^2}^{-1} H\\right)^{-1} H^\\top C_{\\phi, \\sigma^2}^{-1}y_n.\n\\tag{23}\n\\end{align}\n\\] Notice that (23) is simply a generalized least squares estimator. As with the constant mean, we can plug \\(\\hat{\\beta}(\\phi, \\sigma^2)\\) into the marginal likelihood to concentrate out the parameter \\(\\beta\\). The resulting concentrated likelihood can then be numerically optimized as a function of the remaining hyperparameters.\n\n\nLinear Model Coefficients: Closed-Form Marginalization\nThe above section showed that, conditional on fixed kernel hyperparameters, the coefficients of a linear mean function can be optimized in closed form. We now show a similar result: if the mean coefficients are assigned a Gaussian prior then, conditional on fixed kernel hyperparameters, the coefficients can be marginalized in closed form. To this end, we consider the same linear mean function as above, but now equip the coefficients with a Gaussian prior: \\[\n\\begin{align}\n\\mu_{\\psi}(x) &= h(x)^\\top \\beta, &&\\beta \\sim \\mathcal{N}(b, B).\n\\end{align}\n\\] Restricted to the model inputs \\(X\\), the model is thus \\[\n\\begin{align}\ny_n|\\beta &\\sim \\mathcal{N}\\left(H\\beta, C_{\\phi, \\sigma^2} \\right) \\newline\n\\beta &\\sim \\mathcal{N}(b, B).\n\\end{align}\n\\] Our goal here is derive the marginal distribution of \\(y_n\\). We could resort to computing the required integral by hand, but an easier approach is to notice that under the above model \\([y_n, \\beta]\\) is joint Gaussian distributed. Therefore, the marginal distribution of \\(y_n\\) must also be Gaussian. It thus remains to identify the mean and covariance of this distribution. We obtain \\[\n\\begin{align}\n\\mathbb{E}[y_n]\n&= \\mathbb{E}\\mathbb{E}[y_n|\\beta] = \\mathbb{E}[H\\beta] = Hb \\newline\n\\text{Cov}[y_n]\n&= \\mathbb{E}[y_n y_n^\\top] - \\mathbb{E}[y_n]\\mathbb{E}[y_n]^\\top \\newline\n&= \\mathbb{E} \\mathbb{E}\\left[y_n y_n^\\top | \\beta\\right] - (Hb)(Hb)^\\top \\newline\n&= \\mathbb{E}\\left[\\text{Cov}[y_n|\\beta] + \\mathbb{E}[y_n|\\beta] \\mathbb{E}[y_n|\\beta]^\\top \\right] - Hbb^\\top H^\\top \\newline\n&= \\mathbb{E}\\left[C_{\\phi, \\sigma^2} + (H\\beta)(H\\beta)^\\top \\right] - Hbb^\\top H^\\top \\newline\n&= C_{\\phi, \\sigma^2} + H\\mathbb{E}\\left[\\beta \\beta^\\top \\right]H^\\top - Hbb^\\top H^\\top \\newline\n&= C_{\\phi, \\sigma^2} + H\\left[B + bb^\\top \\right]H^\\top - Hbb^\\top H^\\top \\newline\n&= C_{\\phi, \\sigma^2} + HBH^\\top,\n\\end{align}\n\\] where we have used the law of total expectation and the various equivalent definitions for the covariance matrix. To summarize, we have found that the above hierarchical model implies the marginal distribution \\[\n\\begin{align}\ny_n &\\sim \\mathcal{N}\\left(Hb, C_{\\phi, \\sigma^2} + HBH^\\top \\right).\n\\end{align}\n\\] Since this holds for any set of inputs, we obtain the analogous result for the GP prior: \\[\n\\begin{align}\ny(x) &= f(x) + \\epsilon(x) \\newline\nf &\\sim \\mathcal{GP}\\left(\\mu^\\prime, k^\\prime \\right) \\newline\n\\epsilon(x) &\\overset{iid}{\\sim} \\mathcal{N}(0, \\sigma^2),\n\\end{align}\n\\] where \\[\n\\begin{align}\n\\mu^\\prime(x) &= h(x)^\\top b \\newline\nk^\\prime(x_1, x_2) &= k(x_1, x_2) + h(x_1)^\\top B h(x_2).\n\\end{align}\n\\] After marginalizing, we again end up with a mean function that is linear in the basis functions \\(h(\\cdot)\\). The basis function coefficients are now given by the prior mean \\(b\\). The mean \\(b\\) is something that we can prescribe, or we could again entertain an empirical Bayes approach to set its value. Note that we have descended another step in the hierarchical ladder. The kernel that appears from the marginalization is now a sum of two kernels: the original kernel \\(k\\) and the kernel \\(h(x_1)^\\top B h(x_2)\\). The latter can be viewed as a linear kernel in the transformed inputs \\(h(x_1)\\), \\(h(x_2)\\) and weighted by the positive definite matrix \\(B\\). It serves to account for the uncertainty in the coefficients of the mean function."
  },
  {
    "objectID": "posts/gp-specifications.html#special-case-closed-form-solutions-marginal-variance",
    "href": "posts/gp-specifications.html#special-case-closed-form-solutions-marginal-variance",
    "title": "Introduction to Gaussian Process Priors and Hyperparameter Estimation",
    "section": "Special Case Closed-Form Solutions: Marginal Variance",
    "text": "Special Case Closed-Form Solutions: Marginal Variance\nWe now consider a closed-form plug-in estimate for the marginal variance \\(\\alpha^2\\), as mentioned in (12). The takeaway from this section will be that a closed-form estimate is only available when the covariance matrix appearing in the marginal likelihood (16) is of the form \\[\n\\begin{align}\nC_{\\phi} &= \\alpha^2 C. \\tag{24}\n\\end{align}\n\\] This holds for any kernel of the form \\(\\alpha^2 k(\\cdot, \\cdot)\\) provided that \\(\\sigma^2 = 0\\). For example, the exponentiated quadratic kernel in (12) satisfies this requirement. With this assumption, the marginal likelihood is given by \\[\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log\\left(2\\pi \\alpha^2 \\right) - \\frac{1}{2}\\log\\text{det}(C) -\n\\frac{1}{2\\alpha^2} (y_n - \\mu_{\\psi})^\\top C^{-1} (y_n - \\mu_{\\psi}). \\tag{25}\n\\end{align}\n\\] The analytical derivations given below go through for a log marginal likelihood of this form. However, this doesn’t work for the common setting with an observation variance \\(\\sigma^2 &gt; 0\\), since in this case the covariance assumes the form \\[\n\\begin{align}\nC &= \\left(\\alpha^2 k(X) + \\sigma^2 I_n \\right).\n\\end{align}\n\\] This can be addressed via the simple reparameterization \\[\n\\begin{align}\n\\tilde{\\alpha}^2 C &:= \\tilde{\\alpha}^2\\left(k(X) + \\tilde{\\sigma}^2 I_n \\right).\n\\end{align}\n\\] This gives the required form of the covariance, and maintains the same number of parameters as before. The one downside is that we lose the straightforward interpretation of the noise variance; the observation noise is now given by the product \\(\\tilde{\\alpha}^2 \\tilde{\\sigma}^2\\) instead of being encoded in the single parameter \\(\\sigma^2\\). This reparameterization is utilized in the R package hetGP.\n\nPlug-In MLE\nLet’s consider optimizing the log marginal likelihood with respect to \\(\\alpha^2\\). The partial derivative of (25) with respect to \\(\\alpha^2\\) is given by \\[\n\\begin{align}\n\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\alpha^2}\n&= -\\frac{n}{2}\\frac{2\\pi}{2\\pi \\alpha^2} - \\frac{(y_n - \\mu_{\\psi})^\\top C^{-1} (y_n - \\mu_{\\psi})}{2\\alpha^4} \\newline\n&= -\\frac{n}{2\\alpha^2} - \\frac{(y_n - \\mu_{\\psi})^\\top C^{-1} (y_n - \\mu_{\\psi})}{2\\alpha^4}.\n\\end{align}\n\\] Setting this expression equal to zero and solving for \\(\\alpha^2\\) yields \\[\n\\begin{align}\n\\hat{\\alpha}^2 &= \\frac{(y_n - \\mu_{\\psi})^\\top C^{-1} (y_n - \\mu_{\\psi})}{n}.\n\\end{align}\n\\] Following the same procedure as before, the estimate \\(\\hat{\\alpha}^2\\) can be subbed in for \\(\\alpha^2\\) in \\(\\mathcal{L}(\\theta)\\) to obtain the concentrated log marginal likelihood.\n\n\nClosed-Form Marginalization"
  },
  {
    "objectID": "posts/gp-specifications.html#bias-corrections",
    "href": "posts/gp-specifications.html#bias-corrections",
    "title": "Introduction to Gaussian Process Priors and Hyperparameter Estimation",
    "section": "Bias Corrections",
    "text": "Bias Corrections"
  },
  {
    "objectID": "posts/gp-specifications.html#log-marginal-likelihood",
    "href": "posts/gp-specifications.html#log-marginal-likelihood",
    "title": "Introduction to Gaussian Process Priors and Hyperparameter Estimation",
    "section": "Log Marginal Likelihood",
    "text": "Log Marginal Likelihood\nWe start by considering the computation of the log marginal likelihood (16), \\[\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log(2\\pi) -\\frac{1}{2} \\log \\text{det}\\left(C \\right) -\n\\frac{1}{2} (y - \\mu)^\\top C^{-1} (y - \\mu),\n\\end{align}\n\\] where we now suppress all dependence on hyperparameters in the notation for succinctness. Since \\(C = k(X) + \\sigma^2 I_n\\) is positive definite, we may Cholesky decompose it as \\(C = L L^\\top\\). Plugging this decomposition into the log marginal likelihood yields \\[\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log(2\\pi) - \\frac{1}{2} \\log\\text{det}\\left(C\\right) -\n\\frac{1}{2} (y_n - \\mu)^\\top \\left(LL^\\top \\right)^{-1} (y_n - \\mu).\n\\end{align}\n\\] The log determinant and the quadratic term can both be conveniently written in terms of the Cholesky factor. These terms are given respectively by \\[\n\\begin{align}\n\\log\\text{det}\\left(LL^\\top\\right)\n&= \\log\\text{det}\\left(L\\right)^2\n= 2 \\log \\prod_{i=1}^{n} L_{ii}\n= 2 \\sum_{i=1}^{n} \\log\\left(L_{ii} \\right),\n\\end{align}\n\\] and \\[\n\\begin{align}\n(y_n - \\mu)^\\top \\left(LL^\\top \\right)^{-1} (y_n - \\mu)\n&= (y_n - \\mu)^\\top \\left(L^{-1}\\right)^\\top L^{-1} (y_n - \\mu)\n= \\lVert L^{-1}(y - \\mu)\\rVert_2^2.\n\\end{align}\n\\] The linear solve \\(L^{-1}(y - \\mu)\\) can be computed in \\(\\mathcal{O}(n^2)\\) by exploiting the fact that the linear system has lower triangular structure. Plugging these terms back into the log marginal likelihood gives \\[\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log(2\\pi) - \\sum_{i=1}^{n} \\log\\left(L_{ii}\\right) -\n\\frac{1}{2} \\lVert L^{-1}(y - \\mu)\\rVert_2^2.\n\\end{align}\n\\] Note that the Cholesky factor \\(L\\) is a function of \\(\\phi\\) and \\(\\sigma^2\\) and hence must be re-computed whenever the kernel hyperparameters or noise variances change."
  },
  {
    "objectID": "posts/gp-specifications.html#profile-log-marginal-likelihood-with-linear-mean-function",
    "href": "posts/gp-specifications.html#profile-log-marginal-likelihood-with-linear-mean-function",
    "title": "Introduction to Gaussian Process Priors and Hyperparameter Estimation",
    "section": "Profile Log Marginal Likelihood with Linear Mean Function",
    "text": "Profile Log Marginal Likelihood with Linear Mean Function\nWe now consider computation of the concentrated marginal log-likelihood under a mean function of the form (11), \\(\\mu(x) = h(x)^\\top \\beta\\), where the generalized least squares (GLS) estimator \\(\\hat{\\beta} = \\left(H^\\top C^{-1} H\\right)^{-1} H^\\top C^{-1}y\\) (see (23)) is inserted in place of \\(\\beta\\). We are thus considering the profile log marginal likelihood \\[\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log(2\\pi) -\\frac{1}{2} \\log \\text{det}\\left(C \\right) -\n\\frac{1}{2} (y - H\\hat{\\beta})^\\top C^{-1} (y - H\\hat{\\beta}).\n\\end{align}\n\\] We will derive a numerically stable implementation of this expression in two steps, first applying a Cholesky decomposition (as in the previous section), and then leveraging a QR decomposition as in a typical ordinary least squares (OLS) computation. We first write \\(\\hat{\\beta}\\) in terms of the Cholesky factor \\(L\\), where \\(C = LL^\\top\\): \\[\n\\begin{align}\n\\hat{\\beta}\n&= \\left(H^\\top C^{-1} H\\right)^{-1} H^\\top C^{-1}y \\newline\n&= \\left(H^\\top \\left[LL^\\top\\right]^{-1} H\\right)^{-1} H^\\top \\left[LL^\\top\\right]^{-1}y \\newline\n&= \\left(\\left[L^{-1}H \\right]^\\top \\left[L^{-1}H \\right] \\right)^{-1} \\left[L^{-1}H \\right]^\\top\n\\left[L^{-1}y\\right].\n\\end{align}\n\\] Notice that the GLS computation boils down to two lower-triangular linear solves: \\(L^{-1}H\\) and \\(L^{-1}y\\). However, the above expression still requires one non-triangular linear solve that we will now address via the QR decomposition. The above expression for \\(\\hat{\\beta}\\) can be viewed as a standard OLS estimator with design matrix \\(L^{-1}H\\) and response vector \\(L^{-1}y\\). At this point, we could adopt a standard OLS technique of taking the QR decomposition of the design matrix \\(L^{-1}H\\). This was my original thought, but I found a nice alternative looking through the code in the R kergp package (see the function .logLikFun0 in the file kergp/R/logLikFuns.R). The approach is to compute the QR decomposition \\[\n\\begin{align}\n\\begin{bmatrix} L^{-1}H & L^{-1}y \\end{bmatrix} &= QR = Q \\begin{bmatrix} \\tilde{R} & r \\end{bmatrix}.\n\\end{align}\n\\] That is, we compute QR on the matrix formed by concatenating \\(L^{-1}y\\) as an additional column on the design matrix \\(L^{-1}H\\). We have written the upper triangular matrix \\(R \\in \\mathbb{R}^{(p+1) \\times (p+1)}\\) as the concatenation of \\(\\tilde{R} \\in \\mathbb{R}^{(p+1) \\times p}\\) and the vector \\(r \\in \\mathbb{R}^{p+1}\\) so that \\(L^{-1}H = Q\\tilde{R}\\) and \\(L^{-1}y = Qr\\). We recall the basic properties of the QR decomposition: \\(R\\) is upper triangular and invertible, and \\(Q\\) has orthonormal columns with span equal to the column space of \\(\\begin{bmatrix} L^{-1}H & L^{-1}y \\end{bmatrix}\\). Taking the QR decomposition of this concatenated matrix leads to a very nice expression for the quadratic form term of the profile log marginal likelihood. But first let’s rewrite \\(\\hat{\\beta}\\) in terms of these QR factors: \\[\n\\begin{align}\n\\hat{\\beta}\n&= \\left(\\left[L^{-1}H \\right]^\\top \\left[L^{-1}H \\right] \\right)^{-1} \\left[L^{-1}H \\right]^\\top\n\\left[L^{-1}y\\right] \\newline\n&= \\left(\\left[Q\\tilde{R} \\right]^\\top \\left[Q\\tilde{R} \\right] \\right)^{-1} \\left[Q\\tilde{R} \\right]^\\top \\left[Qr\\right] \\newline\n&= \\left(\\tilde{R}^\\top Q^\\top Q\\tilde{R} \\right)^{-1} \\tilde{R}^\\top Q^\\top Qr \\newline\n&= \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top r,\n\\end{align}\n\\] where we have used the fact that \\(Q^\\top Q\\) is the identity since \\(Q\\) is orthogonal. Plugging this into the quadratic form term of the log likelihood gives \\[\n\\begin{align}\n(y - H\\hat{\\beta})^\\top C^{-1} (y - H\\hat{\\beta})\n&= (y - H\\hat{\\beta})^\\top \\left[LL^\\top \\right]^{-1} (y - H\\hat{\\beta}) \\newline\n&= \\lVert L^{-1}(y - H\\hat{\\beta}) \\rVert_2^2 \\newline\n&= \\lVert L^{-1}y - L^{-1}H\\hat{\\beta} \\rVert_2^2 \\newline\n&= \\lVert Qr - Q\\tilde{R} \\hat{\\beta} \\rVert_2^2 \\newline\n&= \\left\\lVert Qr - Q\\tilde{R} \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top r \\right\\rVert_2^2 \\newline\n&= \\left\\lVert Q\\left[r - \\tilde{R} \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top r \\right] \\right\\rVert_2^2 \\newline\n&= \\left\\lVert r - \\tilde{R} \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top \\right\\rVert_2^2 \\newline\n&= \\left\\lVert \\left[I - \\tilde{R} \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top \\right]r \\right\\rVert_2^2,\n\\end{align}\n\\] where the penultimate line follows from the fact that \\(Q\\) is orthogonal, and hence an isometry. At this point, notice that the matrix \\(P := \\tilde{R} \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top\\) is the standard OLS projection matrix (i.e., hat matrix) constructed with the design matrix \\(\\tilde{R}\\). Also, take care to notice that \\(\\tilde{R}\\) is not invertible (it is not even square). Using standard properties of the projection matrix, we know that \\(P\\) has rank \\(p\\), since \\(\\tilde{R}\\) has rank \\(p\\). Also, since \\(R\\) is upper triangular, then the last row of \\(\\tilde{R}\\) contains all zeros. Letting, \\(e_j\\) denote the \\(j^{\\text{th}}\\) standard basis vector of \\(\\mathbb{R}^{p+1}\\), this means that \\[\n\\begin{align}\n\\mathcal{R}(P) \\perp \\text{span}(e_{p+1}),\n\\end{align}\n\\] where \\(\\mathcal{R}(P)\\) denotes the range (i.e., column space) of \\(P\\). The only subspace of \\(\\mathbb{R}^{p+1}\\) with rank \\(p\\) and satisfying this property is \\(\\text{span}(e_1, \\dots, e_p)\\). The conclusion is that \\(P\\) projects onto \\(\\text{span}(e_1, \\dots, e_p)\\), and thus the annihilator \\(I - P\\) projects onto the orthogonal complement \\(\\text{span}(e_{p+1})\\). We thus conclude, \\[\n\\begin{align}\n\\left\\lVert \\left[I - P\\right]r \\right\\rVert_2^2\n&= \\lVert \\langle r, e_{p+1} \\rangle e_{p+1} \\rVert_2^2 \\newline\n&= \\lVert r_{p+1} e_{p+1} \\rVert_2^2 \\newline\n&= r_{p+1}^2,\n\\end{align}\n\\] where \\(r_{p+1}\\) is the last entry of \\(r\\); i.e., the bottom right entry of \\(R\\). We finally arrive at the expression for the concentrated log marginal likelihood \\[\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log(2\\pi) - \\sum_{i=1}^{n} \\log\\left(L_{ii}\\right) -\n\\frac{1}{2} r_{p+1}^2.\n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/nonlinear-LS.html",
    "href": "posts/nonlinear-LS.html",
    "title": "Nonlinear Least Squares",
    "section": "",
    "text": "This post provides an introduction to nonlinear least squares (NLS), which generalizes the familiar least squares problem by allowing for a nonlinear forward map. Unlike least squares, the NLS problem does not admit closed-form solutions, but a great deal of work has been devoted to numerical schemes tailored to this specific problem. We introduce two of the most popular algorithms: Gauss-Newton and Levenberg-Marquardt."
  },
  {
    "objectID": "posts/nonlinear-LS.html#statistical-perspective",
    "href": "posts/nonlinear-LS.html#statistical-perspective",
    "title": "Nonlinear Least Squares",
    "section": "Statistical Perspective",
    "text": "Statistical Perspective\nThe solution \\(u_{\\star}\\) to the optimization problem in (1) can be viewed as the maximum a posterior (MAP) estimate of the Bayesian model \\[\n\\begin{align}\ny|u &\\sim \\mathcal{N}(\\mathcal{G}(u), \\Sigma) \\tag{2} \\newline\nu &\\sim \\mathcal{N}(m, C).\n\\end{align}\n\\] In particular, the posterior density of (2) satisfies \\[\np(u|y) \\propto \\exp\\left[-J(u)\\right], \\tag{3}\n\\] where \\(J(u)\\) is the nonlinear least squares objective (1)."
  },
  {
    "objectID": "posts/nonlinear-LS.html#gradient-and-hessian",
    "href": "posts/nonlinear-LS.html#gradient-and-hessian",
    "title": "Nonlinear Least Squares",
    "section": "Gradient and Hessian",
    "text": "Gradient and Hessian\nThe gradient and Hessian of \\(J(u)\\) play a prominent role in numerical algorithms that seek to solve (1). We provide the expressions for these quantities below, with derivations given in the appendix.\n\n\nGradient and Hessian.  The gradient \\(\\nabla J(u)\\) and Hessian \\(\\nabla^2 J(u)\\) of the cost function defined in (1) are given by \\[\\begin{align}\n  \\nabla J(u) &= -\\nabla \\mathcal{G}(u)\\Sigma^{-1}(y-\\mathcal{G}(u)) + C^{-1}(u-m) \\tag{4} \\newline\n  \\nabla^2 J(u) &= [\\nabla \\mathcal{G}(u)]\\Sigma^{-1}[\\nabla \\mathcal{G}(u)]^\\top  +\n  \\sum_{i=1}^{n} \\epsilon_i(u) \\nabla^2 \\mathcal{G}_i(u) + C^{-1} \\tag{5},\n  \\end{align}\\] where \\(\\epsilon_i(u)\\) is the \\(i^{\\text{th}}\\) entry of the vector \\(\\epsilon(u) := \\Sigma^{-1}(\\mathcal{G}(u) - y)\\).\n\n\nIn contrast to least squares, the Hessian \\(\\nabla^2 J(u)\\) depends on \\(u\\) in general."
  },
  {
    "objectID": "posts/nonlinear-LS.html#newtons-method",
    "href": "posts/nonlinear-LS.html#newtons-method",
    "title": "Nonlinear Least Squares",
    "section": "Newton’s Method",
    "text": "Newton’s Method\nGiven a current iterate \\(u_k\\), Newton’s method constructs a quadratic approximation of \\(J(u)\\) centered at the point \\(u_k\\), then minimizes this quadratic function to select the next iterate \\(u_{k+1}\\). The specific quadratic approximation considered is the second order Taylor expansion \\[\nJ_k(u) := J(u_k) + DJ(u_k)[u-u_k] + \\frac{1}{2}(u-u_k)^\\top [\\nabla^2 J(u_k)] (u-u_k). \\tag{6}\n\\] The next iterate \\(u_{k+1}\\) is thus chosen by solving \\[\n\\nabla J_k(u_{k+1}) = 0, \\tag{7}\n\\] which yields \\[\n\\nabla J_k(u_k) = \\nabla J(u_k) + [\\nabla^2 J(u_k)] (u_{k+1}-u_k) = 0. \\tag{8}\n\\] At this point, note that it is possible \\(J_k\\) doesn’t even have a minimum, which can happen when \\(\\nabla^2 J(u_k)\\) is not positive definite (and hence not invertible). We will not get into this issue here, and instead simply assume that \\(\\nabla^2 J(u_k)\\) is invertible. Then we can solve for \\(u_{k+1}\\) in (7) as \\[\nu_{k+1} = u_k - [\\nabla^2 J(u_k)]^{-1} \\nabla J(u_k). \\tag{9}\n\\] Working with the Hessian here can be difficult in certain settings; note in (5) that computing the Hessian of \\(J\\) requires computing the Hessian of \\(\\mathcal{G}\\) for each output dimension. Since \\(\\mathcal{G}\\) can be a very complicated and computationally-expensive function, this presents challenges. The below methods maintain the spirit of the Newton update (9), but replace the Hessian with something that is easier to compute."
  },
  {
    "objectID": "posts/nonlinear-LS.html#gauss-newton",
    "href": "posts/nonlinear-LS.html#gauss-newton",
    "title": "Nonlinear Least Squares",
    "section": "Gauss-Newton",
    "text": "Gauss-Newton\nWe introduce the Gauss-Newton update from two different perspectives. The first simply approximates the Hessian by ignoring the “difficult” terms, while the second views the update as solving a local least squares problem.\n\nApproximate Hessian\nThe Gauss-Newton procedure performs the Newton update (9) with an approximate Hessian constructed by simply dropping the higher-order terms in (5). Doing so yields the Hessian approximation \\[\n\\hat{H}_k := G_k^\\top \\Sigma^{-1}G_k + C^{-1}, \\tag{10}\n\\] where we have introduced the shorthand for the Jacobian of \\(\\mathcal{G}\\) evaluated at \\(u_k\\), \\[\nG_k := D\\mathcal{G}(u_k). \\tag{11}\n\\] Notice that (10) no longer requires extracting second order derivative information from \\(\\mathcal{G}\\). The resulting Gauss-Newton update takes the form \\[\nu_{k+1} = u_k - \\hat{H}_k^{-1} \\nabla J(u_k). \\tag{12}\n\\] Plugging in the expressions for \\(\\hat{H}_k\\) and \\(\\nabla J(u_k)\\) yields the explicit update\n\n\nGauss Newton Update: Parameter Space \\[\n  u_{k+1}\n  = u_k - \\left(G_k^\\top \\Sigma^{-1}G_k + C^{-1} \\right)^{-1} \\left[C^{-1}(u_k-m) - G_k^\\top \\Sigma^{-1}(y-\\mathcal{G}(u_k)) \\right]. \\tag{13}\n  \\]\n\n\nA nice consequence of using \\(\\hat{H}_k\\) is that, unlike the Hessian, it is guaranteed to be positive definite. This follows from the fact that \\(G^\\top_k \\Sigma^{-1}G_k\\) is positive semidefinite, \\(C^{-1}\\) is positive definite, and the sum of a positive semidefinite and positive definite matrix is positive definite. The invertibility of \\(\\hat{H}_k\\) is thus guaranteed, and the local quadratic function that Gauss-Newton is implicitly minimizing is guaranteed to have a minimum.\nWe refer to (13) as the parameter space version of the Gauss-Newton update, as it requires a \\(d\\)-dimensional linear solve \\(\\hat{H}_k^{-1} \\nabla J(u_k)\\). By applying the Woodbury matrix inversion identity, we obtain an alternative, but equivalent, update that instead requires a linear solve in \\(n\\)-dimensional data space.\n\n\nGauss Newton Update: Data Space \\[\n  u_{k+1}\n  = m + CG_k^\\top \\left(G_k CG_k^\\top + \\Sigma \\right)^{-1}\\left[y-\\left\\{\\mathcal{G}(u_k) + G_k(u_k-m)\\right\\}\\right]. \\tag{14}\n  \\]\n\n\nNotice that the expression in curly brackets is the linearization \\[\n\\begin{align}\n\\mathcal{G}(u_k) + G_k(u_k-m)\n&= \\mathcal{G}(u_k) - G_k(m-u_k) \\newline\n&\\approx \\mathcal{G}(u_k) - \\left[\\mathcal{G}(m) - \\mathcal{G}(u_k) \\right] \\newline\n&= \\mathcal{G}(m), \\tag{15}\n\\end{align}\n\\] and can thus be interpreted as the approximation to \\(\\mathcal{G}(m)\\) induced by the first order Taylor expansion around \\(\\mathcal{G}(u_k)\\). With this in mind, the update (14) looks exactly like a regularized least squares estimator. We explore this perspective below.\n\n\nLocal Least Squares Perspective\nThe Gauss-Newton update can alternatively be viewed as minimizing the following linear least squares cost function: \\[\nJ_k(u) := \\frac{1}{2}\\lVert y - [\\mathcal{G}(u_k)+G_k(u-u_k)] \\rVert^2_{\\Sigma} + \\frac{1}{2}\\lVert u - m\\rVert_C^2. \\tag{16}\n\\] In other words, the NLS objective \\(J(u)\\) is approximated by replacing the true forward model \\(\\mathcal{G}(u)\\) with its linearization, centered at the current iterate \\(u_k\\). It should be emphasized that we are linearizing \\(\\mathcal{G}\\), and then simply plugging the linearization into \\(J(u)\\). This is not the same as directly linearizing \\(J(u)\\), which is the approach taken in gradient descent.\nThis approach of linearizing \\(\\mathcal{G}\\) is quite convenient, since (16) is simply a linear least squares objective function, which we know how to optimize in closed-form. Thus, the Gauss-Newton algorithm can be viewed as iteratively solving a sequence of linear least squares problems. We now try to gain some intuition on the structure of these local least squares problems.\n\\[\n\\begin{align}\ny_k|u &\\sim \\mathcal{N}(G_k u, \\Sigma) \\tag{17} \\newline\nu &\\sim \\mathcal{N}(m, C),\n\\end{align}\n\\] where \\(y_k := y - \\mathcal{G}(u_k) + G_k u_k\\)."
  },
  {
    "objectID": "posts/nonlinear-LS.html#levenberg-marquardt",
    "href": "posts/nonlinear-LS.html#levenberg-marquardt",
    "title": "Nonlinear Least Squares",
    "section": "Levenberg-Marquardt",
    "text": "Levenberg-Marquardt"
  }
]