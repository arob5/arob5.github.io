<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-01-11">

<title>Introduction to Gaussian Process Priors and Hyperparameter Estimation – myblog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">myblog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Introduction to Gaussian Process Priors and Hyperparameter Estimation</h1>
            <p class="subtitle lead">A deep dive into hyperparameter specifications for GP mean and covariance functions, including both frequentist and Bayesian methods for hyperparameter estimation.</p>
                                <div class="quarto-categories">
                <div class="quarto-category">Statistics</div>
                <div class="quarto-category">Gaussian-Process</div>
                <div class="quarto-category">kernel-methods</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 11, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>Gaussian processes (GP) are widely utilized across various fields, each with their own preferences, terminology, and conventions. Some notable domains that make significant use of GPs include - Spatial statistics (kriging) - Design and analysis of computer experiments (emulator/surrogate modeling) - Bayesian optimization - Machine learning</p>
<p>Even if you’re a GP expert in one of these domains, these differences can make navigating the GP literature in other domains a bit tricky. The goal of this post is to summarize common approaches for specifying GP distributions, and emphasize conventions and assumptions that tend to differ across fields. By “specifying GP distributions”, what I am really talking about here is parameterizing the mean and covariance functions that define the GP. While GPs are non-parametric models in a certain sense, specifying and learning the <em>hyperparameters</em> making up the mean and covariance functions is a crucial step to successful GP applications. I will discuss popular parameterizations for these functions, and different algorithms for learning these parameter values from data. In the spirit of drawing connections across different domains, I will try my best to borrow terminology from different fields, and will draw attention to synonymous terms by using boldface.</p>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<section id="gaussian-processes" class="level3">
<h3 class="anchored" data-anchor-id="gaussian-processes">Gaussian Processes</h3>
<p>Gaussian processes (GPs) define a probability distribution over a space of functions in such a way that they can be viewed as a generalization of Gaussian random vectors. Just as Gaussian vectors are defined by their mean vector and covariance matrix, GPs are defined by a mean and covariance <em>function</em>. We will interchangeably refer to the latter as either the <strong>covariance function</strong> or <strong>kernel</strong>.</p>
<p>We will consider GPs defined over a space of functions of the form <span class="math inline">\(f: \mathcal{X} \to \mathbb{R}\)</span>, where <span class="math inline">\(\mathcal{X} \subseteq \mathbb{R}^d\)</span>. We will refer to elements <span class="math inline">\(x \in \mathcal{X}\)</span> as <strong>inputs</strong> or <strong>locations</strong> and the images <span class="math inline">\(f(x) \in \mathbb{R}\)</span> as <strong>outputs</strong> or <strong>responses</strong>. If the use of the word “locations” seems odd, note that in spatial statistical settings, the inputs <span class="math inline">\(x\)</span> are often geographic coordinates. We will denote the mean and covariance function defining the GP by <span class="math inline">\(\mu: \mathcal{X} \to \mathbb{R}\)</span> and <span class="math inline">\(k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}\)</span>, respectively. The mean function is essentially unrestricted, but the covariance function <span class="math inline">\(k(\cdot, \cdot)\)</span> must be a valid positive definite kernel. If <span class="math inline">\(f(\cdot)\)</span> is a GP with mean function <span class="math inline">\(\mu(\cdot)\)</span> and kernel <span class="math inline">\(k(\cdot, \cdot)\)</span> we will denote this by <span class="math display">\[
\begin{align}
f \sim \mathcal{GP}(\mu, k). \tag{1}
\end{align}
\]</span></p>
<p>The defining property of GPs is that their finite-dimensional distributions are Gaussian; that is, for an arbitrary finite set of <span class="math inline">\(n\)</span> inputs <span class="math inline">\(X := \{x_1, \dots, x_N\} \subset \mathcal{X}\)</span>, the vector <span class="math inline">\(f(X) \in \mathbb{R}^n\)</span> is distributed as <span class="math display">\[
\begin{align}
f(X) \sim \mathcal{N}(\mu(X), k(X, X)). \tag{2}
\end{align}
\]</span> We are vectorizing notation here so that <span class="math inline">\([f(X)]_i := f(x_i)\)</span>, <span class="math inline">\([\mu(X)]_i := \mu(x_i)\)</span>, and <span class="math inline">\([k(X, X)]_{i,j} := k(x_i, x_j)\)</span>. When the two input sets to the kernel are equal, we lighten notation by writing <span class="math inline">\(k(X) := k(X, X)\)</span>. Now suppose we have two sets of inputs <span class="math inline">\(X\)</span> and <span class="math inline">\(\tilde{X}\)</span>, containing <span class="math inline">\(n\)</span> and <span class="math inline">\(m\)</span> inputs, respectively. The defining property (2) then implies</p>
<p><span class="math display">\[
\begin{align}
\begin{bmatrix} f(\tilde{X}) \newline f(X) \end{bmatrix}
&amp;\sim \mathcal{N}\left(
  \begin{bmatrix} \mu(\tilde{X}) \newline \mu(X) \end{bmatrix},
  \begin{bmatrix}
  k(\tilde{X}) &amp; k(\tilde{X}, X) \newline
  k(X, \tilde{X}) &amp; k(X)
  \end{bmatrix}
\right). \tag{3}
\end{align}
\]</span></p>
<p>The Gaussian joint distribution (3) implies that the conditional distributions are also Gaussian. In particular, the distribution of <span class="math inline">\(f(\tilde{X})|f(X)\)</span> can be obtained by applying the well-known Gaussian conditioning identities:</p>
<p><span class="math display">\[
\begin{align}
f(\tilde{X})|f(X) &amp;\sim \mathcal{N}(\hat{\mu}(\tilde{X}), \hat{k}(\tilde{X})), \tag{4} \newline
\hat{\mu}(\tilde{X}) &amp;:= \mu(\tilde{X}) + k(\tilde{X}, X)k(X)^{-1} [f(X) - \mu(X)] \newline
\hat{k}(\tilde{X}) &amp;:= k(\tilde{X}) - k(\tilde{X}, X)k(X)^{-1} k(X, \tilde{X}).
\end{align}
\]</span> The fact that the result (4) holds for arbitrary finite sets of inputs <span class="math inline">\(\tilde{X}\)</span> implies that the conditional <span class="math inline">\(f | f(X)\)</span> is also a GP, with mean and covariance functions <span class="math inline">\(\hat{\mu}(\cdot)\)</span> and <span class="math inline">\(\hat{k}(\cdot, \cdot)\)</span> defined by (4). On a terminology note, the <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(k(X)\)</span> is often called the <strong>kernel matrix</strong>. This is the matrix containing the kernel evaluations at the set of <span class="math inline">\(n\)</span> <em>observed</em> locations.</p>
</section>
<section id="regression-with-gps" class="level3">
<h3 class="anchored" data-anchor-id="regression-with-gps">Regression with GPs</h3>
<p>One common application of GPs is their use as a flexible nonlinear regression model. Let’s consider the basic regression setup with observed data pairs <span class="math inline">\((x_1, y_1), \dots, (x_n, y_n)\)</span>. We assume that the <span class="math inline">\(y_i\)</span> are noisy observations of some underlying latent function output <span class="math inline">\(f(x_i)\)</span>. The GP regression model arises by placing a GP prior distribution on the latent function <span class="math inline">\(f\)</span>. We thus consider the regression model <span class="math display">\[
\begin{align}
y(x) &amp;= f(x) + \epsilon(x) \tag{5} \newline
f &amp;\sim \mathcal{GP}(\mu, k) \newline
\epsilon(x) &amp;\overset{iid}{\sim} \mathcal{N}(0, \sigma^2),
\end{align}
\]</span> where we have assumed a simple additive Gaussian noise model. This assumption is quite common in the GP regression setting due to the fact that it results in closed-form conditional distributions, similar to (4). We will assume the error model (5) throughout this post, but note that there are many other possibilities if one is willing to abandon closed-form posterior inference.</p>
<p>The solution of the regression problem is given by the distribution of <span class="math inline">\(f(\cdot)|y(X)\)</span> or <span class="math inline">\(y(\cdot)|y(X)\)</span>, where <span class="math inline">\(y(X)\)</span> is the <span class="math inline">\(n\)</span>-dimensional vector of observed responses. The first distribution is the posterior on the latent function <span class="math inline">\(f\)</span>, while the second incorporates the observation noise as well. Both distributions can be derived in the same way, so we focus on the second. Letting <span class="math inline">\(\tilde{X}\)</span> denote a set of <span class="math inline">\(m\)</span> inputs at which we would like to predict the response, consider the joint distribution <span class="math display">\[
\begin{align}
\begin{bmatrix} y(\tilde{X}) \newline y(X) \end{bmatrix}
&amp;\sim \mathcal{N}\left(
  \begin{bmatrix} \mu(\tilde{X}) \newline \mu(X) \end{bmatrix},
  \begin{bmatrix}
  k(\tilde{X}) + \sigma^2 I_m &amp; k(\tilde{X}, X) \newline
  k(X, \tilde{X}) &amp; k(X) + \sigma^2 I_n
  \end{bmatrix}
\right). \tag{6}
\end{align}
\]</span> This is quite similar to (3), but now takes into account the noise term <span class="math inline">\(\epsilon\)</span>. This does not affect the mean vector since <span class="math inline">\(\epsilon\)</span> is mean-zero; nor does it affect the off-diagonal elements of the covariance matrix since <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(f\)</span> were assumed independent. Applying the Gaussian conditioning identities (4) yields the posterior distribution <span class="math display">\[
\begin{align}
y(\tilde{X})|y(X) &amp;\sim \mathcal{N}(\hat{\mu}(\tilde{X}), \hat{k}(\tilde{X})), \tag{7} \newline
\hat{\mu}(\tilde{X}) &amp;:= \mu(\tilde{X}) + k(\tilde{X}, X)[k(X) + \sigma^2 I_n]^{-1} [f(X) - \mu(X)] \newline
\hat{k}(\tilde{X}) &amp;:= \sigma^2 I_m + k(\tilde{X}) - k(\tilde{X}, X)[k(X) + \sigma^2 I_n]^{-1} k(X, \tilde{X}).
\end{align}
\]</span> We will refer to (7) as the GP <strong>posterior</strong>, <strong>predictive</strong>, or generically <strong>conditional</strong>, distribution. We observe that these equations are identical to (4), modulo the appearance of <span class="math inline">\(\sigma^2\)</span> in the predictive mean and covariance equations. The distribution <span class="math inline">\(f(\tilde{X})|y(X)\)</span> is identical to (7), except that the <span class="math inline">\(\sigma^2 I_m\)</span> is removed in the predictive covariance. Again, this reflects the subtle distinction between doing inference on the latent function <span class="math inline">\(f\)</span> versus on the observation process <span class="math inline">\(y\)</span>.</p>
</section>
<section id="noise-nuggets-and-jitters" class="level3">
<h3 class="anchored" data-anchor-id="noise-nuggets-and-jitters">Noise, Nuggets, and Jitters</h3>
<p>Observe that this whole regression procedure is only slightly different from the noiseless GP setting explored in the previous section (thanks to the Gaussian likelihood assumption). Indeed, the conditional distribution of <span class="math inline">\(f(\tilde{X})|y(X)\)</span> is derived from <span class="math inline">\(f(\tilde{X})|f(X)\)</span> by simply replacing <span class="math inline">\(k(X)\)</span> with <span class="math inline">\(k(X) + \sigma^2 I_n\)</span> (obtaining the distribution <span class="math inline">\(y(\tilde{X})|y(X)\)</span> requires the one additional step of adding <span class="math inline">\(\sigma^2 I_m\)</span> to the predictive covariance). In other words, we have simply applied standard GP conditioning using the modified kernel matrix <span class="math display">\[
\begin{align}
C(X) := k(X) + \sigma^2 I_n. \tag{8}
\end{align}
\]</span> We thus might reasonably wonder if the model (5) admits an alternative equivalent representation by defining a GP directly on the observation process <span class="math inline">\(y\)</span>. Defining such a model would require defining a kernel <span class="math inline">\(c: \mathcal{X} \times \mathcal{X} \to \mathbb{R}\)</span> that is consistent with (8). This route is fraught with difficulties and subtleties, which I will do my best to describe clearly here. At first glance, it seems like the right choice is <span class="math display">\[
\begin{align}
c(x, x^\prime) := k(x, x^\prime) + \sigma^2 \delta(x, x^\prime), \tag{9}
\end{align}
\]</span> where <span class="math inline">\(\delta(x, x^\prime) := 1[x = x^\prime]\)</span> is sometimes called the <strong>stationary white noise kernel</strong>. Why isn’t this quite right? Notice in (9) that <span class="math inline">\(\sigma^2\)</span> is added whenever the inputs <span class="math inline">\(x = x^\prime\)</span> are equal. However, suppose we observe multiple independent realizations of the process at the same inputs <span class="math inline">\(X\)</span>. In the regression model (9) the errors <span class="math inline">\(\epsilon(x)\)</span> are independent across these realizations, <em>even at the same locations</em>. However, this will not hold true in the model under (9), since <span class="math inline">\(\delta(x, x^\prime)\)</span> only sees the values of the inputs, and has no sense of distinction across realizations. We might try to fix this by writing something like <span class="math display">\[
\begin{align}
c(x_i, x_j) := k(x_i, x_j) + \sigma^2 \delta_{ij}, \tag{10}
\end{align}
\]</span> where the Delta function now depends on the labels <span class="math inline">\(i, j\)</span> instead of the values of the inputs. In the spatial statistics literature, it is not uncommon to see a covariance function defined like (10), but this is basically a notational hack. A kernel is a function of two inputs from <span class="math inline">\(\mathcal{X}\)</span> - we can’t have it also depending on some side information like the labels <span class="math inline">\(i, j\)</span>. At the end of the day, (9) and (10) are attempts to incorporate some concept of <strong>white noise</strong> inside the kernel itself, rather than via a hierarchical model like (5). I would just stick with the hierarchical model, which is easily rigorously defined and much more intuitive.</p>
<p>Nonetheless, one should not be surprised if expressions like (10) pop up, especially in the spatial statistics literature. Spatial statisticians refer to the noise term <span class="math inline">\(\epsilon(x)\)</span> as the <strong>nugget</strong>, and <span class="math inline">\(\sigma^2\)</span> the <strong>nugget variance</strong> (sometimes these terms are conflated). In this context, instead of representing observation noise, <span class="math inline">\(\sigma^2\)</span> is often thought of as representing some unresolved small-scale randomness in the spatial field itself. If you imagine sampling a field to determine the concentration of some mineral across space, then you would hope that repeated measurements (taken around the same time) would yield the same values. Naturally, they may not, and the introduction of the nugget is one way to account for this.</p>
<p>While this discussion may seem needlessly abstract, we recall that the effect of incorporating the noise term (however you want to interpret it) is to simply replace the kernel matrix <span class="math inline">\(k(X)\)</span> with the new matrix <span class="math inline">\(c(X) = k(X) + \sigma^2 I_n\)</span>. Confusingly, there is one more reason (having nothing to do with observation error or nuggets) that people use a matrix of the form <span class="math inline">\(c(X)\)</span> in place of <span class="math inline">\(k(X)\)</span>: numerical stability. Indeed, even though <span class="math inline">\(k(X)\)</span> is theoretically positive definite, in practice its numerical instantiation may fail to have this property. A simple approach to deal with this is to add a small, fixed constant <span class="math inline">\(\sigma^2\)</span> to the diagonal of the kernel matrix. In this context, <span class="math inline">\(\sigma^2\)</span> is often called the <strong>jitter</strong>. While computationally its effect is the same as the nugget, note that its introduction is motivated very differently. The jitter is not stemming from some sort of random white noise; it is purely a computational hack to improve the conditioning of the kernel matrix. Check out <a href="https://discourse.mc-stan.org/t/adding-gaussian-process-covariance-functions/237/67">this</a> thread for some entertaining debates on the use of the nugget and jitter concepts.</p>
</section>
<section id="parameterized-means-and-kernels" class="level3">
<h3 class="anchored" data-anchor-id="parameterized-means-and-kernels">Parameterized Means and Kernels</h3>
<p>Everything we have discussed this far assumes fixed mean and covariance functions. In practice, suitable choices for these quantities are not typically known. Thus, the usual approach is to specify some parametric families <span class="math inline">\(\mu = \mu_{\psi}\)</span> and <span class="math inline">\(k = k_{\phi}\)</span> and learn their parameters from data. The parameters <span class="math inline">\(\psi\)</span> and <span class="math inline">\(\phi\)</span> are often referred to as <strong>hyperparameters</strong>, since they are not the primary parameters of interest in the GP regression model. Recalling from (5) that the GP acts as a prior distribution on the latent function, we see that <span class="math inline">\(\psi\)</span> and <span class="math inline">\(\phi\)</span> control the specification of this prior distribution. In addition to <span class="math inline">\(\psi\)</span> and <span class="math inline">\(\phi\)</span>, the parameter <span class="math inline">\(\sigma^2\)</span> is also typically not known. I will not wade back into the previous section’s debate in arguing whether this should be classified as a “hyperparameter” or not. In any case, let’s let <span class="math inline">\(\theta := \{\psi, \phi, \sigma^2 \}\)</span> denote the full set of (hyper)parameters that must be learned from data.</p>
<section id="mean-functions" class="level4">
<h4 class="anchored" data-anchor-id="mean-functions">Mean Functions</h4>
<p>The machine learning community commonly uses the simplest possible form for the mean function: <span class="math inline">\(\mu(x) \equiv 0\)</span>. This zero-mean assumption is less restrictive than it seems, since GPs mainly derive their expressivity from the kernel. A slight generalization is to allow a constant, non-zero mean <span class="math inline">\(\mu(x) \equiv \beta_0\)</span>, where <span class="math inline">\(\beta_0 \in \mathbb{R}\)</span>. However, constant (including zero-mean) GP priors can have some undesirable properties; e.g., in the context of extrapolation. Sometimes one wants more flexibility, and in these cases it is quite common to consider some sort of linear regression model <span class="math display">\[
\begin{align}
\mu(x) = h(x)^\top \beta, \tag{11}
\end{align}
\]</span> where <span class="math inline">\(h: \mathcal{X} \to \mathbb{R}^p\)</span> is some feature map and <span class="math inline">\(\beta \in \mathbb{R}^p\)</span> the associated coefficient vector. For example, <span class="math inline">\(h(x) = [1, x^\top]^\top\)</span> would yield a standard linear model, and <span class="math inline">\(h(x) = [1, x_1, \dots, x_d, x_1^2, \dots, x_d^2]^\top\)</span> would allow for a quadratic trend.</p>
</section>
<section id="kernels" class="level4">
<h4 class="anchored" data-anchor-id="kernels">Kernels</h4>
<p>The positive definite restriction makes defining valid covariance functions much more difficult than defining mean functions. Thus, one typically falls back on one of a few popular choices of known parametric kernel families (though note that kernels can be combined in various ways to give a large variety of options). While the goal of this post is not to explore specific kernels, in order to have a concrete example in mind consider the following parameterization: <span class="math display">\[
\begin{align}
k(x, \tilde{x}) = \alpha^2 \sum_{j=1}^{d} \left(-\frac{\lvert x^{j} - \tilde{x}^j \rvert}{\ell^j}\right)^2.
\tag{12}
\end{align}
\]</span> Note that I’m using superscripts to index vector entries here. This kernel goes by many names, including <strong>exponentiated quadratic</strong>, <strong>squared exponential</strong>, <strong>Gaussian</strong>, <strong>radial basis function</strong>, and <strong>automatic relevance determination</strong>. The parameter <span class="math inline">\(\alpha^2\)</span> is sometimes called the <strong>marginal variance</strong>, or just the <strong>scale parameter</strong>. The parameters <span class="math inline">\(\ell^1, \dots, \ell^d\)</span> are often called <strong>lengthscale</strong>, <strong>smoothness</strong>, or <strong>range</strong> parameters, since they control the smoothness of the GP realizations along each coordinate direction. Other popular kernels (e.g., Matérn) have analogous parameters controlling similar features. Note that in this example we have <span class="math inline">\(\phi = \{\alpha^2, \ell^1, \dots, \ell^d \}\)</span>. Also note that people choose to parameterize the Gaussian kernel in many different ways; for example, it’s not uncommon to see a <span class="math inline">\(1/2\)</span> factor included inside the exponential to make the kernel align with the typical parameterization of the Gaussian probability density function. Knowing which parameterization you’re working with is important for interpreting the hyperparameters, specifying bounds, defining priors, etc.</p>
<p>It is quite common in the spatial statistics (and sometimes the computer experiments) literature to see kernels written like <span class="math inline">\(\alpha^2 k(\cdot, \cdot)\)</span>; in these cases <span class="math inline">\(k(\cdot, \cdot)\)</span> typically represents a <em>correlation</em> function, which becomes the covariance function after multiplying by the marginal variance <span class="math inline">\(\alpha^2\)</span>. There is an advantage in decomposing the kernel this way when it comes to estimating the hyperparameters, which we will discuss shortly.</p>
</section>
</section>
<section id="the-gp-marginal-likelihood-function" class="level3">
<h3 class="anchored" data-anchor-id="the-gp-marginal-likelihood-function">The GP (Marginal) Likelihood Function</h3>
<p>Let’s first recall the GP regression model (5) <span class="math display">\[
\begin{align}
y(x) &amp;= f(x) + \epsilon(x) \newline
f &amp;\sim \mathcal{GP}(\mu_{\psi}, k_{\phi}) \newline
\epsilon &amp;\overset{iid}{\sim} \mathcal{N}(0, \sigma^2),
\end{align}
\]</span> where we have now explicitly added the dependence on <span class="math inline">\(\psi\)</span> and <span class="math inline">\(\phi\)</span>. This model is defined for any <span class="math inline">\(x \in \mathcal{X}\)</span>. However, when estimating hyperparameters, we will naturally be restricting the model to <span class="math inline">\(X\)</span>, the finite set of locations at which we actually have observations. Restricting to <span class="math inline">\(X\)</span> reduces the above model to the standard (finite-dimensional) Bayesian regression model <span class="math display">\[
\begin{align}
y(X)|f(X), \theta &amp;\sim \mathcal{N}(f(X), \sigma^2 I_n) \tag{13} \newline
f(X)|\theta &amp;\sim \mathcal{N}(\mu_{\psi}(X), k_{\phi}(X)).
\end{align}
\]</span> We could consider completing the Bayesian specification by defining a prior on <span class="math inline">\(\theta\)</span>, but we’ll hold off on this for now. Notice that the model (13) defines a joint distribution over <span class="math inline">\([y(X), f(X)] | \theta\)</span>, with <span class="math inline">\(y(X)|f(X), \theta\)</span> representing the likelihood of the observations at the observed input locations <span class="math inline">\(X\)</span>. At present everything is conditional on a fixed <span class="math inline">\(\theta\)</span>. Now, if we marginalize the likelihood <span class="math inline">\(y(X)|f(X), \theta\)</span> with respect to <span class="math inline">\(f(X)\)</span> then we obtain the distribution <span class="math inline">\(y(X) | \theta\)</span>. This is often called the <strong>marginal likelihood</strong>, due to the fact that <span class="math inline">\(f\)</span> was marginalized out. In particular, the distribution <span class="math inline">\(y(X) | \theta\)</span> has implicitly marginalized the function values <span class="math inline">\(f(\tilde{x})\)</span> at all location <span class="math inline">\(\tilde{x}\)</span> other than <span class="math inline">\(X\)</span>. This same logic and terminology applies in the noiseless setting with <span class="math inline">\(\sigma^2 = 0\)</span>, in which case the marginal likelihood is given by <span class="math inline">\(f(X) | \theta\)</span>. In the noiseless setting we are marginalizing both over the unobserved function values and the noise <span class="math inline">\(\epsilon\)</span>. Thanks to all the Gaussian assumptions here, the marginal likelihood is available in closed-form. One could approach the derivation using (13) as the starting point, but it’s much easier to consider the model written out using random variables, <span class="math display">\[
\begin{align}
y(X) &amp;= f(X) + \epsilon(X).
\end{align}
\]</span> Since <span class="math inline">\(f(X)\)</span> and <span class="math inline">\(\epsilon(X)\)</span> are independent Gaussians, then their sum is also Gaussian with mean and covariance given by <span class="math display">\[
\begin{align}
\mathbb{E}[y(X)|\theta]
&amp;= \mathbb{E}[f(X)|\theta] + \mathbb{E}[\epsilon(X)|\theta] = \mu_{\psi}(X) \newline
\text{Cov}[y(X)|\theta]
&amp;= \text{Cov}[f(X)|\theta] + \text{Cov}[\epsilon(X)|\theta]
= k_{\phi}(X) + \sigma^2 I_n.
\end{align}
\]</span> We have thus found that <span class="math display">\[
\begin{align}
y(X)|\theta \sim \mathcal{N}\left(\mu_{\psi}(X), C_{\phi, \sigma^2}(X)\right), \tag{14}
\end{align}
\]</span> recalling the definition <span class="math inline">\(C_{\phi, \sigma^2}(X) := k_{\phi}(X) + \sigma^2 I_n\)</span>. We will let <span class="math inline">\(\mathcal{L}(\theta)\)</span> denote the log density of this Gaussian distribution; i.e.&nbsp;the log <strong>marginal likelihood</strong>: <span class="math display">\[
\begin{align}
\mathcal{L}(\theta)
&amp;:= -\frac{1}{2} \log \text{det}\left(2\pi C_{\phi, \sigma^2}(X) \right) -
\frac{1}{2} (y(X) - \mu_{\psi}(X))^\top C_{\phi, \sigma^2}(X)^{-1} (y(X) - \mu_{\psi}(X)) \tag{15}
\end{align}
\]</span> The function <span class="math inline">\(\mathcal{L}(\theta)\)</span> plays a central role in the typical approach to hyperparameter optimization, as we will explore below. Also note that the above derivations also apply to the noiseless setting (i.e., <span class="math inline">\(y(X) = f(X)\)</span>) by setting <span class="math inline">\(\sigma^2 = 0\)</span>. In this case, the marginal likelihood is simply the GP distribution restricted to the inputs <span class="math inline">\(X\)</span>.</p>
<p>I have henceforth been a bit verbose with the notation in (15) to make very explicit the dependence on the inputs <span class="math inline">\(X\)</span> and the hyperparameters. To lighten notation a bit, we define <span class="math inline">\(y_n := y(X)\)</span>, <span class="math inline">\(\mu_{\psi} := \mu_{\psi}(X)\)</span>, and <span class="math inline">\(C_{\phi, \sigma^2} := C_{\phi, \sigma^2}(X)\)</span>, allowing us to rewrite (15) as <span class="math display">\[
\begin{align}
\mathcal{L}(\theta)
&amp;:= -\frac{1}{2} \log \text{det}\left(2\pi C_{\phi, \sigma^2} \right) -
\frac{1}{2} (y_n - \mu_{\psi})^\top C_{\phi, \sigma^2}^{-1} (y_n - \mu_{\psi}). \tag{16}
\end{align}
\]</span> We have simply suppressed the explicit dependence on <span class="math inline">\(X\)</span> in the notation.</p>
</section>
</section>
<section id="hyperparameter-optimization" class="level1">
<h1>Hyperparameter Optimization</h1>
<p>We now begin to turn out attention to methods for learning the values of the hyperparameters from data. This section starts with the most popular approach: optimizing the marginal likelihood.</p>
<section id="maximum-marginal-likelihood-or-empirical-bayes" class="level2">
<h2 class="anchored" data-anchor-id="maximum-marginal-likelihood-or-empirical-bayes">Maximum Marginal Likelihood, or Empirical Bayes</h2>
<p>Recall that (16) gives the expression for the log marginal likelihood <span class="math inline">\(\mathcal{L}(\theta)\)</span>, which is just the log density of <span class="math inline">\(y(X)|\theta\)</span> viewed as a function of <span class="math inline">\(\theta\)</span>. A natural approach is to set the hyperparameters <span class="math inline">\(\theta\)</span> to their values that maximize <span class="math inline">\(\mathcal{L}(\theta)\)</span>: <span class="math display">\[
\begin{align}
\hat{\theta} := \text{argmax} \ \mathcal{L}(\theta). \tag{17}
\end{align}
\]</span> At first glance, the Gaussian form of <span class="math inline">\(\mathcal{L}(\theta)\)</span> might look quite friendly to closed-form optimization. After all, maximum likelihood estimates of the mean and covariance of Gaussian vectors are indeed available in closed-form. However, upon closer inspection notice that the covariance is not being directly optimized; we are optimizing <span class="math inline">\(\phi\)</span>, and the covariance <span class="math inline">\(C_{\phi, \sigma^2}\)</span> is a <em>nonlinear</em> function of this parameter. Thus, in general some sort of iterative numerical scheme is is used for the optimization. Typically, gradient-based approaches are preferred, meaning we must be able to calculate quantities like <span class="math inline">\(\frac{\partial}{\partial \phi} C_{\phi, \sigma^2}\)</span>. The exact gradient calculations will thus depend on the choice of kernel; specifics on kernels and optimization schemes are not the focus of this post. We will instead focus on the high level ideas here. The general approach to GP regression that we have outlined so far can be summarized as: 1. Solve the optimization problem (17) and fix the hyperparameters at their optimized values <span class="math inline">\(\hat{\theta}\)</span>. The hyperparameters will be fixed from this point onward. 2. Use the GP predictive equations (7) to perform inference at a set of locations of interest <span class="math inline">\(\tilde{X}\)</span>.</p>
<p>One might object to the fact that we are estimating the hyperparameters from data, and then neglecting the uncertainty in <span class="math inline">\(\hat{\theta}\)</span> during the prediction step. It is true that this uncertainty is being ignored, but it is also very computationally convenient to do so. We will discuss alternatives later on, but this simple approach is probably the most commonly used in practice today. One way to think about this strategy is in an <strong>empirical Bayes</strong> context; that is, we can view this approach as an approximation to a fully Bayesian hierarchical model, which would involve equipping the hyperparameters with their own priors. Instead of marginalizing the hyperparameters, we instead fix them at their most likely values with respect to the observed data. We are using the data to “fine tune” the GP prior distribution. In the literature you will see this general hyperparameter optimization strategy referred to as either <strong>empirical Bayes</strong>, <strong>maximum marginal likelihood</strong>, or even just <strong>maximum likelihood</strong>.</p>
</section>
<section id="special-case-closed-form-solutions-mean-function" class="level2">
<h2 class="anchored" data-anchor-id="special-case-closed-form-solutions-mean-function">Special Case Closed-Form Solutions: Mean Function</h2>
<p>As mentioned above, in general the maximization of <span class="math inline">\(\mathcal{L}(\theta)\)</span> requires numerical methods. However, in certain cases elements of <span class="math inline">\(\theta\)</span> can be optimized in closed-form, meaning that numerical optimization may only be required for a subset of the hyperparameters. We start by considering closed form optimizers for the parameters defining the mean functions.</p>
<section id="constant-mean-plug-in-mle" class="level3">
<h3 class="anchored" data-anchor-id="constant-mean-plug-in-mle">Constant Mean: Plug-In MLE</h3>
<p>With the choice of constant mean <span class="math inline">\(\mu_{\psi}(x) \equiv \beta_0\)</span> the log marginal likelihood becomes</p>
<p><span class="math display">\[
\begin{align}
\mathcal{L}(\theta)
&amp;:= -\frac{1}{2} \log \text{det}\left(2\pi C_{\phi, \sigma^2} \right) -
\frac{1}{2} (y_n - \beta_0 1_n)^\top C_{\phi, \sigma^2}(X)^{-1} (y_n - \beta_0 1_n),
\end{align}
\]</span></p>
<p>with <span class="math inline">\(1_n \in \mathbb{R}^n\)</span> denoting a vector of ones. We now consider optimizing <span class="math inline">\(\mathcal{L}(\theta)\)</span> as a function of <span class="math inline">\(\beta_0\)</span> only. The partial derivative with respect to the constant mean equals <span class="math display">\[
\begin{align}
\frac{\partial \mathcal{L}(\theta)}{\partial \beta_0}
&amp;= y_n^\top C_{\phi, \sigma^2}^{-1}1_n - \beta_0 1_n^\top C_{\phi, \sigma^2}^{-1} 1_n.    \tag{18}
\end{align}
\]</span> Setting (18) equal to zero and solving for <span class="math inline">\(\beta_0\)</span> gives the optimum <span class="math display">\[
\begin{align}
\hat{\beta}_0(\phi, \sigma^2) = \frac{y_n^\top C_{\phi, \sigma^2}^{-1} 1_n}{1_n^\top C_{\phi, \sigma^2}^{-1} 1_n}. \tag{19}
\end{align}
\]</span> Notice that <span class="math inline">\(\hat{\beta}_0(\phi, \sigma^2)\)</span> depends on the values of the other hyperparameters <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\sigma^2\)</span>. Therefore, while this does not give us the outright value for the mean, we can plug <span class="math inline">\(\hat{\beta}_0(\phi, \sigma^2)\)</span> in place of <span class="math inline">\(\beta_0\)</span> in the marginal likelihood. This yields the <strong>profile likelihood</strong> (aka the <strong>concentrated likelihood</strong>), which is no longer a function of <span class="math inline">\(\beta_0\)</span> and hence the dimensionality of the subsequent numerical optimization problem has been reduced.</p>
</section>
<section id="linear-model-coefficients-plug-in-mle" class="level3">
<h3 class="anchored" data-anchor-id="linear-model-coefficients-plug-in-mle">Linear Model Coefficients: Plug-In MLE</h3>
<p>Let’s try to do the same thing with the mean function <span class="math inline">\(\mu_{\psi}(x) = h(x)^\top \beta\)</span>. The constant mean function is actually just a special case of this more general setting, but its common enough that it warranted its own section. If we denote by <span class="math inline">\(H \in \mathbb{R}^{n \times p}\)</span> the feature matrix with rows equal to <span class="math inline">\(h(x_i)^\top\)</span>, <span class="math inline">\(i = 1, \dots, n\)</span> then the marginal likelihood becomes <span class="math display">\[
\begin{align}
\mathcal{L}(\theta)
&amp;:= -\frac{1}{2} \log \text{det}\left(2\pi C_{\phi, \sigma^2} \right) -
\frac{1}{2} (y_n - H\beta)^\top C_{\phi, \sigma^2}^{-1} (y_n - H\beta), \tag{20}
\end{align}
\]</span></p>
<p>with gradient <span class="math display">\[
\begin{align}
\nabla_{\beta} \mathcal{L}(\theta)
&amp;= H^\top C_{\phi, \sigma^2}^{-1}y_n - (H^\top C_{\phi, \sigma^2}^{-1} H)\beta. \tag{21}
\end{align}
\]</span> Setting the gradient equal to zero and solving for <span class="math inline">\(\beta\)</span> yields the optimality condition <span class="math display">\[
\begin{align}
\left(H^\top C_{\phi, \sigma^2}^{-1} H\right)\hat{\beta} &amp;= H^\top C_{\phi, \sigma^2}^{-1}y_n. \tag{22}
\end{align}
\]</span> A unique solution for <span class="math inline">\(\hat{\beta}\)</span> thus exists when <span class="math inline">\(H^\top C_{\phi, \sigma^2}^{-1} H\)</span> is invertible. When does this happen? First note that this matrix is positive semidefinite, since <span class="math display">\[
\begin{align}
\beta^\top \left(H^\top C_{\phi, \sigma^2}^{-1} H\right) \beta
&amp;= \beta^\top (H^\top [LL^\top]^{-1} H) \beta
= \lVert L^{-1} H\beta \rVert_2^2 \geq 0,
\end{align}
\]</span> where we have used the fact that <span class="math inline">\(C_{\phi, \sigma^2}\)</span> is positive definite and hence admits a decomposition <span class="math inline">\(LL^\top\)</span>. The matrix <span class="math inline">\(H^\top C_{\phi, \sigma^2}^{-1} H\)</span> is thus positive definite when <span class="math inline">\(L^{-1}H\)</span> has linearly independent columns; i.e., when it is full rank. We already know that <span class="math inline">\(L^{-1}\)</span> is full rank. If we assume that <span class="math inline">\(H\)</span> is also full rank and <span class="math inline">\(n \geq p\)</span> then we can conclude that <span class="math inline">\(L^{-1}H\)</span> is full rank; see <a href="https://math.stackexchange.com/questions/272049/rank-of-matrix-ab-when-a-and-b-have-full-rank">this</a> post for a quick proof. Thus, under these assumptions we conclude that <span class="math inline">\(H^\top C_{\phi, \sigma^2}^{-1} H\)</span> is invertible and so <span class="math display">\[
\begin{align}
\hat{\beta}(\phi, \sigma^2) &amp;= \left(H^\top C_{\phi, \sigma^2}^{-1} H\right)^{-1} H^\top C_{\phi, \sigma^2}^{-1}y_n.
\tag{23}
\end{align}
\]</span> Notice that (23) is simply a <a href="https://en.wikipedia.org/wiki/Generalized_least_squares">generalized least squares</a> estimator. As with the constant mean, we can plug <span class="math inline">\(\hat{\beta}(\phi, \sigma^2)\)</span> into the marginal likelihood to concentrate out the parameter <span class="math inline">\(\beta\)</span>. The resulting concentrated likelihood can then be numerically optimized as a function of the remaining hyperparameters.</p>
</section>
<section id="linear-model-coefficients-closed-form-marginalization" class="level3">
<h3 class="anchored" data-anchor-id="linear-model-coefficients-closed-form-marginalization">Linear Model Coefficients: Closed-Form Marginalization</h3>
<p>The above section showed that, conditional on fixed kernel hyperparameters, the coefficients of a linear mean function can be optimized in closed form. We now show a similar result: if the mean coefficients are assigned a Gaussian prior then, conditional on fixed kernel hyperparameters, the coefficients can be marginalized in closed form. To this end, we consider the same linear mean function as above, but now equip the coefficients with a Gaussian prior: <span class="math display">\[
\begin{align}
\mu_{\psi}(x) &amp;= h(x)^\top \beta, &amp;&amp;\beta \sim \mathcal{N}(b, B).
\end{align}
\]</span> Restricted to the model inputs <span class="math inline">\(X\)</span>, the model is thus <span class="math display">\[
\begin{align}
y_n|\beta &amp;\sim \mathcal{N}\left(H\beta, C_{\phi, \sigma^2} \right) \newline
\beta &amp;\sim \mathcal{N}(b, B).
\end{align}
\]</span> Our goal here is derive the marginal distribution of <span class="math inline">\(y_n\)</span>. We could resort to computing the required integral by hand, but an easier approach is to notice that under the above model <span class="math inline">\([y_n, \beta]\)</span> is joint Gaussian distributed. Therefore, the marginal distribution of <span class="math inline">\(y_n\)</span> must also be Gaussian. It thus remains to identify the mean and covariance of this distribution. We obtain <span class="math display">\[
\begin{align}
\mathbb{E}[y_n]
&amp;= \mathbb{E}\mathbb{E}[y_n|\beta] = \mathbb{E}[H\beta] = Hb \newline
\text{Cov}[y_n]
&amp;= \mathbb{E}[y_n y_n^\top] - \mathbb{E}[y_n]\mathbb{E}[y_n]^\top \newline
&amp;= \mathbb{E} \mathbb{E}\left[y_n y_n^\top | \beta\right] - (Hb)(Hb)^\top \newline
&amp;= \mathbb{E}\left[\text{Cov}[y_n|\beta] + \mathbb{E}[y_n|\beta] \mathbb{E}[y_n|\beta]^\top \right] - Hbb^\top H^\top \newline
&amp;= \mathbb{E}\left[C_{\phi, \sigma^2} + (H\beta)(H\beta)^\top \right] - Hbb^\top H^\top \newline
&amp;= C_{\phi, \sigma^2} + H\mathbb{E}\left[\beta \beta^\top \right]H^\top - Hbb^\top H^\top \newline
&amp;= C_{\phi, \sigma^2} + H\left[B + bb^\top \right]H^\top - Hbb^\top H^\top \newline
&amp;= C_{\phi, \sigma^2} + HBH^\top,
\end{align}
\]</span> where we have used the law of total expectation and the various equivalent definitions for the covariance matrix. To summarize, we have found that the above hierarchical model implies the marginal distribution <span class="math display">\[
\begin{align}
y_n &amp;\sim \mathcal{N}\left(Hb, C_{\phi, \sigma^2} + HBH^\top \right).
\end{align}
\]</span> Since this holds for any set of inputs, we obtain the analogous result for the GP prior: <span class="math display">\[
\begin{align}
y(x) &amp;= f(x) + \epsilon(x) \newline
f &amp;\sim \mathcal{GP}\left(\mu^\prime, k^\prime \right) \newline
\epsilon(x) &amp;\overset{iid}{\sim} \mathcal{N}(0, \sigma^2),
\end{align}
\]</span> where <span class="math display">\[
\begin{align}
\mu^\prime(x) &amp;= h(x)^\top b \newline
k^\prime(x_1, x_2) &amp;= k(x_1, x_2) + h(x_1)^\top B h(x_2).
\end{align}
\]</span> After marginalizing, we again end up with a mean function that is linear in the basis functions <span class="math inline">\(h(\cdot)\)</span>. The basis function coefficients are now given by the prior mean <span class="math inline">\(b\)</span>. The mean <span class="math inline">\(b\)</span> is something that we can prescribe, or we could again entertain an empirical Bayes approach to set its value. Note that we have descended another step in the hierarchical ladder. The kernel that appears from the marginalization is now a sum of two kernels: the original kernel <span class="math inline">\(k\)</span> and the kernel <span class="math inline">\(h(x_1)^\top B h(x_2)\)</span>. The latter can be viewed as a linear kernel in the transformed inputs <span class="math inline">\(h(x_1)\)</span>, <span class="math inline">\(h(x_2)\)</span> and weighted by the positive definite matrix <span class="math inline">\(B\)</span>. It serves to account for the uncertainty in the coefficients of the mean function.</p>
</section>
</section>
<section id="special-case-closed-form-solutions-marginal-variance" class="level2">
<h2 class="anchored" data-anchor-id="special-case-closed-form-solutions-marginal-variance">Special Case Closed-Form Solutions: Marginal Variance</h2>
<p>We now consider a closed-form plug-in estimate for the marginal variance <span class="math inline">\(\alpha^2\)</span>, as mentioned in (12). The takeaway from this section will be that a closed-form estimate is only available when the covariance matrix appearing in the marginal likelihood (16) is of the form <span class="math display">\[
\begin{align}
C_{\phi} &amp;= \alpha^2 C. \tag{24}
\end{align}
\]</span> This holds for any kernel of the form <span class="math inline">\(\alpha^2 k(\cdot, \cdot)\)</span> provided that <span class="math inline">\(\sigma^2 = 0\)</span>. For example, the exponentiated quadratic kernel in (12) satisfies this requirement. With this assumption, the marginal likelihood is given by <span class="math display">\[
\begin{align}
\mathcal{L}(\theta)
&amp;= -\frac{n}{2} \log\left(2\pi \alpha^2 \right) - \frac{1}{2}\log\text{det}(C) -
\frac{1}{2\alpha^2} (y_n - \mu_{\psi})^\top C^{-1} (y_n - \mu_{\psi}). \tag{25}
\end{align}
\]</span> The analytical derivations given below go through for a log marginal likelihood of this form. However, this doesn’t work for the common setting with an observation variance <span class="math inline">\(\sigma^2 &gt; 0\)</span>, since in this case the covariance assumes the form <span class="math display">\[
\begin{align}
C &amp;= \left(\alpha^2 k(X) + \sigma^2 I_n \right).
\end{align}
\]</span> This can be addressed via the simple reparameterization <span class="math display">\[
\begin{align}
\tilde{\alpha}^2 C &amp;:= \tilde{\alpha}^2\left(k(X) + \tilde{\sigma}^2 I_n \right).
\end{align}
\]</span> This gives the required form of the covariance, and maintains the same number of parameters as before. The one downside is that we lose the straightforward interpretation of the noise variance; the observation noise is now given by the product <span class="math inline">\(\tilde{\alpha}^2 \tilde{\sigma}^2\)</span> instead of being encoded in the single parameter <span class="math inline">\(\sigma^2\)</span>. This reparameterization is utilized in the R package <a href="https://cran.r-project.org/package=hetGP">hetGP</a>.</p>
<section id="plug-in-mle" class="level3">
<h3 class="anchored" data-anchor-id="plug-in-mle">Plug-In MLE</h3>
<p>Let’s consider optimizing the log marginal likelihood with respect to <span class="math inline">\(\alpha^2\)</span>. The partial derivative of (25) with respect to <span class="math inline">\(\alpha^2\)</span> is given by <span class="math display">\[
\begin{align}
\frac{\partial \mathcal{L}(\theta)}{\partial \alpha^2}
&amp;= -\frac{n}{2}\frac{2\pi}{2\pi \alpha^2} - \frac{(y_n - \mu_{\psi})^\top C^{-1} (y_n - \mu_{\psi})}{2\alpha^4} \newline
&amp;= -\frac{n}{2\alpha^2} - \frac{(y_n - \mu_{\psi})^\top C^{-1} (y_n - \mu_{\psi})}{2\alpha^4}.
\end{align}
\]</span> Setting this expression equal to zero and solving for <span class="math inline">\(\alpha^2\)</span> yields <span class="math display">\[
\begin{align}
\hat{\alpha}^2 &amp;= \frac{(y_n - \mu_{\psi})^\top C^{-1} (y_n - \mu_{\psi})}{n}.
\end{align}
\]</span> Following the same procedure as before, the estimate <span class="math inline">\(\hat{\alpha}^2\)</span> can be subbed in for <span class="math inline">\(\alpha^2\)</span> in <span class="math inline">\(\mathcal{L}(\theta)\)</span> to obtain the concentrated log marginal likelihood.</p>
</section>
<section id="closed-form-marginalization" class="level3">
<h3 class="anchored" data-anchor-id="closed-form-marginalization">Closed-Form Marginalization</h3>
</section>
</section>
<section id="bias-corrections" class="level2">
<h2 class="anchored" data-anchor-id="bias-corrections">Bias Corrections</h2>
</section>
</section>
<section id="bayesian-approaches" class="level1">
<h1>Bayesian Approaches</h1>
</section>
<section id="computational-considerations" class="level1">
<h1>Computational Considerations</h1>
<section id="log-marginal-likelihood" class="level2">
<h2 class="anchored" data-anchor-id="log-marginal-likelihood">Log Marginal Likelihood</h2>
<p>We start by considering the computation of the log marginal likelihood (16), <span class="math display">\[
\begin{align}
\mathcal{L}(\theta)
&amp;= -\frac{n}{2} \log(2\pi) -\frac{1}{2} \log \text{det}\left(C \right) -
\frac{1}{2} (y - \mu)^\top C^{-1} (y - \mu),
\end{align}
\]</span> where we now suppress all dependence on hyperparameters in the notation for succinctness. Since <span class="math inline">\(C = k(X) + \sigma^2 I_n\)</span> is positive definite, we may Cholesky decompose it as <span class="math inline">\(C = L L^\top\)</span>. Plugging this decomposition into the log marginal likelihood yields <span class="math display">\[
\begin{align}
\mathcal{L}(\theta)
&amp;= -\frac{n}{2} \log(2\pi) - \frac{1}{2} \log\text{det}\left(C\right) -
\frac{1}{2} (y_n - \mu)^\top \left(LL^\top \right)^{-1} (y_n - \mu).
\end{align}
\]</span> The log determinant and the quadratic term can both be conveniently written in terms of the Cholesky factor. These terms are given respectively by <span class="math display">\[
\begin{align}
\log\text{det}\left(LL^\top\right)
&amp;= \log\text{det}\left(L\right)^2
= 2 \log \prod_{i=1}^{n} L_{ii}
= 2 \sum_{i=1}^{n} \log\left(L_{ii} \right),
\end{align}
\]</span> and <span class="math display">\[
\begin{align}
(y_n - \mu)^\top \left(LL^\top \right)^{-1} (y_n - \mu)
&amp;= (y_n - \mu)^\top \left(L^{-1}\right)^\top L^{-1} (y_n - \mu)
= \lVert L^{-1}(y - \mu)\rVert_2^2.
\end{align}
\]</span> The linear solve <span class="math inline">\(L^{-1}(y - \mu)\)</span> can be computed in <span class="math inline">\(\mathcal{O}(n^2)\)</span> by exploiting the fact that the linear system has lower triangular structure. Plugging these terms back into the log marginal likelihood gives <span class="math display">\[
\begin{align}
\mathcal{L}(\theta)
&amp;= -\frac{n}{2} \log(2\pi) - \sum_{i=1}^{n} \log\left(L_{ii}\right) -
\frac{1}{2} \lVert L^{-1}(y - \mu)\rVert_2^2.
\end{align}
\]</span> Note that the Cholesky factor <span class="math inline">\(L\)</span> is a function of <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\sigma^2\)</span> and hence must be re-computed whenever the kernel hyperparameters or noise variances change.</p>
</section>
<section id="profile-log-marginal-likelihood-with-linear-mean-function" class="level2">
<h2 class="anchored" data-anchor-id="profile-log-marginal-likelihood-with-linear-mean-function">Profile Log Marginal Likelihood with Linear Mean Function</h2>
<p>We now consider computation of the concentrated marginal log-likelihood under a mean function of the form (11), <span class="math inline">\(\mu(x) = h(x)^\top \beta\)</span>, where the generalized least squares (GLS) estimator <span class="math inline">\(\hat{\beta} = \left(H^\top C^{-1} H\right)^{-1} H^\top C^{-1}y\)</span> (see (23)) is inserted in place of <span class="math inline">\(\beta\)</span>. We are thus considering the profile log marginal likelihood <span class="math display">\[
\begin{align}
\mathcal{L}(\theta)
&amp;= -\frac{n}{2} \log(2\pi) -\frac{1}{2} \log \text{det}\left(C \right) -
\frac{1}{2} (y - H\hat{\beta})^\top C^{-1} (y - H\hat{\beta}).
\end{align}
\]</span> We will derive a numerically stable implementation of this expression in two steps, first applying a Cholesky decomposition (as in the previous section), and then leveraging a QR decomposition as in a typical ordinary least squares (OLS) computation. We first write <span class="math inline">\(\hat{\beta}\)</span> in terms of the Cholesky factor <span class="math inline">\(L\)</span>, where <span class="math inline">\(C = LL^\top\)</span>: <span class="math display">\[
\begin{align}
\hat{\beta}
&amp;= \left(H^\top C^{-1} H\right)^{-1} H^\top C^{-1}y \newline
&amp;= \left(H^\top \left[LL^\top\right]^{-1} H\right)^{-1} H^\top \left[LL^\top\right]^{-1}y \newline
&amp;= \left(\left[L^{-1}H \right]^\top \left[L^{-1}H \right] \right)^{-1} \left[L^{-1}H \right]^\top
\left[L^{-1}y\right].
\end{align}
\]</span> Notice that the GLS computation boils down to two lower-triangular linear solves: <span class="math inline">\(L^{-1}H\)</span> and <span class="math inline">\(L^{-1}y\)</span>. However, the above expression still requires one non-triangular linear solve that we will now address via the QR decomposition. The above expression for <span class="math inline">\(\hat{\beta}\)</span> can be viewed as a standard OLS estimator with design matrix <span class="math inline">\(L^{-1}H\)</span> and response vector <span class="math inline">\(L^{-1}y\)</span>. At this point, we could adopt a standard OLS technique of taking the QR decomposition of the design matrix <span class="math inline">\(L^{-1}H\)</span>. This was my original thought, but I found a nice alternative looking through the code in the R <a href="https://github.com/cran/kergp/blob/master/R/logLikFuns.R">kergp</a> package (see the function <code>.logLikFun0</code> in the file <code>kergp/R/logLikFuns.R</code>). The approach is to compute the QR decomposition <span class="math display">\[
\begin{align}
\begin{bmatrix} L^{-1}H &amp; L^{-1}y \end{bmatrix} &amp;= QR = Q \begin{bmatrix} \tilde{R} &amp; r \end{bmatrix}.
\end{align}
\]</span> That is, we compute QR on the matrix formed by concatenating <span class="math inline">\(L^{-1}y\)</span> as an additional column on the design matrix <span class="math inline">\(L^{-1}H\)</span>. We have written the upper triangular matrix <span class="math inline">\(R \in \mathbb{R}^{(p+1) \times (p+1)}\)</span> as the concatenation of <span class="math inline">\(\tilde{R} \in \mathbb{R}^{(p+1) \times p}\)</span> and the vector <span class="math inline">\(r \in \mathbb{R}^{p+1}\)</span> so that <span class="math inline">\(L^{-1}H = Q\tilde{R}\)</span> and <span class="math inline">\(L^{-1}y = Qr\)</span>. We recall the basic properties of the QR decomposition: <span class="math inline">\(R\)</span> is upper triangular and invertible, and <span class="math inline">\(Q\)</span> has orthonormal columns with span equal to the column space of <span class="math inline">\(\begin{bmatrix} L^{-1}H &amp; L^{-1}y \end{bmatrix}\)</span>. Taking the QR decomposition of this concatenated matrix leads to a very nice expression for the quadratic form term of the profile log marginal likelihood. But first let’s rewrite <span class="math inline">\(\hat{\beta}\)</span> in terms of these QR factors: <span class="math display">\[
\begin{align}
\hat{\beta}
&amp;= \left(\left[L^{-1}H \right]^\top \left[L^{-1}H \right] \right)^{-1} \left[L^{-1}H \right]^\top
\left[L^{-1}y\right] \newline
&amp;= \left(\left[Q\tilde{R} \right]^\top \left[Q\tilde{R} \right] \right)^{-1} \left[Q\tilde{R} \right]^\top \left[Qr\right] \newline
&amp;= \left(\tilde{R}^\top Q^\top Q\tilde{R} \right)^{-1} \tilde{R}^\top Q^\top Qr \newline
&amp;= \left(\tilde{R}^\top \tilde{R} \right)^{-1} \tilde{R}^\top r,
\end{align}
\]</span> where we have used the fact that <span class="math inline">\(Q^\top Q\)</span> is the identity since <span class="math inline">\(Q\)</span> is orthogonal. Plugging this into the quadratic form term of the log likelihood gives <span class="math display">\[
\begin{align}
(y - H\hat{\beta})^\top C^{-1} (y - H\hat{\beta})
&amp;= (y - H\hat{\beta})^\top \left[LL^\top \right]^{-1} (y - H\hat{\beta}) \newline
&amp;= \lVert L^{-1}(y - H\hat{\beta}) \rVert_2^2 \newline
&amp;= \lVert L^{-1}y - L^{-1}H\hat{\beta} \rVert_2^2 \newline
&amp;= \lVert Qr - Q\tilde{R} \hat{\beta} \rVert_2^2 \newline
&amp;= \left\lVert Qr - Q\tilde{R} \left(\tilde{R}^\top \tilde{R} \right)^{-1} \tilde{R}^\top r \right\rVert_2^2 \newline
&amp;= \left\lVert Q\left[r - \tilde{R} \left(\tilde{R}^\top \tilde{R} \right)^{-1} \tilde{R}^\top r \right] \right\rVert_2^2 \newline
&amp;= \left\lVert r - \tilde{R} \left(\tilde{R}^\top \tilde{R} \right)^{-1} \tilde{R}^\top \right\rVert_2^2 \newline
&amp;= \left\lVert \left[I - \tilde{R} \left(\tilde{R}^\top \tilde{R} \right)^{-1} \tilde{R}^\top \right]r \right\rVert_2^2,
\end{align}
\]</span> where the penultimate line follows from the fact that <span class="math inline">\(Q\)</span> is orthogonal, and hence an isometry. At this point, notice that the matrix <span class="math inline">\(P := \tilde{R} \left(\tilde{R}^\top \tilde{R} \right)^{-1} \tilde{R}^\top\)</span> is the standard OLS projection matrix (i.e., hat matrix) constructed with the design matrix <span class="math inline">\(\tilde{R}\)</span>. Also, take care to notice that <span class="math inline">\(\tilde{R}\)</span> is not invertible (it is not even square). Using standard properties of the projection matrix, we know that <span class="math inline">\(P\)</span> has rank <span class="math inline">\(p\)</span>, since <span class="math inline">\(\tilde{R}\)</span> has rank <span class="math inline">\(p\)</span>. Also, since <span class="math inline">\(R\)</span> is upper triangular, then the last row of <span class="math inline">\(\tilde{R}\)</span> contains all zeros. Letting, <span class="math inline">\(e_j\)</span> denote the <span class="math inline">\(j^{\text{th}}\)</span> standard basis vector of <span class="math inline">\(\mathbb{R}^{p+1}\)</span>, this means that <span class="math display">\[
\begin{align}
\mathcal{R}(P) \perp \text{span}(e_{p+1}),
\end{align}
\]</span> where <span class="math inline">\(\mathcal{R}(P)\)</span> denotes the range (i.e., column space) of <span class="math inline">\(P\)</span>. The only subspace of <span class="math inline">\(\mathbb{R}^{p+1}\)</span> with rank <span class="math inline">\(p\)</span> and satisfying this property is <span class="math inline">\(\text{span}(e_1, \dots, e_p)\)</span>. The conclusion is that <span class="math inline">\(P\)</span> projects onto <span class="math inline">\(\text{span}(e_1, \dots, e_p)\)</span>, and thus the annihilator <span class="math inline">\(I - P\)</span> projects onto the orthogonal complement <span class="math inline">\(\text{span}(e_{p+1})\)</span>. We thus conclude, <span class="math display">\[
\begin{align}
\left\lVert \left[I - P\right]r \right\rVert_2^2
&amp;= \lVert \langle r, e_{p+1} \rangle e_{p+1} \rVert_2^2 \newline
&amp;= \lVert r_{p+1} e_{p+1} \rVert_2^2 \newline
&amp;= r_{p+1}^2,
\end{align}
\]</span> where <span class="math inline">\(r_{p+1}\)</span> is the last entry of <span class="math inline">\(r\)</span>; i.e., the bottom right entry of <span class="math inline">\(R\)</span>. We finally arrive at the expression for the concentrated log marginal likelihood <span class="math display">\[
\begin{align}
\mathcal{L}(\theta)
&amp;= -\frac{n}{2} \log(2\pi) - \sum_{i=1}^{n} \log\left(L_{ii}\right) -
\frac{1}{2} r_{p+1}^2.
\end{align}
\]</span></p>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ul>
<li>Surrogates (Gramacy)</li>
<li>Statistics or geostatistics? Sampling error or nugget effect? (Clark)</li>
<li>Michael Betencourt’s very nice <a href="https://betanalpha.github.io/assets/case_studies/gp_part3/part3.html#4_adding_an_informative_prior_for_the_length_scale">post</a> on setting priors on GP hyperparameters.</li>
<li>Large-Scale Spatial Modeling via Local Approximate Gaussian Processes in R. (nice discussion of setting ranges for hyperparameters in the appendix; see laGP function <code>darg</code>).</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>