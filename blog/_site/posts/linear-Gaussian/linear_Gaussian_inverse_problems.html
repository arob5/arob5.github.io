<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-07-03">

<title>Linear Gaussian Inverse Problems – Andrew G. Roberts</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Andrew G. Roberts</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Linear Gaussian Inverse Problems</h1>
            <p class="subtitle lead">Derivations and discussion of linear Gaussian inverse problems.</p>
                                <div class="quarto-categories">
                <div class="quarto-category">statistics</div>
                <div class="quarto-category">inverse-problem</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 3, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>This post focuses on Bayesian inverse problems with the following features: 1. Linear forward model. 2. Additive Gaussian observation noise. 3. Gaussian prior distribution. 4. Prior independence of the observation noise and prior.</p>
<p>We refer to such inverse problems as <em>linear Gaussian</em>. The typical Bayesian linear regression model with a Gaussian prior on the coefficients constitutes a common example of a linear Gaussian inverse problem. The assumptions of linearity and Gaussianity play quite nicely together, resulting in a closed-form Gaussian posterior distribution. Moreover, many extensions to nonlinear and/or non-Gaussian settings rely on methods rooted in our understanding of the linear Gaussian regime.</p>
<section id="setup" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Setup</h1>
<p>We consider the following linear Gaussian regression model <span class="math display">\[
\begin{align}
y &amp;= Gu + \epsilon \tag{1} \newline
\epsilon &amp;\sim \mathcal{N}(0, \Sigma) \newline
u &amp;\sim \mathcal{N}(m, C), &amp;&amp; u \perp \epsilon
\end{align}
\]</span> consisting of the observation (i.e., data) <span class="math inline">\(y \in \mathbb{R}^n\)</span>, parameter <span class="math inline">\(u \in \mathbb{R}^d\)</span>, noise <span class="math inline">\(\epsilon \in \mathbb{R}^n\)</span>, and linear forward model represented by the matrix <span class="math inline">\(G \in \mathbb{R}^{n \times d}\)</span>. The observation covariance <span class="math inline">\(\Sigma \in \mathbb{R}^{n \times n}\)</span> and prior covariance <span class="math inline">\(C \in \mathbb{R}^{d \times d}\)</span> are both fixed positive definite matrices. The vector <span class="math inline">\(m \in \mathbb{R}^d\)</span> is the <em>prior mean</em>. We write <span class="math inline">\(u \perp \epsilon\)</span> to indicate the key assumption that <span class="math inline">\(u\)</span> and <span class="math inline">\(\epsilon\)</span> are a priori probabilistically independent. The model (1) can equivalently be written as <span class="math display">\[
\begin{align}
y|u &amp;\sim \mathcal{N}(Gu, \Sigma) \tag{2} \newline
u &amp;\sim \mathcal{N}(m, C),
\end{align}
\]</span> which gives the explicit expression for the Gaussian likelihood <span class="math inline">\(p(y|u)\)</span>. The solution of the Bayesian inverse problem is the posterior distribution <span class="math inline">\(p(u|y)\)</span>. We provide two approaches to calculating this distribution below, which yield different (but equivalent) expressions.</p>
</section>
<section id="the-posterior-distribution" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> The Posterior Distribution</h1>
<p>The ubiquity of the linear Gaussian models stems from its analytic tractability. The assumptions of linearity and Gaussianity play nicely together, resulting in a Gaussian posterior distribution that we can characterize in closed form. In Bayesian statistics, this is called a <em>conjugate</em> model, as the prior and posterior belong to the same distributional family (Gaussian). Once it is established that the posterior is Gaussian, it remains only to specify its mean and covariance matrix. We give two equivalent expressions for these quantities below, with complete derivations given in the appendix. The first formulation is the one typically seen in a standard Bayesian linear regression context where <span class="math inline">\(d \leq n\)</span>. It is therefore advantageous to work with quantities in <span class="math inline">\(\mathbb{R}^d\)</span> rather than <span class="math inline">\(\mathbb{R}^n\)</span>. In the first set of equations, the main computation - the inversion of a matrix - takes place in <span class="math inline">\(\mathbb{R}^d\)</span>, so we refer to this formulation as the <em>parameter space update</em>. The second set of equations instead requires the inversion of a <span class="math inline">\(n \times n\)</span> matrix. This <em>data space update</em> in commonly seen in data assimilation applications where <span class="math inline">\(d &gt; n\)</span>.</p>
<blockquote class="blockquote">
<p>
<strong>Posterior moments: parameter space update.</strong><br> The posterior distribution under the linear Gaussian model (1) is Gaussian <span class="math inline">\(u|y \sim \mathcal{N}(m_{\star}, C_{\star})\)</span>, with moments <span class="math display">\[
  \begin{align}
  m_{\star} &amp;= C_{\star} \left[G^\top \Sigma^{-1}y + C^{-1}m \right] \tag{3} \newline
  C_{\star} &amp;= \left[G^\top \Sigma^{-1} G + C^{-1} \right]^{-1}.
  \end{align}
  \]</span>
</p>
</blockquote>
<blockquote class="blockquote">
<p>
<strong>Posterior moments: data space update.</strong><br> The posterior moments can equivalently be computed as <span class="math display">\[
  \begin{align}
  m_{\star} &amp;= m + CG^\top [GCG^\top + \Sigma]^{-1}(y - Gm) \tag{4} \newline
  C_{\star} &amp;= C - CG^\top [GCG^\top + \Sigma]^{-1} GC.
  \end{align}
  \]</span>
</p>
</blockquote>
</section>
<section id="the-optimization-perspective" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> The Optimization Perspective</h1>
<p>In this section we consider the maximum a posteriori (MAP) estimate</p>
<p><span class="math display">\[
\begin{equation}
u_{\star} := \text{argmax}_{u \in \mathbb{R}^d} p(u|y).
\end{equation}
\]</span></p>
<p>Since <span class="math inline">\(p(u|y)\)</span> is Gaussian, its mean corresponds with its mode. Thus, the MAP estimate <span class="math inline">\(u_{\star}\)</span> is equal to the posterior mean <span class="math inline">\(m_{\star}\)</span>, given in (3) or (4). Note that maximizing <span class="math inline">\(p(u|y)\)</span> is equivalent to minimizing <span class="math inline">\(-\log p(u|y)\)</span>, since the logarithm is monotonic. Therefore, we can view the MAP estimate as the minimizer of the loss function <span class="math display">\[
\begin{equation}
J(u) := -\log p(u|y) = -\log \mathcal{N}(y|Gu, \Sigma) - \log \mathcal{N}(u|m,C).
\end{equation}
\]</span> For a positive definite matrix <span class="math inline">\(A\)</span>, let’s introduce the following notation for the norm and inner product, weighted by <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[
\langle u, v \rangle_A := \langle A^{-1}u, v\rangle = u^\top A^{-1}v
\]</span></p>
<p><span class="math display">\[
\lVert u \rVert^2_{A} := \langle u, u\rangle_{A} = u^\top A^{-1}u,
\]</span></p>
<p>where <span class="math inline">\(\langle \cdot, \cdot \rangle\)</span> and <span class="math inline">\(\lVert \cdot \rVert\)</span> denote the standard Euclidean inner product and norm, respectively. By substituting this notation, and dropping additive constants that do not depend on <span class="math inline">\(u\)</span>, we find the following.</p>
<blockquote class="blockquote">
<p>
<strong>MAP Estimate.</strong><br> The MAP estimate of the linear Gaussian inverse problem (1) solves <span class="math display">\[
  \begin{equation}
  u_{\star} := \text{argmin}_{u \in \mathbb{R}^d} \ J(u),
  \end{equation}
  \]</span> with the loss function given by <span class="math display">\[
  \begin{equation}
  J(u) = \frac{1}{2} \lVert y - Gu\rVert^2_{\Sigma} + \frac{1}{2} \lVert u-m\rVert^2_{C}.
  \end{equation}
  \]</span>
</p>
</blockquote>
<p>Observe that the MAP estimate solves a regularized least squares (i.e., ridge regression) problem. The gradient and Hessian of this loss function prove to be useful quantities, both in practice and for theoretical understanding. The following result gives expressions for these quantities, with derivations provided in the appendix.</p>
<blockquote class="blockquote">
<p>
<strong>Gradient and Hessian of Loss.</strong> <br> The loss function <span class="math inline">\(J(u)\)</span> above has gradient <span class="math inline">\(\nabla J(u) \in \mathbb{R}^d\)</span> and Hessian <span class="math inline">\(\nabla^2 J(u) \in \mathbb{R}^{d \times d}\)</span> given by <span class="math display">\[
  \begin{align}
  \nabla J(u) &amp;= -G^\top \Sigma^{-1}(y-Gu) + C^{-1}(u-m) \newline
  \nabla^2 J(u) &amp;\equiv G^\top \Sigma^{-1}G + C^{-1}
  \end{align}
  \]</span>
</p>
</blockquote>
<p>Notice that the Hessian does not vary with <span class="math inline">\(u\)</span>, as expected since the loss <span class="math inline">\(J(u)\)</span> is a quadratic function. In addition, observe the similarity between the Hessian and the posterior covariance in (3). The following result summarizes this connection, providing a link between the optimization perspective and the posterior moments.</p>
<blockquote class="blockquote">
<p>
<strong>Connection with posterior moments.</strong> <br> The moments of the posterior distribution <span class="math inline">\(\mathcal{N}(m_{\star}, C_{\star})\)</span> satisfy <span class="math display">\[
  \begin{align}
  &amp;\nabla J(m_{\star}) = 0, &amp;&amp;C^{-1}_{\star} = \nabla^2 J.
  \end{align}
  \]</span> In words, this says that <br> 1. The posterior mean minimizes the loss function <span class="math inline">\(J(u)\)</span>. <br> 2. The posterior precision (inverse covariance) is given by the Hessian <span class="math inline">\(\nabla^2 J\)</span>.
</p>
</blockquote>
</section>
<section id="investigating-the-posterior-equations" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Investigating the Posterior Equations</h1>
<section id="the-posterior-covariance" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="the-posterior-covariance"><span class="header-section-number">4.1</span> The Posterior Covariance</h2>
<p>A first important observation is that the the posterior covariance <span class="math inline">\(\overline{C}\)</span> is independent of the data <span class="math inline">\(y\)</span>. In this sense, the specific data realization observed does not affect the uncertainty in the estimation of <span class="math inline">\(u\)</span>. The expression coming from the first derivation (4) tells us that the posterior <em>precision</em> (inverse covariance) <span class="math inline">\(\overline{C}^{-1}\)</span> is the sum of the prior precision <span class="math inline">\(C^{-1}\)</span> and <span class="math inline">\(G^\top \Sigma^{-1}G\)</span>, which is the observation precision <span class="math inline">\(\Sigma^{-1}\)</span> modified by the forward model. Since the posterior covariance is the inverse of <span class="math inline">\(G^\top \Sigma^{-1}G + C^{-1}\)</span>, we should verify that this matrix is indeed invertible. First, note that <span class="math inline">\(\Sigma^{-1}\)</span> and <span class="math inline">\(C^{-1}\)</span> are both positive definite, since the inverse of positive definite matrices are also positive definite. Thus, the factorization <span class="math inline">\(\Sigma^{-1} = SS^\top\)</span> exists, which implies <span class="math display">\[
\begin{align}
x^\top [G^\top \Sigma^{-1}G]x
&amp;= x^\top [G^\top SS^\top G]x
= \lVert S^\top Gx \rVert^2_2 \geq 0. \tag{10}
\end{align}
\]</span> That is, <span class="math inline">\(G^\top \Sigma^{-1}G\)</span> is positive semidefinite. Since the sum of a positive semidefinite and positive definite matrix is positive definite, then <span class="math inline">\(G^\top \Sigma^{-1}G + C^{-1}\)</span> is positive definite, and thus invertible.</p>
<p>The covariance expression in (9) provides an alternative perspective. First, the expression tells us that conditioning on the data <span class="math inline">\(y\)</span> always decreases variance. This can be seen by noting that the matrix <span class="math inline">\(CG^\top [GCG^\top + \Sigma]^{-1} GC\)</span> (which is subtracted from the prior covariance) is positive semidefinite, and thus in particular has nonnegative values on its diagonal. To show this, we use the fact that we have just proven that <span class="math inline">\([GCG^\top + \Sigma]^{-1}\)</span> is positive definite, and thus admits a decomposition of the form <span class="math inline">\(SS^\top\)</span>. Thus, <span class="math display">\[
\begin{align}
x^\top \left(CG^\top [GCG^\top + \Sigma]^{-1} GC\right) x
= x^\top \left(CG^\top SS^\top GC\right) x
= \lVert S^\top GCx \rVert_2^2 \geq 0, \tag{11}
\end{align}
\]</span> so <span class="math inline">\(CG^\top [GCG^\top + \Sigma]^{-1} GC\)</span> is indeed positive semidefinite. Note that the covariance expression in (9) can also be written as <span class="math display">\[
\begin{align}
\text{Cov}[u|y]
&amp;= \text{Cov}[u] - \text{Cov}[u,y] \text{Cov}[y]^{-1} \text{Cov}[y, u]. \tag{12}
\end{align}
\]</span></p>
</section>
</section>
<section id="posterior-predictive-distribution" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Posterior Predictive Distribution</h1>
<p>Suppose that we are now interested in the outputs <span class="math inline">\(\tilde{y} \in \mathbb{R}^m\)</span> resulting from the application of a new forward map <span class="math inline">\(\tilde{G} \in \mathbb{R}^{m \times d}\)</span>, assuming the same prior distribution on <span class="math inline">\(u\)</span> as in (1). That is, we extend the model as <span class="math display">\[
\begin{align}
y &amp;= Gu + \epsilon \newline
\tilde{y} &amp;= \tilde{G}u + \tilde{\epsilon} \newline
u &amp;\sim \mathcal{N}(m, C), &amp;&amp; u \perp \epsilon.
\end{align}
\]</span> It remains to specify the joint distribution of the noise <span class="math inline">\((\epsilon, \tilde{\epsilon})\)</span> terms. We will assume <span class="math display">\[
\begin{align}
\begin{bmatrix} \tilde{\epsilon} \newline \epsilon \end{bmatrix} \sim
\mathcal{N}\left(
\begin{bmatrix} 0 \newline 0 \end{bmatrix},
\begin{bmatrix} \tilde{\Sigma} &amp; \Sigma^\prime \newline
                [\Sigma^\prime]^\top &amp; \Sigma \end{bmatrix}
\right),
\end{align}
\]</span> along with the independence assumption <span class="math inline">\(u \perp (\epsilon, \tilde{\epsilon})\)</span>. The classic example of this setup is prediction at a new set of inputs using a linear regression model. In this context, <span class="math inline">\(G\)</span> represents the <em>design matrix</em> and <span class="math inline">\(\tilde{G}\)</span> is a new set of <span class="math inline">\(m\)</span> inputs at which we would like to predict the corresponding responses.</p>
<p>In the generic inverse problem formulation, we see that this is a question of propagating the posterior uncertainty in <span class="math inline">\(u\)</span> through a new forward model <span class="math inline">\(\tilde{G}\)</span>. Concretely, we are interested in characterizing the <em>posterior predictive</em> distribution <span class="math display">\[
\begin{align}
p(\tilde{y} | y)
&amp;= \int p(\tilde{y},u|y)du
= \int p(\tilde{y}|u,y)p(u|y) du.
\end{align}
\]</span> Notice that the term <span class="math inline">\(p(u|y)\)</span> is the posterior distribution of the model parameters <span class="math inline">\(u\)</span>. We begin by deriving the closed-form posterior predictive distribution in the common setting <span class="math inline">\(\Sigma^\prime = 0\)</span>. We then turn to the general setting where <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(\tilde{\epsilon}\)</span> may be correlated. In either case, the posterior predictive <span class="math inline">\(p(\tilde{y} | y)\)</span> is Gaussian. This is verified in the appendix by showing that <span class="math inline">\((\tilde{y}, y)\)</span> is joint Gaussian, and hence has Gaussian conditionals.</p>
<section id="uncorrelated-errors" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="uncorrelated-errors"><span class="header-section-number">5.1</span> Uncorrelated Errors</h2>
<p>We begin with the assumption of zero cross-covariance: <span class="math inline">\(\text{Cov}[\tilde{\epsilon}, \epsilon] = \Sigma^\prime = 0\)</span>. This assumption makes the required computations quite straightforward. As noted above, we know that <span class="math inline">\(p(\tilde{y}|y)\)</span> is Gaussian, and thus is only remains to characterize the mean and covariance of this distribution. For the mean, we apply the law of iterated expectations to obtain <span class="math display">\[
\begin{align}
\mathbb{E}[\tilde{y}|y]
&amp;= \mathbb{E}\left[\mathbb{E}[\tilde{y}|u,y]|y\right] \newline
&amp;= \mathbb{E}\left[\mathbb{E}[\tilde{G}u + \tilde{\epsilon}|u,y]|y\right] \newline
&amp;= \mathbb{E}\left[\tilde{G}u|y\right] \newline
&amp;= \tilde{G} \mathbb{E}\left[u|y\right] \newline
&amp;= \tilde{G} m_{\star}.
\end{align}
\]</span> Note that everything is conditional on <span class="math inline">\(y\)</span> here; the law of iterated expectations is applied with respect to <span class="math inline">\(u\)</span>, not <span class="math inline">\(y\)</span>. The third equality uses the fact that <span class="math inline">\(\mathbb{E}[\tilde{\epsilon}|u,y] = \mathbb{E}[\tilde{\epsilon}] = 0\)</span>, owing to the assumptions that <span class="math inline">\(\tilde{\epsilon}\)</span> is uncorrelated with both <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(u\)</span>. Now, for the covariance we apply the law of total covariance <span class="math display">\[
\begin{align}
\text{Cov}[x] = \mathbb{E}\text{Cov}[x|y] + \text{Cov}\left[\mathbb{E}(x|y) \right],
\end{align}
\]</span> which holds for arbitrary (square integrable) random vectors <span class="math inline">\(x,y\)</span>. Doing so (again, with everything conditional on <span class="math inline">\(y\)</span>) we obtain: <span class="math display">\[
\begin{align}
\text{Cov}[\tilde{y}|y]
&amp;= \mathbb{E}\left[\text{Cov}(\tilde{y}|u,y)|y\right] + \text{Cov}\left[\mathbb{E}(\tilde{y}|u,y)|y \right] \newline
&amp;= \mathbb{E}\left[\text{Cov}(\tilde{G}u + \tilde{\epsilon}|u,y)|y\right] + \text{Cov}\left[\tilde{G}u|y \right] \newline
&amp;= \mathbb{E}\left[\tilde{\Sigma}|y\right] + \text{Cov}\left[\tilde{G}u|y \right] \newline
&amp;= \tilde{\Sigma} + \tilde{G}C_{\star}\tilde{G}^\top.
\end{align}
\]</span> We have used the fact that <span class="math inline">\(\text{Cov}[\tilde{\epsilon}|u,y] = \text{Cov}[\tilde{\epsilon}] = \tilde{\epsilon}\)</span>, again due to the independence assumptions. Putting everything together, we have found that <span class="math display">\[
\begin{align}
\tilde{y}|y &amp;\sim \mathcal{N}\left(\tilde{G}m_{\star}, \tilde{\Sigma} + \tilde{G}C_{\star}\tilde{G}^\top \right).
\end{align}
\]</span> This result is quite intuitive. The predictive mean simply results from propagating the posterior mean <span class="math inline">\(m_{\star}\)</span> through the new forward model <span class="math inline">\(\tilde{G}\)</span>. The predictive covariance is the sum of the noise covariance <span class="math inline">\(\tilde{\Sigma}\)</span> and the covariance resulting from propagating the random variable <span class="math inline">\(u|y\)</span> through the new forward model <span class="math inline">\(\tilde{G}\)</span>.</p>
</section>
<section id="correlated-errors" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="correlated-errors"><span class="header-section-number">5.2</span> Correlated Errors</h2>
<p>We now deal with the general case where <span class="math inline">\(\Sigma^\prime\)</span> may be non-zero. The calculations here are trickier given that <span class="math inline">\(\tilde{\epsilon}|y\)</span> is not equal in distribution to <span class="math inline">\(\tilde{\epsilon}\)</span>, as was the case above. The reason is that <span class="math inline">\(y\)</span> contains information about <span class="math inline">\(\epsilon\)</span>, and <span class="math inline">\(\epsilon\)</span> contains information about <span class="math inline">\(\tilde{\epsilon}\)</span> since they are correlated. Therefore, in this general setting I find it easiest to proceed by considering the joint distribution <span class="math inline">\((\tilde{y},y)\)</span> and then applying the Gaussian conditioning identities to obtain the distribution of <span class="math inline">\(\tilde{y}|y\)</span>. The joint distribution is given by <span class="math display">\[
\begin{align}
\begin{bmatrix} \tilde{y} \newline y \end{bmatrix}
&amp;\sim \mathcal{N}\left(
\begin{bmatrix} \tilde{G}m \newline Gm \end{bmatrix},
\begin{bmatrix} \tilde{G}C\tilde{G}^\top + \tilde{\Sigma} &amp;
                \tilde{G}CG^\top + \Sigma^\prime \newline
                GC\tilde{G}^\top + [\Sigma^\prime]^\top &amp;
                GCG^\top + \Sigma \end{bmatrix}
\right).
\end{align}
\]</span> The cross covariance follows from <span class="math display">\[
\begin{align}
\text{Cov}[\tilde{G}u + \tilde{\epsilon}, Gu + \epsilon]
&amp;= \text{Cov}[\tilde{G}u, Gu] + \text{Cov}[\tilde{\epsilon}, \epsilon]
= \tilde{G}CG^\top + \Sigma^\prime,
\end{align}
\]</span> since <span class="math inline">\(\text{Cov}[u, \tilde{\epsilon}] = \text{Cov}[u, \epsilon] = 0\)</span>. The marginal calculations follow similarly. We can now apply the Gaussian conditioning identities to obtain</p>
<p><span class="math display">\[
\begin{align}
\mathbb{E}[\tilde{y}|y]
&amp;= \tilde{G}m + \left[\tilde{G}CG^\top + \Sigma^\prime \right]
\left(GCG^\top + \Sigma \right)^{-1} \left[y - Gm \right] \newline
\text{Cov}[\tilde{y}|y]
&amp;= \tilde{\Sigma} + \tilde{G}C\tilde{G}^\top -
\left[\tilde{G}CG^\top + \Sigma^\prime \right]
\left(GCG^\top + \Sigma \right)^{-1}
\left[GC\tilde{G}^\top + [\Sigma^\prime]^\top \right]
\end{align}
\]</span></p>
</section>
</section>
<section id="example-linear-regression" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Example: Linear Regression</h1>
<p>Perhaps the most common example of a linear Gaussian model is Bayesian linear regression. In this setting, we suppose that we have access to observed input-output pairs <span class="math inline">\((x_i, y_i)_{i=1}^{n}\)</span> and assume that the <span class="math inline">\(y_i\)</span> arise as a linear function of the <span class="math inline">\(x_i\)</span>, which are then perturbed by noise. While different formulations are possible, a popular specification assumes <span class="math display">\[
\begin{equation}
y_i|x_i, \beta \sim \mathcal{N}(x_i^\top \beta, \sigma^2),
\end{equation}
\]</span> meaning that the magnitude of the observation noise is iid across observations. If we stack the inputs row-wise into a matrix <span class="math inline">\(X \in \mathbb{R}^{n \times d}\)</span> and the outputs into a vector <span class="math inline">\(y \in \mathbb{R}^n\)</span>, then this model can be written as <span class="math display">\[
\begin{align}
y &amp;= X\beta + \epsilon \newline
\epsilon &amp;\sim \mathcal{N}(0, \sigma^2 I_n) \newline
\beta &amp;\sim \mathcal{N}(m, \sigma^2 C),
\end{align}
\]</span> where we have also assumed a Gaussian prior on <span class="math inline">\(\beta\)</span>. Connecting to our generic inverse problem setup, we see that the forward model <span class="math inline">\(G = X\)</span> is given by the data matrix, while the parameter <span class="math inline">\(u = \beta\)</span> is the coefficient vector. The parameterization of the prior covariance as <span class="math inline">\(\sigma^2 C\)</span> is common in this setting, as it will lead to some convenient cancellations in the posterior formulas. Indeed, applying (3) gives the posterior <span class="math inline">\(\beta|y \sim \mathcal{N}(m_{\star}, C_{\star})\)</span>, with covariance <span class="math display">\[
\begin{align}
C_{\star} &amp;= \left[\frac{1}{\sigma^2} X^\top X + \frac{1}{\sigma^2} C^{-1} \right]^{-1}
= \sigma^2 \left[X^\top X + C^{-1} \right]^{-1}.
\end{align}
\]</span> The posterior mean is thus <span class="math display">\[
\begin{equation}
m_{\star}
= C_{\star} \left[\frac{1}{\sigma^2} X^\top y + \frac{1}{\sigma^2} C^{-1}m \right]
= \left[X^\top X + C^{-1} \right]^{-1} \left[X^\top y + C^{-1}m \right],
\end{equation}
\]</span> since the <span class="math inline">\(\sigma^2\)</span> term from the covariance cancels with its reciprocal.</p>
</section>
<section id="appendix" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Appendix</h1>
<section id="joint-gaussian-distribution" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="joint-gaussian-distribution"><span class="header-section-number">7.1</span> Joint Gaussian Distribution</h2>
<p>Throughout this post we rely on the claim that various quantities are jointly Gaussian distributed. We verify these claims here. In the proofs, we use the fact that a random vector <span class="math inline">\(v\)</span> is Gaussian if and only if it is equal in distribution to <span class="math inline">\(a + Bz\)</span>, for <span class="math inline">\(z \sim \mathcal{N}(0, I)\)</span> and some constant vector <span class="math inline">\(a\)</span> and matrix <span class="math inline">\(B\)</span>. Any random variables labelled “z” in this section should be interpreted as standard Gaussians, with subscripts potentially indicating the random variables that they generate; e.g., <span class="math inline">\(z_{\epsilon}\)</span>.</p>
<section id="joint-distribution-u-y." class="level3" data-number="7.1.1">
<h3 data-number="7.1.1" class="anchored" data-anchor-id="joint-distribution-u-y."><span class="header-section-number">7.1.1</span> Joint distribution: <span class="math inline">\((u, y)\)</span>.</h3>
<p>We first verify that the vector <span class="math inline">\((u, y)\)</span> has a joint Gaussian distribution under model (1). Taking the square roots of the covariance matrices allows us to write the correlated Gaussian variables as transformations of iid Gaussian noise. We have, <span class="math display">\[
\begin{align}
\begin{bmatrix} u \newline y \end{bmatrix}
&amp;\overset{d}{=}
\begin{bmatrix} Gu + \epsilon \newline u \end{bmatrix} \newline
&amp;\overset{d}{=}
\begin{bmatrix} G\left(m + C^{1/2}z_u\right) + \Sigma^{1/2}z_{\epsilon} \newline m + C^{1/2}z_u \end{bmatrix} \newline
&amp;\overset{d}{=}
\begin{bmatrix} Gu \newline m \end{bmatrix} +
\begin{bmatrix} GC^{1/2}z_u + \Sigma^{1/2}z_{\epsilon} \newline
                 C^{1/2}z_u \end{bmatrix} \newline
&amp;\overset{d}{=}
\begin{bmatrix} Gu \newline m \end{bmatrix} +
\begin{bmatrix} GC^{1/2} &amp; \Sigma^{1/2} \newline C^{1/2} &amp; 0 \end{bmatrix}
\begin{bmatrix} z_u \newline z_{\epsilon} \end{bmatrix}.
\end{align}
\]</span> Under the assumption <span class="math inline">\(u \perp \epsilon\)</span> it follows that <span class="math inline">\(z_u\)</span> and <span class="math inline">\(z_{\epsilon}\)</span> are independent, so <span class="math inline">\((z_u, z_{\epsilon})^\top \sim \mathcal{N}(0, I_{d+n})\)</span>, thus verifying the claim.</p>
</section>
<section id="joint-distribution-tildey-y." class="level3" data-number="7.1.2">
<h3 data-number="7.1.2" class="anchored" data-anchor-id="joint-distribution-tildey-y."><span class="header-section-number">7.1.2</span> Joint distribution: <span class="math inline">\((\tilde{y}, y)\)</span>.</h3>
<p>We similarly show that <span class="math inline">\((\tilde{y}, y)\)</span> is joint Gaussian, under model (13). This fact is used in the derivation of the posterior predictive distribution. We recall that we are allowing the noise terms <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(\tilde{\epsilon}\)</span> to be correlated; let’s partition the square root of their joint covariance by <span class="math display">\[
\begin{align}
\text{Cov}[(\tilde{\epsilon}, \epsilon)]^{1/2}
&amp;= \begin{bmatrix} B_{11} &amp; B_{12} \newline B_{21} &amp; B_{22} \end{bmatrix}.
\end{align}
\]</span> We then have <span class="math display">\[
\begin{align}
\begin{bmatrix} \tilde{y} \newline y \end{bmatrix}
&amp;\overset{d}{=}
\begin{bmatrix} \tilde{G}u \newline Gu\end{bmatrix} +
\begin{bmatrix} \tilde{\epsilon} \newline \epsilon \end{bmatrix} \newline
&amp;\overset{d}{=}
\begin{bmatrix} \tilde{G}\left(m + C^{1/2}z_u \right)\newline G\left(m + C^{1/2}z_u \right) \end{bmatrix} +
\begin{bmatrix} B_{11} &amp; B_{12} \newline B_{21} &amp; B_{22} \end{bmatrix}
\begin{bmatrix} z_1 \newline z_2 \end{bmatrix} \newline
&amp;\overset{d}{=}
\begin{bmatrix} \tilde{G}m \newline Gm \end{bmatrix} +
\begin{bmatrix} \tilde{G}C^{1/2} &amp; B_{11} &amp; B_{12} \newline
                 GC^{1/2} &amp; B_{21} &amp; B_{22} \end{bmatrix}
\begin{bmatrix} z_u \newline z_1 \newline z_2 \end{bmatrix}.
\end{align}
\]</span> Again, the independence of <span class="math inline">\(z_u\)</span> with respect to <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span> is assured by the assumptions <span class="math inline">\(u \perp \tilde{\epsilon}\)</span> and <span class="math inline">\(u \perp \epsilon\)</span>. Thus, <span class="math inline">\((z_u, z_1, z_2)^\top \sim \mathcal{N}(0, I_{d+m+n})\)</span>. The matrices <span class="math inline">\(B_{12}\)</span> and <span class="math inline">\(B_{21}\)</span> serve to “mix up” the two independent noise sources <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span> in order to produce the correlations between <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(\tilde{\epsilon}\)</span>. If <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(\tilde{\epsilon}\)</span> are assumed uncorrelated then <span class="math inline">\(B_{12} = B_{21} = 0\)</span>. Note also that essentially the same derivations show that <span class="math inline">\((\tilde{y}, y, u)\)</span> are also jointly Gaussian; the “B” matrix is simply augmented with the addition of the row <span class="math inline">\(\begin{bmatrix} C^{1/2} &amp; 0 &amp; 0 \end{bmatrix}\)</span>.</p>
</section>
</section>
<section id="posterior-derivations" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="posterior-derivations"><span class="header-section-number">7.2</span> Posterior Derivations</h2>
<section id="parameter-space-update" class="level3" data-number="7.2.1">
<h3 data-number="7.2.1" class="anchored" data-anchor-id="parameter-space-update"><span class="header-section-number">7.2.1</span> Parameter Space Update</h3>
<p>We now derive the posterior moments appearing in (3). This derivation also provides another way to show that the posterior is Gaussian. The strategy here is to apply Bayes’ theorem and then use the matrix analog of completing the square from elementary algebra. Any quantities that do not depend on <span class="math inline">\(u\)</span> will be absorbed in the proportionality symbol, and hence are dropped from the expression. Applying Bayes’ theorem to (2) yields <span class="math display">\[
\begin{align}
p(u|y)
&amp;\propto p(y|u)p(u) \newline
&amp;\propto \exp\left(-\frac{1}{2}\left[(y-Gu)^\top \Sigma^{-1} (y-Gu) + (u-m)^\top C^{-1} (u-m) \right] \right) \newline
&amp;\propto \exp\left(-\frac{1}{2}\left[u^\top(G^\top \Sigma^{-1}G + C^{-1})u -
2u^\top(G^\top \Sigma^{-1}y + C^{-1}m)\right] \right). \tag{A1}
\end{align}
\]</span> All we have done above is to combine the Gaussian likelihood and prior, dropping any multiplicative constants that don’t depend on <span class="math inline">\(u\)</span>, and grouping like terms in <span class="math inline">\(u\)</span>. Note that since (A1) is an exponential of a quadratic in <span class="math inline">\(u\)</span>, then we immediately know that the posterior must be Gaussian. It therefore remains to find the mean <span class="math inline">\(m_{\star}\)</span> and covariance <span class="math inline">\(C_{\star}\)</span>. Knowing that (A1) is proportional to a Gaussian density, let’s set the term in square brackets equal to <span class="math display">\[\begin{align}
(u - m_{\star})^\top C_{\star}^{-1} (u - m_{\star})
= u^\top C_{\star}^{-1}u - 2u^\top C_{\star}^{-1} m_{\star} +
m_{\star}^\top C_{\star}^{-1} m_{\star} \tag{A2}
\end{align}\]</span> and equate like terms to solve for the unknowns <span class="math inline">\(m_{\star}\)</span> and <span class="math inline">\(C_{\star}\)</span>. Doing so, we find that <span class="math display">\[
\begin{align}
C_{\star}^{-1} &amp;= G^\top \Sigma^{-1} G + C^{-1} \newline
C_{\star}^{-1}m_{\star} &amp;= G^\top \Sigma^{-1}y + C^{-1}m. \tag{A3}
\end{align}
\]</span> The <span class="math inline">\(m_{\star}^\top C_{\star}^{-1} m_{\star}\)</span> is not a problem, as it will simply be absorbed in the proportionality sign. Rearranging the above expressions gives the desired mean and covariance equations in (3).</p>
</section>
<section id="data-space-update" class="level3" data-number="7.2.2">
<h3 data-number="7.2.2" class="anchored" data-anchor-id="data-space-update"><span class="header-section-number">7.2.2</span> Data Space Update</h3>
<p>We now present a second method for computing <span class="math inline">\(p(u|y)\)</span>, resulting in the data space update (4). The general idea here is to observe that <span class="math inline">\((u, y)^\top \in \mathbb{R}^{d+n}\)</span> follows a joint Gaussian distribution, and thus the posterior distribution <span class="math inline">\(p(u|y)\)</span> is simply a conditional of this joint distribution. Since <a href="https://en.wikipedia.org/w/index.php?title=Multivariate_normal_distribution&amp;action=edit&amp;section=26">Gaussian conditionals</a> can be characterized in closed-form, we can simply apply well-known Gaussian conditional formulas to obtain the equations in (4). Note that we have already verified earlier in the appendix that <span class="math inline">\((u,y)^\top\)</span> is a Gaussian random vector, which relied on the assumed prior independence of <span class="math inline">\(u\)</span> and <span class="math inline">\(\epsilon\)</span>. Writing out this joint Gaussian explicitly gives <span class="math display">\[
\begin{align}
\begin{bmatrix} u \newline y \end{bmatrix}
&amp;\sim \mathcal{N}\left(
\begin{bmatrix} m \newline Gm \end{bmatrix},
\begin{bmatrix} C &amp; CG^\top \newline GC &amp; GCG^\top + \Sigma \end{bmatrix}
\right). \tag{A4}
\end{align}
\]</span> The mean and covariance of <span class="math inline">\(u\)</span> are immediate from (1), and the remaining quantities are computed as: <span class="math display">\[
\begin{align}
\mathbb{E}[y] &amp;= \mathbb{E}[Gu + \epsilon]
= G\mathbb{E}[u] + \mathbb{E}[\epsilon] = Gm \tag{A5} \newline
\text{Cov}[y] &amp;= \text{Cov}[Gu + \epsilon]
= \text{Cov}[Gu] + \text{Cov}[\epsilon] = GCG^\top + \Sigma \tag{A6} \newline
\text{Cov}[y, u] &amp;= \text{Cov}[Gu + \epsilon, u]
= \text{Cov}[Gu, u] + \text{Cov}[\epsilon, u]
= GC. \tag{A7}
\end{align}
\]</span> In (A5) we use the linearity of expectation and the fact that the noise is zero-mean. The covariance splits into the sum in (A6) due to the independence of <span class="math inline">\(u\)</span> and <span class="math inline">\(\epsilon\)</span>. This independence assumption is similarly leveraged in (A7). The Gaussian conditioning <a href="https://en.wikipedia.org/w/index.php?title=Multivariate_normal_distribution&amp;action=edit&amp;section=26">identities</a> give <span class="math display">\[
\begin{align}
m_{\star} &amp;= \mathbb{E}[u] - \text{Cov}[u,y]\text{Cov}[y]^{-1}(y-\mathbb{E}[y]) \tag{A8} \newline
C_{\star} &amp;= \text{Cov}[u] - \text{Cov}[u,y]\text{Cov}[y]^{-1}\text{Cov}[y,u].
\end{align}
\]</span> Inserting the above expressions for the means and covariances recovers the data space update (4).</p>



</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>