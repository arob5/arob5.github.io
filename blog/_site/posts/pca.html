<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2023-12-15">
<meta name="description" content="A deep dive into PCA.">

<title>Principal Components Analysis – Andrew G. Roberts</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Andrew G. Roberts</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="../about.qmd"> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Principal Components Analysis</h1>
                  <div>
        <div class="description">
          A deep dive into PCA.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Statistics</div>
                <div class="quarto-category">Linear-Algebra</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 15, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="part-1-formulating-and-solving-the-pca-optimization-problem" class="level1">
<h1>Part 1: Formulating and Solving the PCA Optimization Problem</h1>
<section id="setup-and-notation" class="level2">
<h2 class="anchored" data-anchor-id="setup-and-notation">Setup and Notation</h2>
<p>Suppose that we have data <span class="math inline">\(\mathbf{x}_1, \dots, \mathbf{x}_N \in \mathbb{R}^D\)</span>, stacked into the rows of a matrix <span class="math inline">\(X \in \mathbb{R}^{N \times D}\)</span>. Our task is to find a subspace of smaller dimension <span class="math inline">\(R &lt; D\)</span> such that the projection of the data points onto the subspace retains as much information as possible. By restricting our attention to orthonormal bases for the low-dimensional subspace, we reduce the problem to finding a set of orthonormal basis vectors <span class="math display">\[
\begin{align}
&amp;\mathbf{b}_1, \dots, \mathbf{b}_R \in \mathbb{R}^D,
&amp;&amp;\langle \mathbf{b}_r, \mathbf{b}_s \rangle = \delta_{r,s}.
\end{align}
\]</span> Define <span class="math inline">\(B \in \mathbb{R}^{D \times R}\)</span> to be the matrix with <span class="math inline">\(r^{\text{th}}\)</span> column equal to <span class="math inline">\(\mathbf{b}_r\)</span>. The subspace generated by the basis <span class="math inline">\(B\)</span> is given by <span class="math display">\[
\text{span}(B) := \text{span}(\mathbf{b}_1, \dots, \mathbf{b}_R).
\]</span> Throughout this post I will abuse notation by referring to the matrix <span class="math inline">\(B\)</span> when actually talking about the set of vectors <span class="math inline">\(\{\mathbf{b}_1, \dots, \mathbf{b}_R\}\)</span>. Since there is no a priori reason to assume that the data is centered, we should also allow for the subspace to be shifted by some intercept <span class="math inline">\(\mathbf{w}_0 \in \mathbb{R}^D\)</span>, resulting in the affine space <span class="math display">\[
\mathbf{w}_0 + \text{span}(B) = \left\{\mathbf{w}_0 +
\sum_{r=1}^{R} w_r \mathbf{b}_r : w_1, \dots, w_R \in \mathbb{R} \right\}.
\]</span> Loosely speaking, the task is to find the basis <span class="math inline">\(B\)</span>, intercept <span class="math inline">\(\mathbf{w}_0\)</span>, and pointwise weights <span class="math inline">\(\mathbf{w}_1, \dots, \mathbf{w}_N \in \mathbb{R}^R\)</span> such that <span class="math display">\[
\begin{align}
\mathbf{x}_n &amp;\approx \mathbf{w}_0 + \sum_{r=1}^{R} (\mathbf{w}_n)_r \mathbf{b}_r &amp;&amp;\forall n=1,\dots,N \\
&amp;= \mathbf{w}_0 + B\mathbf{w}_n.
\end{align}
\]</span> To formalize this notion, PCA measures the error in the above approximation using Euclidean distance, averaged over the <span class="math inline">\(N\)</span> data points. To further simplify notation, we stack the <span class="math inline">\(\mathbf{w}_n\)</span> in the columns of a matrix <span class="math inline">\(W \in \mathbb{R}^{R \times N}\)</span>. With all of this notation established, we can state that PCA solves the optimization problem <span class="math display">\[
\text{argmin}_{B, W, \mathbf{w}_0} \sum_{n=1}^{N} \lVert \mathbf{x}_n - (\mathbf{w}_0 + B\mathbf{w}_n) \rVert_2^2, \tag{1}
\]</span> where the basis <span class="math inline">\(B\)</span> is constrained to be orthonormal. As we will see, this optimization naturally breaks down into two distinct problems which can be solved sequentially: 1. Given the basis <span class="math inline">\(B\)</span> and intercept <span class="math inline">\(\mathbf{w}_0\)</span>, find the optimal basis coefficients <span class="math inline">\(\mathbf{w}_n\)</span> corresponding to each data point <span class="math inline">\(\mathbf{x}_n\)</span>. 2. Find the optimal basis and intercept.</p>
<p>Part of the popularity of PCA stems from the fact that both problems can be solved in closed-form. Let us consider both problems in turn.</p>
</section>
<section id="optimizing-the-basis-coefficients" class="level2">
<h2 class="anchored" data-anchor-id="optimizing-the-basis-coefficients">Optimizing the Basis Coefficients</h2>
<p>Let us first consider <span class="math inline">\(\mathbf{w}_0\)</span> and <span class="math inline">\(B\)</span> to be fixed, meaning that we are fixing an affine subspace of dimension <span class="math inline">\(R\)</span>. We seek to find the optimal way to represent the data <span class="math inline">\(X\)</span> in this lower-dimensional space. As we will show, the Euclidean objective used by PCA implies that this problem reduces to straightforward orthogonal projection. For now, let <span class="math inline">\(\mathbf{x}^c_n := \mathbf{x}_n - \mathbf{w}_0\)</span> denote the centered data points (we will deal with the intercept shortly). We are thus considering the problem <span class="math display">\[
\text{argmin}_{W} \sum_{n=1}^{N} \lVert \mathbf{x}^c_n - B\mathbf{w}_n \rVert_2^2 \tag{2}
\]</span> Observe that <span class="math inline">\(\mathbf{w}_n\)</span> only appears in the <span class="math inline">\(n^{\text{th}}\)</span> term of the sum, meaning that we can consider each summand independently, <span class="math display">\[
\text{argmin}_{\mathbf{w}_n} \lVert \mathbf{x}^c_n - B\mathbf{w}_n \rVert_2^2.
\]</span> In words, we seek the linear combination of the basis vectors <span class="math inline">\(B\)</span> that results in minimal Euclidean distance from <span class="math inline">\(\mathbf{x}^c_n\)</span>; this is a standard orthogonal projection problem from linear algebra. Since the basis vectors are orthonormal, the optimal projection coefficients are given by <span class="math display">\[
\begin{align}
&amp;(\mathbf{w}_n)_r = \langle \mathbf{x}_n^c, \mathbf{b}_r \rangle,
&amp;&amp;\mathbf{w}_n = B^\top \mathbf{x}_n^c
\end{align}
\]</span> which can be written succinctly for all data points by stacking the <span class="math inline">\(\mathbf{w}_n^\top\)</span> as rows in a matrix <span class="math inline">\(W\)</span>; i.e., <span class="math display">\[
W := X^c B,
\]</span> with <span class="math inline">\(X^c\)</span> denoting the centered data matrix with rows set to the <span class="math inline">\((\mathbf{x}^c_n)^\top\)</span>.</p>
</section>
<section id="optimizing-the-basis" class="level2">
<h2 class="anchored" data-anchor-id="optimizing-the-basis">Optimizing the Basis</h2>
<p>In the previous section, we saw that for a fixed basis and intercept, optimizing the basis weights reduced to an orthogonal projection problem. In this section we show that with the weights fixed at their optimal values, optimizing the basis reduces to solving a sequence of eigenvalue problems. To be clear, we are now considering the problem <span class="math display">\[
\text{argmin}_{B} \sum_{n=1}^{N} \lVert \mathbf{x}^c_n - B\mathbf{w}^*_n \rVert_2^2, \tag{3}
\]</span> where the <span class="math inline">\(\mathbf{w}^*_n\)</span> are now fixed at the optimal values satisfying (2); i.e., <span class="math inline">\(\mathbf{w}^*_n = B^\top \mathbf{x}^c_n\)</span>. However, in the derivations below we will just write <span class="math inline">\(\mathbf{w}_n = \mathbf{w}^*_n\)</span> to keep the notation lighter. Note that we are still treating <span class="math inline">\(\mathbf{w}_0\)</span> as fixed for the time being. We will make another notational simplification in this section by writing <span class="math inline">\(\mathbf{x}_n = \mathbf{x}_n^c\)</span>. Just keep in mind that throughout this section, <span class="math inline">\(\mathbf{x}_n\)</span> should be interpreted as <span class="math inline">\(\mathbf{x}_n - \mathbf{w}_0\)</span>.</p>
<p>This problem is also referred to as minimizing the <em>reconstruction error</em>, since <span class="math inline">\(\lVert \mathbf{x}_n - \mathbf{\hat{x}}_n \rVert_2 := \lVert \mathbf{x}_n - B\mathbf{w}_n \rVert_2\)</span> is the error between the original data point <span class="math inline">\(\mathbf{x}_n\)</span> and the <span class="math inline">\(D\)</span>-dimensional vector <span class="math inline">\(\mathbf{\hat{x}}_n\)</span> which can be thought of as an approximation to <span class="math inline">\(\mathbf{x}_n\)</span> that has been <em>reconstructed</em> from its lower-dimensional representation <span class="math inline">\(\mathbf{w}_n\)</span>. The key here is to re-write this objective function so that this optimization problem takes the form of an eigenvalue problem, which is something that we already know how to solve (see the appendix, A1).</p>
<p>To start, we extend the orthonormal set <span class="math inline">\(\mathbf{b}_1, \dots, \mathbf{b}_R\)</span> to an orthonormal basis <span class="math inline">\(\mathbf{b}_1, \dots, \mathbf{b}_D\)</span> for <span class="math inline">\(\mathbb{R}^D\)</span>. Now we can write the original data point <span class="math inline">\(\mathbf{x}_n\)</span> and its approximation <span class="math inline">\(\mathbf{\hat{x}}_n\)</span> with respect to this basis as <span class="math display">\[
\begin{align}
&amp;\mathbf{x}_n = \sum_{r=1}^{D} \langle \mathbf{x}_n, \mathbf{b}_r \rangle \mathbf{b}_r,
&amp;&amp;\mathbf{\hat{x}}_n = \sum_{r=1}^{R} \langle \mathbf{x}_n, \mathbf{b}_r \rangle \mathbf{b}_r
\end{align}
\]</span></p>
<p>and hence the residual <span class="math inline">\(\mathbf{x}_n - \mathbf{\hat{x}}_n\)</span> is given by <span class="math display">\[
\begin{align}
\mathbf{x}_n - \mathbf{\hat{x}}_n &amp;= \sum_{r=R+1}^{D} \langle \mathbf{x}_n, \mathbf{b}_r \rangle \mathbf{b}_r.
\end{align}
\]</span></p>
<p>Thus, the objective function in (3) can be written as <span class="math display">\[
\sum_{n=1}^{N} \lVert \mathbf{x}_n - \mathbf{\hat{x}}_n \rVert_2^2
= \sum_{n=1}^{N} \bigg\lVert \sum_{r=R+1}^{D} \langle \mathbf{x}_n, \mathbf{b}_r \rangle \mathbf{b}_r \bigg\rVert_2^2
= \sum_{n=1}^{N} \sum_{r=R+1}^{D} \langle \mathbf{x}_n, \mathbf{b}_r \rangle^2 \lVert \mathbf{b}_r \rVert_2^2
= \sum_{n=1}^{N} \sum_{r=R+1}^{D} \langle \mathbf{x}_n, \mathbf{b}_r \rangle^2,
\]</span></p>
<p>where the second and third equalities use the facts that the <span class="math inline">\(\mathbf{b}_r\)</span> are orthogonal and of unit norm, respectively.</p>
<p>We could continue working with this formulation, but at this point it is convenient to re-write the minimization problem we have been working with as an equivalent maximization problem. Note that the above residual calculation is of the form <span class="math inline">\(\mathbf{\hat{e}}_n = \mathbf{x}_n - \mathbf{\hat{x}}_n\)</span> (and summed over <span class="math inline">\(n\)</span>). Since <span class="math inline">\(\mathbf{x}_n\)</span> is fixed, then minimizing the residual (i.e., the reconstruction error) is equivalent to maximizing <span class="math inline">\(\mathbf{\hat{x}}_n\)</span>. More rigorously, we have</p>
<p><span class="math display">\[
\begin{align}
\text{argmin}_B \sum_{n=1}^{N} \sum_{r=R+1}^{D} \langle \mathbf{x}_n, \mathbf{b}_r \rangle^2
&amp;= \text{argmin}_B \sum_{n=1}^{N}
\left(\sum_{r=1}^{D} \langle \mathbf{x}_n, \mathbf{b}_r \rangle^2 - \sum\_{r=1}^{R} \langle \mathbf{x}_n, \mathbf{b}_r \rangle^2\right) \\
&amp;= \text{argmin}_B \sum_{n=1}^{N}
\left(\lVert \mathbf{x}_n \rVert_2^2 - \sum_{r=1}^{R} \langle \mathbf{x}_n, \mathbf{b}_r \rangle^2\right) \\
&amp;= \text{argmax}_B \sum_{n=1}^{N} \sum_{r=1}^{R} \langle \mathbf{x}_n, \mathbf{b}_r \rangle^2. \tag{4}
\end{align}
\]</span></p>
<p>We can now re-write the squared inner product to obtain <span class="math display">\[
\begin{align}
\sum_{n=1}^{N} \sum_{r=1}^{R} \langle \mathbf{x}_n, \mathbf{b}_r \rangle^2
&amp;= \sum_{n=1}^{N} \sum_{r=1}^{R} \mathbf{b}_r^\top \mathbf{x}_n \mathbf{x}_n^\top \mathbf{b}_r
= \sum_{r=1}^{R} \mathbf{b}_r^\top \left(\sum_{n=1}^{N}\mathbf{x}_n \mathbf{x}_n^\top\right) \mathbf{b}_r
= \sum_{r=1}^{R} \mathbf{b}_r^\top (X^\top X) \mathbf{b}_r^\top,
\end{align}
\]</span></p>
<p>where the final step uses <a href="https://gregorygundersen.com/blog/2020/07/17/matmul/">this</a> fact. We have managed to re-write (3) as <span class="math display">\[
\text{argmax}_{B} \sum_{r=1}^{D} \mathbf{b}_r^\top (X^\top X) \mathbf{b}_r^\top, \tag{5}
\]</span> where we recall that this is also subject to the constraint that <span class="math inline">\(B\)</span> is orthogonal.</p>
<p>Before proceeding, we note that <span class="math inline">\(X^\top X\)</span> is a positive semi-definite matrix, whose eigenvalues we denote <span class="math inline">\(\lambda_1, \dots, \lambda_D\)</span>, sorted in decreasing order. Note that the eigenvalues are all non-negative due to the positive definiteness. Let <span class="math inline">\(\mathbf{e}_1, \dots, \mathbf{e}_D\)</span> denote the respective eigenvectors, normalized to have unit norm. These vectors are guaranteed to be orthogonal by the Spectral Theorem.</p>
<p>We now notice in (5) that the objective function has been decomposed into <span class="math inline">\(R\)</span> different terms, each of which only depends on a single <span class="math inline">\(\mathbf{b}_r\)</span>. However, these do not constitute <span class="math inline">\(R\)</span> independent optimization problems, as they are all coupled through the orthogonality constraint. We will thus consider solving them in a recursive fashion, beginning with the first term, <span class="math display">\[
\text{argmax}_{\lVert \mathbf{b}_1 \rVert_2=1} \mathbf{b}_1^\top (X^\top X) \mathbf{b}_1^\top
= \text{argmax}_{\lVert \mathbf{b}_1 \rVert_2=1} \lVert X \mathbf{b}_1 \rVert_2^2.
\]</span> This is an eigenvalue problem! It is precisely of the form (A4) (see appendix) and so we apply that result to conclude that the optimal argument is <span class="math inline">\(\mathbf{b}_1 = \mathbf{e}_1\)</span> with associated optimal value <span class="math inline">\(\lambda_1\)</span> (note the objective here is the <em>squared</em> norm, in contrast to the statement in the appendix). Taking this as the base case, we now proceed inductively. Assume that at the <span class="math inline">\(r^{\text{th}}\)</span> problem in the sequence, the solution is given by <span class="math inline">\((\mathbf{b}_1, \dots, \mathbf{b}_r) = (\mathbf{e}_1, \dots, \mathbf{e}_r)\)</span>. We must show the solution to the <span class="math inline">\((r+1)^{\text{st}}\)</span> problem is <span class="math inline">\(\mathbf{e}_{r+1}\)</span>. Under the inductive hypothesis, this problem is constrained so that <span class="math inline">\(\mathbf{b}_{r+1}\)</span> is orthogonal to each of <span class="math inline">\(\mathbf{e}_1, \dots, \mathbf{e}_r\)</span>; i.e., we require <span class="math inline">\(\mathbf{b}_{r+1} \perp \text{span}(\mathbf{e}_1, \dots, \mathbf{e}_r)\)</span>. If we denote <span class="math inline">\(\mathcal{E}_{r} := \text{span}(\mathbf{e}_1, \dots, \mathbf{e}_r)\)</span> and <span class="math inline">\(\mathcal{E}^{\perp}_{r}\)</span> the orthogonal complement of <span class="math inline">\(\mathcal{E}_{r}\)</span>, then a succinct way to write the orthogonality constraint is that <span class="math inline">\(\mathbf{b}_{r+1} \in \mathcal{E}^{\perp}_r\)</span>. The problem can thus be written as <span class="math display">\[
\begin{align}
\text{argmax}_{\mathbf{b}_{r+1} \in \mathcal{E}^{\perp}_{r}, \lVert \mathbf{b}_{r+1} \rVert_2=1} \lVert X \mathbf{b}_{r+1} \rVert_2^2, \tag{6}
\end{align}
\]</span> which is another eigenvalue problem, precisely of the form (A3). Using this result from the appendix, we conclude that this is solved by <span class="math inline">\(\mathbf{b}_{r+1} = \mathbf{e}_{r+1}\)</span>, with the maximal objective value <span class="math inline">\(\lambda_{r+1}\)</span>.</p>
<p>That was a lot, so before moving on let’s briefly summarize. First of all, recall that I have been abusing notation by writing <span class="math inline">\(\mathbf{x}_n\)</span> where I should be writing <span class="math inline">\(\mathbf{x}_n^c = \mathbf{x}_n - \mathbf{w}_0\)</span>. In summarizing the result here I will make this correction. Here we have considered the problem of finding the optimal orthonormal basis <span class="math inline">\(B\)</span>, for any fixed <span class="math inline">\(\mathbf{w}_0 \in \mathbb{R}^D\)</span>, but with the <span class="math inline">\(\mathbf{w}_n\)</span> set to their optimal values satisfying (2); i.e., <span class="math inline">\(\mathbf{w}_n = B^\top \mathbf{x}^c_n\)</span>. Given this, we showed that the reconstruction error (5) is minimized by setting <span class="math inline">\(B\)</span> equal to the matrix with columns given by the dominant <span class="math inline">\(R\)</span> (normalized) eigenvectors of <span class="math inline">\((X^c)^\top X^c\)</span>. We arrived at this solution by showing that the error minimization problem (5) could be viewed as a sequence of <span class="math inline">\(R\)</span> eigenvalue problems.</p>
</section>
<section id="optimizing-the-intercept" class="level2">
<h2 class="anchored" data-anchor-id="optimizing-the-intercept">Optimizing the Intercept</h2>
<p>The last ingredient we are missing to solve (1) is the optimal value of <span class="math inline">\(\mathbf{w}_0\)</span>, which has henceforth been viewed as fixed in the above derivations. At first glance, this problem might seem like somewhat of an afterthought, but there are some subtleties that are worth exploring here.</p>
<p>The problem we are now considering is <span class="math display">\[
\text{argmin}_{\mathbf{w}_0} \sum_{n=1}^{N} \lVert \mathbf{x}_n - \mathbf{w}_0 - B\mathbf{w}^*_n \rVert_2^2, \tag{7}
\]</span> with <span class="math inline">\(\mathbf{w}^*_n\)</span> denoting the optimal weights <span class="math inline">\(\mathbf{w}^*_n = B^\top \mathbf{x}_n\)</span> derived above (these derivations will go through with any orthonormal basis <span class="math inline">\(B\)</span>). Plugging in this expression for <span class="math inline">\(\mathbf{w}^*_n\)</span> gives <span class="math display">\[
\sum_{n=1}^{N} \lVert \mathbf{x}_n - \mathbf{w}_0 - B\mathbf{w}^*_n \rVert_2^2
= \sum_{n=1}^{N} \lVert \mathbf{x}_n - \mathbf{w}_0 - BB^\top \mathbf{x}_n \rVert_2^2
= \sum_{n=1}^{N} \lVert (I - BB^\top)(\mathbf{x}_n - \mathbf{w}_0) \rVert_2^2.
\]</span> Computing the gradient of this expression with respect to <span class="math inline">\(\mathbf{w}_0\)</span> and setting it equal to zero yields the optimality condition <span class="math display">\[
\sum_{n=1}^{N} (I - BB^\top)(\mathbf{x}_n - \mathbf{w}_0) = 0,
\]</span> where we have used the fact that <span class="math inline">\((I - BB^\top)^2 = (I - BB^\top)\)</span> (since <span class="math inline">\(I - BB^\top\)</span> is a projection matrix; see appendix). By linearity we then have <span class="math display">\[
\sum_{n=1}^{N} (I - BB^\top)(\mathbf{x}_n - \mathbf{w}_0)
= (I - BB^\top) \sum_{n=1}^{N} (\mathbf{x}_n - \mathbf{w}_0)
= (I - BB^\top)(N \bar{\mathbf{x}} - N\mathbf{w}_0), \tag{7}
\]</span> where we have defined <span class="math display">\[
\bar{\mathbf{x}} := \frac{1}{N} \sum_{n=1}^{N} \mathbf{x}_n,
\]</span> the empirical mean of the data. Since <span class="math inline">\(\mathbf{w}_0\)</span> is optimal when (8) is equal to zero, this leads to the condition <span class="math display">\[
(I - BB^\top)(\bar{\mathbf{x}} - \mathbf{w}_0) = 0,
\]</span> or equivalently, <span class="math display">\[
\bar{\mathbf{x}} - \mathbf{w}_0 \in \text{Null}(I - BB^\top).
\]</span> Noting that the null space is non-trivial, since <span class="math inline">\(\text{Null}(I - BB^\top) = \text{span}(\mathbf{b}_1, \dots, \mathbf{b}_R)\)</span> (again, see the appendix section on projection matrices), we then conclude that there are infinitely many optimal solutions! Using the basis <span class="math inline">\(\mathbf{b}_1, \dots, \mathbf{b}_R\)</span> for the null space, we can characterize the set of optimal <span class="math inline">\(\mathbf{w}_0\)</span> as those satisfying <span class="math display">\[
\bar{\mathbf{x}} - \mathbf{w}_0 \in \text{span}(\mathbf{b}_1, \dots, \mathbf{b}_R),
\]</span> or, more explicitly, all vectors within the affine space <span class="math display">\[
\mathbf{w}_0 \in \bar{\mathbf{x}} + \text{span}(\mathbf{b}_1, \dots, \mathbf{b}_R). \tag{9}
\]</span> While we have an infinity of optimal solutions we could choose from, the obvious choice <span class="math inline">\(\mathbf{w}_0^* := \bar{\mathbf{x}}\)</span> stands out. Indeed, this is the choice that is essentially always made in practice, so much so that many PCA tutorials will begin by assuming that the data points have all been centered by subtracting off their empirical mean. I find it more insightful to include the intercept as a variable in the PCA optimization problem, and then show that the choice to set it equal to <span class="math inline">\(\bar{\mathbf{x}}\)</span> is actually justified.</p>
<p>Moreover, it is quite interesting that the mean is not actually the unique optimal choice here. Why is this? The characterization (9) says that we can add any vector lying in the span of the orthonormal basis to <span class="math inline">\(\bar{\mathbf{x}}\)</span> and still maintain optimality. So the key requirement is that, after shifting the data by subtracting off <span class="math inline">\(\mathbf{w}_0\)</span>, the resulting shifted points must “lie along” the lower-dimensional subspace <span class="math inline">\(\text{span}(B)\)</span>. Since, <span class="math inline">\(\text{span}(B)\)</span> defines a hyperplane, the data must lie somewhere along this plane; from the perspective of the optimization problem, it doesn’t matter whether it lies around the origin or somewhere very far away, so long as it is clustered around this plane. A picture is worth a thousand words here, and I will try to add one once I have time.</p>
<p>Finally, note that the specific choice of <span class="math inline">\(\bar{\mathbf{x}}\)</span> has various other practical benefits. It leads to projections that are clustered around the origin, thus keeping numbers relatively small. It also leads to a nice statistical interpretation of the eigenvalue problems discussed in the previous subsection; e.g.&nbsp;the basis vector <span class="math inline">\(\mathbf{b}_1\)</span> can be viewed as the direction along which the empirical variance of the projected data is maximized. This maximum variance perspective is discussed in more detail below.</p>
</section>
</section>
<section id="part-2-interpreting-pca" class="level1">
<h1>Part 2: Interpreting PCA</h1>
<section id="minimum-error-or-maximum-variance" class="level2">
<h2 class="anchored" data-anchor-id="minimum-error-or-maximum-variance">Minimum Error or Maximum Variance?</h2>
<p>While the derivations in the preceding section are somewhat lengthy, recall that this was all in the pursuit of solving the optimization problem (1). In words, we derived the best <span class="math inline">\(R\)</span>-dimensional affine subspace to represent the data <span class="math inline">\(X\)</span>, where “best” is defined as minimizing the average Euclidean error between the data points and their projections onto the subspace. We showed that this error minimization problem could be re-written as a sequence of <span class="math inline">\(R\)</span> maximization problems of the form (6). We now show that these maximization problems have a very nice statistical interpretation.</p>
<section id="sample-covariance-of-the-mathbfx_n" class="level3">
<h3 class="anchored" data-anchor-id="sample-covariance-of-the-mathbfx_n">Sample covariance of the <span class="math inline">\(\mathbf{x}_n\)</span></h3>
<p>We first recall that the empirical covariance matrix of the data points <span class="math inline">\(\mathbf{x}_1, \dots, \mathbf{x}_N\)</span> is defined to be <span class="math display">\[
\hat{C} := \frac{1}{N-1} \sum_{n=1}^{N} (\mathbf{x}_n - \bar{\mathbf{x}}) (\mathbf{x}_n - \bar{\mathbf{x}})^\top,
\]</span> which can be re-written as <span class="math display">\[
\hat{C} = \frac{1}{N-1} \sum_{n=1}^{N} \mathbf{x}^c_n (\mathbf{x}^c_n)^\top = \frac{1}{N-1} X^c (X^c)^\top, \tag{10}
\]</span> where the superscript <em>c</em> indicates that the observations have been centered by subtracting off their empirical mean.</p>
<p>Recall that solving the maximization problems (6) revealed that the optimal basis vectors are given by the dominant eigenvectors of the matrix <span class="math inline">\(X^c (X^c)^\top\)</span>, which is the (unscaled) covariance (10)! The scaling factor does not affect the optimal basis vectors, it simply scales the objective function. Specifically, <span class="math display">\[
\begin{align}
\text{argmax}_{\mathbf{b}_{r+1} \in \mathcal{E}^{\perp}_{r}, \lVert \mathbf{b}_{r+1} \rVert_2=1}
\left(\mathbf{b}_{r+1}^\top X^c (X^c)^\top \mathbf{b}_{r+1}\right)
= \text{argmax}_{\mathbf{b}_{r+1} \in \mathcal{E}^{\perp}_{r}, \lVert \mathbf{b}_{r+1} \rVert_2=1}
\left(\mathbf{b}_{r+1}^\top \hat{C} \mathbf{b}_{r+1}\right). \tag{11}
\end{align}
\]</span> We haven’t changed anything from (6) here, other than noting that a re-scaling of the objective function allows us involve <span class="math inline">\(\hat{C}\)</span> in the expression.</p>
</section>
<section id="sample-covariance-of-the-mathbfw_n" class="level3">
<h3 class="anchored" data-anchor-id="sample-covariance-of-the-mathbfw_n">Sample covariance of the <span class="math inline">\(\mathbf{w}_n\)</span></h3>
<p>Given that the sample covariance matrix of the data <span class="math inline">\(\mathbf{x}_1, \dots, \mathbf{x}_N \in \mathbb{R}^D\)</span> is given by <span class="math inline">\(\hat{C}\)</span>, it is natural to also consider the empirical covariance of <span class="math inline">\(\mathbf{w}_1, \dots, \mathbf{w}_N \in \mathbb{R}^R\)</span>. We begin by computing the sample mean <span class="math display">\[
\frac{1}{N} \sum_{n=1}^{N} \mathbf{w}_n
= \frac{1}{N} \sum_{n=1}^{N} B\mathbf{x}^c_n
= B \left[\frac{1}{N} \sum_{n=1}^{N} \mathbf{x}^c_n \right] = 0,
\]</span> using the fact that the empirical mean of the centered data points is <span class="math inline">\(0\)</span>. Recalling that the row vectors <span class="math inline">\(\mathbf{w}_n^\top\)</span> are stored in the rows of the matrix <span class="math inline">\(W\)</span>, it follows that the empirical covariance matrix of <span class="math inline">\(\mathbf{w}_1, \dots, \mathbf{w}_N\)</span> is given by <span class="math display">\[
\hat{C}_w := \frac{1}{N-1} W^\top W, \tag{12}
\]</span> which follows from the calculation (10) with <span class="math inline">\(W\)</span> in place of <span class="math inline">\(X^c\)</span>. Since <span class="math inline">\(W = XB\)</span>, we have <span class="math display">\[
\hat{C}_w = \frac{1}{N-1} (XB)^\top (XB) = B^\top \hat{C} B,
\]</span> which allows us to write the covariance of the <span class="math inline">\(\mathbf{w}_n\)</span> as a function of the covariance of the <span class="math inline">\(\mathbf{x}_n\)</span>. We can say even more since we know that <span class="math inline">\(B\)</span> is given by <span class="math inline">\(V_R\)</span>, the truncated set of eigenvectors obtained from the eigendecomposition <span class="math inline">\(X^\top X = V \Lambda V^\top\)</span>. We thus have <span class="math display">\[
\hat{C}_w
= \frac{1}{N-1} B^\top (X^\top X) B
= \frac{1}{N-1} V_R^\top (X^\top X) V_R
= \frac{1}{N-1} V_R^\top V_R \Lambda_R
= \frac{1}{N-1} \Lambda_R,  \tag{13}
\]</span> where <span class="math inline">\((X^\top X) V_R = V_R \Lambda_R\)</span> follows from the fact that the columns of <span class="math inline">\(V_R\)</span> store the first <span class="math inline">\(R\)</span> eigenvectors of <span class="math inline">\(X^\top X\)</span>. The conclusion here is that <span class="math inline">\(\hat{C}_w\)</span> is diagonal, with variances equal to the eigenvalues of <span class="math inline">\(\hat{C}\)</span>. In other words, PCA computes a change-of-basis that diagonalizes the empirical covariance of the data.</p>
</section>
<section id="pca-as-variance-maximization" class="level3">
<h3 class="anchored" data-anchor-id="pca-as-variance-maximization">PCA as Variance Maximization</h3>
<p>We now return to the goal of providing a statistical interpretation of the objective function <span class="math inline">\(\mathbf{b}_{r}^\top \hat{C} \mathbf{b}_r\)</span> in (11). Given the derivation of <span class="math inline">\(\hat{C}_w\)</span> in (13), we see the empirical variance of <span class="math inline">\((\mathbf{w}_1)_r, \dots, (\mathbf{w}_N)_r\)</span> (i.e., the values in the <span class="math inline">\(r^{\text{th}}\)</span> column of <span class="math inline">\(W\)</span>) is equal to <span class="math display">\[
[\hat{C}_w]_{rr} = \mathbf{b}_r^\top \hat{C} \mathbf{b}_r,
\]</span> which is precisely the objective being maximized. To interpret this quantity more clearly, we consider the projection of <span class="math inline">\(\mathbf{x}_n^c\)</span> onto the span of <span class="math inline">\(\mathbf{b}_r\)</span>, <span class="math display">\[
\text{proj}_{\mathbf{b}_r} \mathbf{x}^c_n
:= \langle \mathbf{x}^c_n, \mathbf{b}_r \rangle \mathbf{b}_r
= (\mathbf{w}_n)_r \mathbf{b}_r,
\]</span> which implies that <span class="math inline">\((\mathbf{w}_n)_r\)</span> is the magnitude of the projection. Therefore, we conclude that <span class="math inline">\(\mathbf{b}_r^\top \hat{C} \mathbf{b}_r\)</span> is the sample variance of the magnitude of the projections onto the subspace <span class="math inline">\(\text{span}(\mathbf{b}_r)\)</span>; loosely speaking, the variance of the projection along the <span class="math inline">\(r^{\text{th}}\)</span> basis vector. Combining all of these equivalent expressions yields the chain of equalities, <span class="math display">\[
\text{Tr}(\hat{C}_w)
= \frac{1}{N-1} \sum_{r=1}^{R} \lambda_r
= \sum_{n=1}^{N} \sum_{r=1}^{R} W_{nr}^2.
= \sum_{r=1}^{R} \mathbf{b}_r^\top \hat{C} \mathbf{b}_r \tag{15}
\]</span> The trace <span class="math inline">\(\text{Tr}(\hat{C}_w)\)</span> thus represents the total variance of the projection, summed over all of the basis vectors. The total variance is equivalently given by the sum of the eigenvalues of <span class="math inline">\(\hat{C}\)</span> or by the squared Frobenius norm <span class="math inline">\(\lVert W \rVert_F^2\)</span>.</p>
<p>The final term in (15) provides an alternative interpretation of the objective function in (5); namely, that PCA seeks the basis <span class="math inline">\(B\)</span> which results in the maximal projected total variance. The resulting sequence of constrained problems, as in (11), are interpreted similarly. In particular, <span class="math display">\[
\text{argmax}_{\mathbf{b}_{r+1} \in \mathcal{E}^{\perp}_{r}, \lVert \mathbf{b}_{r+1} \rVert_2=1}
\left(\mathbf{b}_{r+1}^\top \hat{C} \mathbf{b}_{r+1}\right)
\]</span> can be viewed as seeking the direction along which the variance of the projections is maximized, subject to the constraint that the direction be orthogonal to the previous <span class="math inline">\(r\)</span> directions. The optimal solution is the direction corresponding to the <span class="math inline">\((r+1)^{\text{st}}\)</span> eigenvector of the empirical covariance <span class="math inline">\(\hat{C}\)</span>, and the resulting maximal variance in this direction is given by the associated eigenvalue.</p>
</section>
</section>
<section id="the-singular-value-decomposition" class="level2">
<h2 class="anchored" data-anchor-id="the-singular-value-decomposition">The Singular Value Decomposition</h2>
</section>
<section id="a-matrix-approximation-problem-the-eckart-young-theorem" class="level2">
<h2 class="anchored" data-anchor-id="a-matrix-approximation-problem-the-eckart-young-theorem">A Matrix Approximation Problem: The Eckart-Young Theorem</h2>
<p>Ignoring the intercept (or assuming that the <span class="math inline">\(\mathbf{x}_n\)</span> have already been centered), we can re-write the reconstruction error as <span class="math display">\[
\sum_{n=1}^{N} \lVert \mathbf{x}_n - B\mathbf{w}_n \rVert_2^2
= \lVert X - WB^\top \rVert_F^2,
\]</span> where <span class="math inline">\(\lVert \cdot \rVert_F\)</span> denotes the <a href="https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm">Frobenius norm</a>. The PCA optimization problem can then be written as the matrix approximation problem <span class="math display">\[
\text{argmin}_{B, W} \lVert X - WB^\top \rVert_F^2, \tag{10}
\]</span> where <span class="math inline">\(B\)</span> is constrained to be an orthogonal matrix. We can equivalently write this as <span class="math display">\[
\begin{align}
&amp;\text{argmin}_{\hat{X} \in \mathcal{M}} \lVert X - \hat{X} \rVert_F^2,
&amp;&amp;\mathcal{M} = \{\hat{X} \in \mathbb{R}^{N \times D} : \hat{X}=WB^\top, B \in \mathbb{R}^{D \times R} \text{ is orthogonal}\}, \tag{11}
\end{align}
\]</span> which makes it even more clear that PCA can generically be viewed as the problem of approximating the data matrix <span class="math inline">\(X\)</span> with another matrix <span class="math inline">\(\hat{X}\)</span> that is constrained to lie in a subset <span class="math inline">\(\mathcal{M}\)</span> of all <span class="math inline">\(N \times D\)</span> matrices. We can phrase this even more succinctly by noticing that <span class="math inline">\(\mathcal{M}\)</span> is precisely the set of all <span class="math inline">\(N \times D\)</span> matrices with rank at most <span class="math inline">\(R\)</span>. Indeed, if <span class="math inline">\(\hat{X} \in \mathcal{M}\)</span> then <span class="math inline">\(\text{rank}(\hat{X}) = \text{rank}(W B^\top) \leq R\)</span> since <span class="math inline">\(W\)</span> has <span class="math inline">\(R\)</span> columns and thus the rank of this matrix product cannot exceed <span class="math inline">\(R\)</span>. Conversely, if we let <span class="math inline">\(\hat{X}\)</span> be an arbirary <span class="math inline">\(N \times D\)</span> matrix of rank <span class="math inline">\(r \leq R\)</span> then <span class="math inline">\(\hat{X}\)</span> can be expanded using the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition#Reduced_SVDs">compact SVD</a> as <span class="math display">\[
\hat{X} = U\tilde{\Sigma}V^\top = (U\tilde{\Sigma})V^\top,
\]</span> where <span class="math inline">\(U \in \mathbb{R}^{N \times r}\)</span>, <span class="math inline">\(V \in \mathbb{R}^{D \times r}\)</span> are orthogonal and <span class="math inline">\(\tilde{\Sigma} \in \mathbb{R}^{r \times r}\)</span> is diagonal. By setting <span class="math inline">\(W := U\tilde{\Sigma}\)</span> and <span class="math inline">\(B := V\)</span> we have almost written things in the form required by (11), but (11) restricts <span class="math inline">\(B\)</span> to be orthogonal with exactly <span class="math inline">\(R\)</span> columns. Thus, we can simply extend <span class="math inline">\(V\)</span> to have <span class="math inline">\(R\)</span> orthonormal columns by appending columns <span class="math inline">\(B = [V|\mathbf{v}_{r+1}, \dots, \mathbf{v}_{R}]\)</span> (this is justified by Gram-Schmidt) and then set <span class="math inline">\(W = [U\tilde{\Sigma}|\boldsymbol{0}]\)</span>. Thus, <span class="math inline">\(\hat{X} = (U\tilde{\Sigma})V^\top = WB^\top\)</span> with <span class="math inline">\(B\)</span> now of the required form.</p>
<p>We have thus verified that (11) can equivalently be written as the low-rank matrix approximation problem <span class="math display">\[
\begin{align}
&amp;\text{argmin}_{\hat{X} \in \mathcal{M}} \lVert X - \hat{X} \rVert_F^2,
&amp;&amp;\mathcal{M} = \{\hat{X} \in \mathbb{R}^{N \times D} : \text{rank}(\hat{X}) \leq R \}. \tag{12}
\end{align}
\]</span></p>
<p>This is precisely the problem considered by the celebrated <a href="https://en.wikipedia.org/wiki/Low-rank_approximation">Eckart-Young theorem</a>. The theorem concludes that the optimal solution to (12) is given by the truncated SVD <span class="math inline">\(X = U_R \Sigma_R V_R^\top\)</span>, which is precisely the PCA solution computed using SVD as discussed in the previous section.</p>
</section>
<section id="regression-with-an-optimal-basis" class="level2">
<h2 class="anchored" data-anchor-id="regression-with-an-optimal-basis">Regression with an optimal basis</h2>
</section>
<section id="statisticalprobabalistic-perspectives" class="level2">
<h2 class="anchored" data-anchor-id="statisticalprobabalistic-perspectives">Statistical/Probabalistic Perspectives</h2>
<p>There are various ways we might attach a statistical or probabilistic perspective to PCA - we briefly discuss a few of them here. Throughout this section we will take <span class="math display">\[
\text{argmin}_{W,B} \sum_{n=1}^{N} \lVert \mathbf{x}_n - B\mathbf{w}_n \rVert_2^2  \tag{13}
\]</span> as the jumping off point for a probabilistic generalization; that is, we are implicitly assuming the data is centered so that we can ignore the intercept. Note that, as always, <span class="math inline">\(B\)</span> is constrained to be orthogonal.</p>
<section id="a-special-case-of-the-karhunen-loeve-expansion" class="level3">
<h3 class="anchored" data-anchor-id="a-special-case-of-the-karhunen-loeve-expansion">A Special Case of the Karhunen-Loeve Expansion</h3>
<p>We start by noting that the objective function in (13) looks like a sample average of the quantities <span class="math inline">\(\lVert \mathbf{x}_n - B\mathbf{w}_n \rVert_2^2\)</span>. It is therefore natural to consider some underlying true expectation that this sample average is approximating. To this end, let us view <span class="math inline">\(\mathbf{x} = \mathbf{x}(\omega)\)</span> and <span class="math inline">\(\mathbf{w} = \mathbf{w}(\omega)\)</span> as random vectors, defined over some probability space <span class="math inline">\((\Omega, \mathcal{A}, \mathbb{P})\)</span>. We can then consider <span class="math display">\[
\text{argmin}_{\mathbf{w},B} \mathbb{E} \lVert \mathbf{x} - B\mathbf{w} \rVert_2^2
= \text{argmin}_{\mathbf{w},B} \int_{\Omega} \left\lVert \mathbf{x}(\omega) - \sum_{r=1}^{R} \mathbf{w}_r(\omega) \mathbf{b}_r \right\rVert_2^2 \mathbb{P}(d\omega), \tag{14}
\]</span> which can be viewed as the population analog of the sample approximation in (13). We note that the second expression above shows that the low-rank approximation of the random vector <span class="math inline">\(\mathbf{x}\)</span> takes the form of a linear combination of (non-random) basis vectors, with random coefficients.</p>
<p>With <span class="math inline">\(B\)</span> fixed, the optimization in <span class="math inline">\(\mathbf{w}\)</span> is just as easy as before. Indeed, for any fixed <span class="math inline">\(\omega \in \Omega\)</span>, <span class="math display">\[
\text{argmin}_{\mathbf{w}(\omega)} \lVert \mathbf{x}(\omega) - B\mathbf{w}(\omega) \rVert_2^2 = B^\top \mathbf{x}(\omega),
\]</span> using the same exact orthogonal projection reasoning as before. We have thus optimized for <span class="math inline">\(\mathbf{w}(\omega)\)</span> on an <span class="math inline">\(\omega\)</span>-by-<span class="math inline">\(\omega\)</span> basis. The same result thus holds in expectation: <span class="math display">\[
\text{argmin}_{\mathbf{w}} \mathbb{E} \lVert \mathbf{x} - B\mathbf{w} \rVert_2^2 = B^\top \mathbf{x}.
\]</span> It is important to note that we have found the optimal <em>random vector</em> <span class="math inline">\(\mathbf{w}\)</span>, which was shown to be a linear transformation of the random vector <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p>Optimizing for <span class="math inline">\(B\)</span> with the optimal <span class="math inline">\(\mathbf{w}\)</span> fixed also follows similarly from previous derivations. Using some of the results already derived above (see the section on optimizing the basis), we have</p>
<p><span class="math display">\[
\begin{align}
\text{argmin}_{B} \mathbb{E}\lVert \mathbf{x} - B\mathbf{w} \rVert_2^2
&amp;= \text{argmin}_{B} \mathbb{E} \sum_{r=R+1}^{D} \langle \mathbf{x}, \mathbf{b}_r \rangle^2 \newline
&amp;= \text{argmin}_{B} \mathbb{E} \left[\lVert \mathbf{x} \rVert_2^2 - \sum_{r=1}^{R} \langle \mathbf{x}, \mathbf{b}_r \rangle^2 \right] \newline
&amp;= \text{argmax}_{B} \mathbb{E} \sum_{r=1}^{R} \langle \mathbf{x}, \mathbf{b}_r \rangle^2 \newline
&amp;= \text{argmax}_{B} \mathbb{E} \sum_{r=1}^{R} \mathbf{b}_r^\top \mathbf{x}\mathbf{x}^\top \mathbf{b}_r \newline
&amp;= \text{argmax}_{B} \sum_{r=1}^{R} \mathbf{b}_r^\top \mathbb{E}\left[\mathbf{x}\mathbf{x}^\top\right] \mathbf{b}_r \newline
&amp;= \text{argmax}_{B} \sum_{r=1}^{R} \mathbf{b}_r^\top C \mathbf{b}_r,
\end{align}
\]</span></p>
where <span class="math inline">\(C := \text{Cov}[\mathbf{x}]\)</span>. Here we have used the centering assumption <span class="math inline">\(\mathbb{E}[\mathbf{x}] = 0\)</span>, as well as the implicit assumption that the random vector <span class="math inline">\(\mathbf{x}\)</span> has a well-defined covariance matrix. At this point, the derivations go through exactly as before, with <span class="math inline">\(C\)</span> replacing the empirical covariance <span class="math inline">\(\hat{C}\)</span>; that is, the optimal basis <span class="math inline">\(B\)</span> is obtained by extracting the first <span class="math inline">\(R\)</span> columns of <span class="math inline">\(V\)</span>, where <span class="math inline">\(C = V\Lambda V^\top\)</span>. Unsurprisingly, the results that held for the sample covariance hold analogously in this setting. For example, since <span class="math inline">\(\mathbf{x}\)</span> has zero mean then <span class="math display">\[
\mathbb{E}[\mathbf{w}_r] = \mathbb{E}[\mathbf{v}_r^\top \mathbf{x}] = 0.
\]</span> Moreover, <span class="math display">\[
\text{Cov}[\mathbf{w}_r, \mathbf{w}_s]
= \text{Cov}[\mathbf{v}_r^\top \mathbf{x}, \mathbf{v}_s^\top \mathbf{x}]
= \mathbf{v}_r^\top C \mathbf{v}_s
= \mathbf{v}_r^\top V \Lambda V^\top \mathbf{v}_s
= \lambda_r 1[r = s],
\]</span> which says that the weights <span class="math inline">\(\mathbf{w}_r\)</span> are uncorrelated, with variance equal to their respective eigenvalues. It is common, therefore, to normalize these random variables to have unit variance <span class="math display">\[
\mathbf{\tilde{w}}_r := \frac{1}{\sqrt{\lambda_r}} \mathbf{w}_r,
\]</span> which means the optimal rank <span class="math inline">\(R\)</span> approximation to the random vector <span class="math inline">\(\mathbf{x}\)</span> may be expressed as <span class="math display">\[
\mathbf{\hat{x}}(\omega) := \sum_{r=1}^{R} \sqrt{\lambda_r} \mathbf{\tilde{w}}_r(\omega) \mathbf{v}_r,
\]</span> where we have included the <span class="math inline">\(\omega\)</span> argument to emphasize which quantities are random here. The following result summarizes our findings, extending to the case with non-zero mean.
<blockquote class="blockquote">
<p>
<strong>Proposition.</strong> Let <span class="math inline">\(\mathbf{x}\)</span> be a square-integrable <span class="math inline">\(D\)</span>-dimensional random vector defined on <span class="math inline">\((\Omega, \mathcal{A}, \mathbb{P})\)</span>. Among all such random vectors constrained to take values in an <span class="math inline">\(R\)</span>-dimensional subspace of <span class="math inline">\(\mathbb{R}^D\)</span>, the random vector <span class="math display">\[
  \mathbf{\hat{x}}(\omega) = \mathbb{E}[\mathbf{x}] + \sum_{r=1}^{R} \sqrt{\lambda_r} \mathbf{\tilde{w}}_r(\omega) \mathbf{v}_r, \tag{15}
  \]</span> provides an optimal approximation to <span class="math inline">\(\mathbf{x}\)</span>, in the expected Euclidean distance sense (14). Moreover, the random weights <span class="math inline">\(\mathbf{\tilde{w}}_r\)</span> are uncorrelated, have zero mean and unit variance.
</p>
</blockquote>
<p>Note that random vectors can conceptually be thought of as stochastic processes with finite-dimensional index sets. A similar decomposition to (15) can be constructed for more general stochastic processes with potentially uncountable index sets, with the modification that the sum in (15) will require a countably infinite number of terms. This result is generally known as the <strong>Karhunen-Loeve expansion</strong>. Thus, from this perspective PCA can be thought of as the special finite-dimensional case of the Karhunen-Loeve expansion.</p>
<section id="gaussian-random-vectors" class="level4">
<h4 class="anchored" data-anchor-id="gaussian-random-vectors">Gaussian Random Vectors</h4>
<p>The above derivations did not make any assumptions regarding the distribution of <span class="math inline">\(\mathbf{x}\)</span>, just that its covariance exists. Consequently, the main result (15) does not give the distribution of the weights <span class="math inline">\(\mathbf{\tilde{w}}_r\)</span>, only that they are uncorrelated, have zero mean, and unit variance. If we add the distributional assumption <span class="math inline">\(\mathbf{x} \sim \mathcal{N}(\mathbf{m}, C)\)</span> then we are able to say more. Indeed, recalling that the (unscaled) weights are given by projections <span class="math inline">\(\mathbf{w}_r = \langle \mathbf{x} - \mathbf{m}, \mathbf{v}_r \rangle\)</span>, then we find that <span class="math display">\[
\mathbf{w} = V^\top (\mathbf{x} - \mathbf{m}) \sim \mathcal{N}(0, V^\top C V).
\]</span> The key point here are that the <span class="math inline">\(\mathbf{w}_r\)</span> are <em>jointly</em> Gaussian, and hence their uncorrelatedness implies that they are in fact independent. The weights inherit Gaussianity from <span class="math inline">\(\mathbf{x}\)</span>. Thus, under this stronger assumption we are able to characterize the distribution of the weights exactly.</p>
</section>
</section>
<section id="maximum-likelihood-estimation-with-gaussian-noise" class="level3">
<h3 class="anchored" data-anchor-id="maximum-likelihood-estimation-with-gaussian-noise">Maximum Likelihood Estimation with Gaussian Noise</h3>
</section>
<section id="probabilistic-pca" class="level3">
<h3 class="anchored" data-anchor-id="probabilistic-pca">Probabilistic PCA</h3>
<p>https://stats.stackexchange.com/questions/190308/why-does-probabilistic-pca-use-gaussian-prior-over-latent-variables</p>
</section>
</section>
</section>
<section id="part-3-using-pca" class="level1">
<h1>Part 3: Using PCA</h1>
<ol type="1">
<li>Decorrelating</li>
<li>Dimensionality reduction</li>
</ol>
</section>
<section id="part-4-computing-pca" class="level1">
<h1>Part 4: Computing PCA</h1>
<ol type="1">
<li>Eigendecomposition</li>
<li>SVD</li>
</ol>
</section>
<section id="part-5-application-and-code" class="level1">
<h1>Part 5: Application and Code</h1>
</section>
<section id="appendix" class="level1">
<h1>Appendix</h1>
<section id="eigenvalue-problems" class="level2">
<h2 class="anchored" data-anchor-id="eigenvalue-problems">Eigenvalue Problems</h2>
<p>In this section, I briefly discuss the spectral norm and eigenvalue problems in finite-dimensional vector spaces, which I utilize above when optimizing the basis <span class="math inline">\(B\)</span> in the PCA derivation. Consider a matrix <span class="math inline">\(A \in \mathbb{R}^{N \times D}\)</span>, which represents a linear transformation from <span class="math inline">\(\mathbb{R}^{D}\)</span> to <span class="math inline">\(\mathbb{R}^N\)</span>. We define the <strong>spectral norm</strong> of <span class="math inline">\(A\)</span> as the largest factor by which the map <span class="math inline">\(A\)</span> can “stretch” a vector <span class="math inline">\(\mathbf{u} \in \mathbb{R}^{D}\)</span>, <span class="math display">\[
\lVert A \rVert_2 := \max_{\lVert \mathbf{u} \rVert_2 \neq \boldsymbol{0}}
\frac{\lVert A\mathbf{u} \rVert_2}{\lVert \mathbf{u} \rVert_2}.
\]</span> Using the linearity of <span class="math inline">\(A\)</span>, one can show that we need only consider vectors of unit length; that is, <span class="math display">\[
\lVert A \rVert_2 = \max_{\lVert \mathbf{u}_2 \rVert=1} \lVert A\mathbf{u} \rVert_2. \tag{A1}
\]</span> Optimization problems of this type are called <strong>eigenvalue problems</strong> for reasons that will shortly become clear. For the purposes of the PCA derivation, we will require consideration of a slightly more general eigenvalue problem. To define this problem, first note that the matrix <span class="math inline">\(A^\top A\)</span> is symmetric, positive semi-definite since <span class="math display">\[
\begin{align}
&amp;(A^\top A)^\top = A^\top (A^\top)^\top = A^\top A,
&amp;\mathbf{u}^\top (A^\top A)\mathbf{u} = \lVert A \mathbf{u} \rVert_2^2 \geq 0.
\end{align}
\]</span> Thus, by the spectral theorem <span class="math inline">\(A^\top A\)</span> has <span class="math inline">\(D\)</span> orthogonal eigenvectors, which we will denote by <span class="math inline">\(\mathbf{e}_1, \dots, \mathbf{e}_D\)</span> and assume that they have been normalized to have unit norm. By positive definiteness the respective eigenvalues <span class="math inline">\(\lambda_1, \dots, \lambda_D\)</span> (sorted in decreasing order) are all non-negative. For <span class="math inline">\(d = 1, \dots, D\)</span>, let <span class="math inline">\(\mathcal{E}_d := \text{span}(\mathbf{e}_1, \dots, \mathbf{e}_d)\)</span>, with <span class="math inline">\(\mathcal{E}^{\perp}_d := \text{span}(\mathbf{e}_{d+1}, \dots, \mathbf{e}_D)\)</span> its orthogonal complement. We now consider the eigenvalue problem</p>
<p><span class="math display">\[
\begin{align}
\max_{\mathbf{u} \in \mathcal{E}^{\perp}_d, \lVert \mathbf{u} \rVert_2=1} \lVert A\mathbf{u} \rVert_2. \tag{A2}
\end{align}
\]</span></p>
<p>We have generalized (A1) by adding an orthogonality constraint. Taking as a convention <span class="math inline">\(\mathcal{E}_0 := \{\mathbf{0}\}\)</span> we then have <span class="math inline">\(\mathcal{E}^{\perp}_0 = \mathbb{R}^D\)</span>, which means that setting <span class="math inline">\(d = 0\)</span> in (A2) recovers the original problem (A1) as a special case. We will prove the following result.</p>
<blockquote class="blockquote">
<p>
<strong>Proposition.</strong> Let <span class="math inline">\(A \in \mathbb{R}^{N \times D}\)</span> be a matrix. Let <span class="math inline">\(\mathbf{e}_1, \dots, \mathbf{e}_D\)</span> denote the orthonormal eigenbasis of <span class="math inline">\(A^\top A\)</span>, with respective eigenvalues <span class="math inline">\(\lambda_1, \dots, \lambda_D\)</span> sorted in descending order by magnitude. For <span class="math inline">\(d = 1, \dots, D\)</span> define <span class="math inline">\(\mathcal{E}_d := \text{span}(\mathbf{e}_1, \dots, \mathbf{e}_d)\)</span>, with <span class="math inline">\(\mathcal{E}^{\perp}_d := \text{span}(\mathbf{e}_{d+1}, \dots, \mathbf{e}_D)\)</span> its orthogonal complement. Then <span class="math display">\[
  \mathbf{e}_{d+1} = \text{argmax}_{\mathbf{u} \in \mathcal{E}^{\perp}_d, \mathbf{u}=1} \lVert A\mathbf{u} \rVert_2. \tag{A3}
  \]</span> with the maximal value equal to <span class="math inline">\(\sqrt{\lambda_{d+1}}\)</span>. In particular, we have <span class="math display">\[
  \mathbf{e}_1 = \text{argmax}_{\lVert \mathbf{u} \rVert_2=1} \lVert A\mathbf{u} \rVert_2, \tag{A4}
  \]</span> with maximal value <span class="math inline">\(\sqrt{\lambda_1}\)</span>.
</p>
</blockquote>
<p><strong>Proof.</strong> Let <span class="math inline">\(\mathbf{u} \in \mathcal{E}^{\perp}_d\)</span> be an arbitrary vector of unit length. This vector may be represented with respect to the eigenbasis as <span class="math display">\[
\mathbf{u} = \sum_{r=d+1}^{D} u_r \mathbf{e}_r, \qquad u_r := \langle \mathbf{u}, \mathbf{e}_r \rangle
\]</span> We will use this representation to show that 1. <span class="math inline">\(\lVert A\mathbf{u} \rVert_2\)</span> is upper bounded by <span class="math inline">\(\sqrt{\lambda_{d+1}}\)</span>. 2. The upper bound is achieved by some <span class="math inline">\(\mathbf{u} \in \mathcal{E}^{\perp}_d\)</span>,</p>
<p>which together imply the claimed result. We will actually work with the squared norm instead, which allows us to leverage the inner product. We have <span class="math display">\[
\begin{align}
\lVert A\mathbf{u} \rVert^2_2
= \langle A\mathbf{u}, A\mathbf{u} \rangle
= \langle A^\top A\mathbf{u}, \mathbf{u} \rangle
&amp;= \left\langle A^\top A \sum_{r=d+1}^{D} u_r \mathbf{e}_r,
\sum_{r=d+1}^{D} u_r \mathbf{e}_r \right\rangle \newline
&amp;= \left\langle \sum_{r=d+1}^{D} u_r (A^\top A \mathbf{e}_r),
\sum_{r=d+1}^{D} u_r \mathbf{e}_r \right\rangle \newline
&amp;= \left\langle \sum_{r=d+1}^{D} u_r \lambda_r \mathbf{e}_r,
\sum_{r=d+1}^{D} u_r \mathbf{e}_r \right\rangle,
\end{align}
\]</span> having used the fact the <span class="math inline">\(\mathbf{e}_r\)</span> are eigenvectors of <span class="math inline">\(A^\top A\)</span>. Now we can take advantage of the fact that the <span class="math inline">\(\mathbf{e}_r\)</span> are orthonormal to obtain <span class="math display">\[
\begin{align}
\left\langle \sum_{r=d+1}^{D} u_r \lambda_r \mathbf{e}_r,
\sum_{r=d+1}^{D} u_r \mathbf{e}_r \right\rangle
= \sum_{r=d+1}^{D} u_r^2 \lambda_r \lVert \mathbf{e}_r \rVert^2_2
= \sum_{r=d+1}^{D} u_r^2 \lambda_r
\leq \sum_{r=d+1}^{D} u_r^2 \lambda_{d+1}
= \lambda_{d+1} \lVert \mathbf{u} \rVert_2^2
= \lambda_{d+1}
\end{align}
\]</span> where the inequality follows from the fact that the eigenvalues are sorted in descending order. This verifies the upper bound <span class="math inline">\(\lVert A\mathbf{u} \rVert_2 \leq \sqrt{\lambda_{d+1}}\)</span>. To show that the bound is achieved, we consider setting <span class="math inline">\(\mathbf{u} = \mathbf{e}_{d+1}\)</span>. Then,</p>
<p><span class="math display">\[
\begin{align}
\lVert A\mathbf{e}_{d+1} \rVert^2_2
= \langle A\mathbf{e}_{d+1}, A\mathbf{e}_{d+1} \rangle
= \langle A^\top A\mathbf{e}_{d+1}, \mathbf{e}_{d+1} \rangle
= \langle \lambda_{d+1} \mathbf{e}_{d+1}, \mathbf{e}_{d+1} \rangle
= \lambda_{d+1} \lVert \mathbf{e}_{d+1} \rVert^2_2
= \lambda_{d+1},
\end{align}
\]</span></p>
<p>so we have indeed verified that the equality <span class="math inline">\(\lVert A\mathbf{u} \rVert_2 = \sqrt{\lambda_{d+1}}\)</span> is achieved for some unit-norm vector <span class="math inline">\(\mathbf{u} \in \mathcal{E}^{\perp}_{d}\)</span>. The claim is thus proved. <span class="math inline">\(\qquad \blacksquare\)</span></p>
</section>
<section id="projections" class="level2">
<h2 class="anchored" data-anchor-id="projections">Projections</h2>
</section>
<section id="truncated-svd-and-eigendecomposition" class="level2">
<h2 class="anchored" data-anchor-id="truncated-svd-and-eigendecomposition">Truncated SVD and Eigendecomposition</h2>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>