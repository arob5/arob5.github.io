<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-06-25">
<meta name="description" content="A discussion of the popular output dimensionality strategy for emulating multi-output functions.">

<title>Basis Expansions for Black-Box Function Emulation – Andrew G. Roberts</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Andrew G. Roberts</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="../about.qmd"> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Basis Expansions for Black-Box Function Emulation</h1>
                  <div>
        <div class="description">
          A discussion of the popular output dimensionality strategy for emulating multi-output functions.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Gaussian-Process</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 25, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>{% katexmm %} # Setup Suppose we are working with a function <span class="math display">\[\begin{align}
\mathcal{G}: \mathcal{U} \subset \mathbb{R}^d \to \mathbb{R}^p, \tag{1}
\end{align}\]</span> where the output dimension <span class="math inline">\(p\)</span> is presumed to be “large”, potentially in the hundreds or thousands. We treat <span class="math inline">\(\mathcal{G}\)</span> generically as a black-box function, but in common applications of interest it takes the form of an expensive computer simulation model. We will therefore use the terms <em>black-box function</em>, <em>computer model</em>, and <em>simulator</em> interchangeably throughout this post. Our primary goal of interest is to construct a model that approximates the map <span class="math inline">\(u \mapsto \mathcal{G}(u)\)</span>. We will refer to such a model as an <em>emulator</em> or <em>surrogate model</em>. The implicit assumption here is that computing <span class="math inline">\(\mathcal{G}(u)\)</span> is quite expensive, so the idea is to replace <span class="math inline">\(\mathcal{G}\)</span> with a computationally cheaper approximation. If you don’t care about emulating expensive computer models, you can also view this generically as a regression (or interpolation) problem with a very high-dimensional output space. To further elucidate the connection, suppose that we evaluate the function at a set of points <span class="math inline">\(u_1, \dots, u_n \in \mathcal{U}\)</span>, resulting in the input-output pairs <span class="math inline">\(\{u_i, \mathcal{G}(u_i)\}_{i=1}^{n}\)</span>. We can now treat these pairs as training examples that can be use to fit a predictive model. From this point of view, the primary distinction from more traditional regression is that in this case we get to choose the input points <span class="math inline">\(u_i\)</span> at which we evaluate the function.</p>
<p>The methods we discuss below attempt to address two primary challenges: 1. Function evaluations <span class="math inline">\(\mathcal{G}(u)\)</span> are expensive, and thus we seek to minimize the number of points at which <span class="math inline">\(\mathcal{G}\)</span> is evaluated. 2. The output space of <span class="math inline">\(\mathcal{G}\)</span> is very high-dimensional.</p>
<p>The second challenge can be problematic for standard regression methods, which are often tailored to scalar-valued outputs. The obvious solution here might be to fit a separate model to predict each individual output; i.e., a model to emulate the map <span class="math inline">\(u \mapsto \mathcal{G}_j(u)\)</span> for each <span class="math inline">\(j=1, \dots, p\)</span>. With parallel computing resources, such an approach might even be feasible for very large values of <span class="math inline">\(p\)</span>. However, larger numbers of outputs typically come with more structure; e.g., the outputs may consist of a time series or spatial fields. The independent output-by-output regression approach completely fails to leverage this structure. The method discussed in this post seeks to take advantage of such structure by finding a set of basis vectors that explain the majority of the variation in the outputs. We proceed by first generically discussing basis representations of the output space, and how such structure can be leveraged to address the two challenges noted above. With the general method defined, we conclude by discussing details for a specific choice of basis approximation (principal components analysis) and a specific choice of emulator model (Gaussian process regression). {% endkatexmm %}</p>
<p>{% katexmm %} # Basis Representation From a very generic perspective, the methods we discuss here can be thought of as a method for dimension reduction of the <em>output</em> space of a regression problem. It is perhaps more typical to see dimensionality reduction applied to the <em>input</em> space in such settings, with principal component regression being the obvious example. In this section, we start by discussing a low-dimensional basis representation of the output space, and then explore how such a representation can be leveraged in solving the regression problem. Throughout this introduction, I will assume that the output of <span class="math inline">\(\mathcal{G}\)</span> is centered; this is made explicit later on when considering a concrete application of PCA.</p>
<section id="a-basis-representation-of-the-output-space" class="level2">
<h2 class="anchored" data-anchor-id="a-basis-representation-of-the-output-space">A Basis Representation of the Output Space</h2>
<p>Let’s start by considering approximately representing vectors in the range of <span class="math inline">\(\mathcal{G}\)</span>, denoted <span class="math inline">\(\mathcal{R}(\mathcal{G})\)</span>, with respect to a set of <span class="math inline">\(r \ll p\)</span> orthonormal basis vectors <span class="math inline">\(\{b_1, \dots, b_r\} \subset \mathbb{R}^p\)</span>. Given such a set of vectors, we can approximate <span class="math inline">\(g \in \mathcal{R}(\mathcal{G})\)</span> by its projection onto the subspace <span class="math inline">\(\text{span}(b_1, \dots, b_r)\)</span>: <span class="math display">\[
\hat{g} := \sum_{j=1}^{r} \langle g, b_r\rangle b_r. \tag{2}
\]</span> If we stack the basis vectors as columns in a matrix <span class="math inline">\(B \in \mathbb{R}^{p \times r}\)</span> then we can write this projection compactly as <span class="math display">\[
\hat{g}
= \sum_{j=1}^{r} \langle g, b_r\rangle b_r
= \sum_{j=1}^{r} (b_r b_r^\top)g
= BB^\top g, \tag{3}
\]</span> We see that <span class="math inline">\(BB^\top\)</span> is the projection matrix that projects onto the span of the basis vectors. With regards to dimensionality reduction, the benefit here is that the simulator output can now be (approximately) represented using <span class="math inline">\(r \ll p\)</span> numbers <span class="math inline">\(B^\top g\)</span>. The question is now: how do we find the basis vectors <span class="math inline">\(B\)</span>? If we are given a set of vectors <span class="math inline">\(g_1, \dots, g_n \in \mathcal{R}(\mathcal{G})\)</span>, we can take an empirical approach and try to use these examples to determine a <span class="math inline">\(B\)</span> that is optimal in some well-defined sense. Assuming that <span class="math inline">\(\mathcal{R}(\mathcal{G}) \subseteq \mathbb{R}^p\)</span> is indeed a subspace and we define “optimal” in an average squared error sense, the problem we have laid out here is exactly that of principal components analysis (PCA), a topic I discuss in depth in <a href="https://arob5.github.io/blog/2023/12/15/PCA/">this</a> post. The only difference is that we are applying PCA on the subspace <span class="math inline">\(\mathcal{R}(\mathcal{G})\)</span>. At this point, we should emphasize that in practice <span class="math inline">\(\mathcal{R}(\mathcal{G})\)</span> will often not be a subspace. Computer simulations may produce outputs that are subject to certain constraints, and thus <span class="math inline">\(\mathcal{R}(\mathcal{G})\)</span> may represent a more complicated subset of <span class="math inline">\(\mathbb{R}^p\)</span>. In these cases, one can still typically apply the PCA algorithm to obtain <span class="math inline">\(B\)</span>, but the result may be sub-optimal. Alternate methods of basis construction may be warranted depending on the problem at hand.</p>
</section>
<section id="linking-the-basis-representation-with-the-input-parameters" class="level2">
<h2 class="anchored" data-anchor-id="linking-the-basis-representation-with-the-input-parameters">Linking the Basis Representation with the Input Parameters</h2>
<p>In the previous subsection, we considered approximating vectors in the range of the simulator with respect to a set of basis vectors <span class="math inline">\(B\)</span>. However, recall that our underlying goal here is to approximate the map <span class="math inline">\(u \mapsto \mathcal{G}(u)\)</span>. We thus need to consider how to leverage the basis representation of the output space in achieving this goal. Assuming we have already constructed the basis vectors <span class="math inline">\(B\)</span>, the map <span class="math inline">\(u \mapsto \mathcal{G}(u)\)</span> can be approximated as <span class="math display">\[
u \mapsto \sum_{j=1}^{r} \langle \mathcal{G}(u), b_r\rangle b_r = B [B^\top \mathcal{G}(u)]. \tag{4}
\]</span> In words: feed <span class="math inline">\(u\)</span> through the simulator and project the resulting output onto the low-dimensional subspace spanned by the basis vectors. Note that <span class="math inline">\(B^\top \mathcal{G}(u)\)</span> stores the <span class="math inline">\(r\)</span> weights defining the projection of <span class="math inline">\(\mathcal{G}(u)\)</span> onto the subspace generated by <span class="math inline">\(B\)</span>, thus providing a low dimensional summary of the simulator output. Let’s introduce the notation <span class="math display">\[\begin{align}
w(u) &amp;:= B^\top \mathcal{G}(u) = \left[w_1(u), \dots, w_r(u) \right]^\top \in \mathbb{R}^r,
&amp;&amp; w_r(u) := \langle \mathcal{G}(u), b_r \rangle
\end{align}\]</span> to denote this weights. The basis function approximation to the simulator can thus be written as <span class="math display">\[
\hat{\mathcal{G}}_r(u) := \sum_{j=1}^{r} w_r(u)b_r = Bw(u) \in \mathbb{R}^p. \tag{5}
\]</span> At this point, this isn’t helpful since the expensive simulation still needs to be run every time <span class="math inline">\(\hat{\mathcal{G}}_r(u)\)</span> is evaluated. To address this, we now turn back to the idea of using emulators. Recall that such an approach was originally hindered due to the high-dimensional output space of the simulator. However, under the approximation <span class="math inline">\(\hat{\mathcal{G}}_r(u)\)</span>, the dependence on <span class="math inline">\(u\)</span> has been reduced to <span class="math inline">\(w(u)\)</span>, which effectively reduces the output dimension to <span class="math inline">\(r\)</span>. The idea is thus to use some sort of statistical model to emulate the map <span class="math inline">\(u \mapsto w(u)\)</span>. Suppressing all details for now, let’s suppose we have fit such a model <span class="math inline">\(w^*(u)\)</span>. We can now plug <span class="math inline">\(w^*(u)\)</span> in place of <span class="math inline">\(w(u)\)</span> in (5) to obtain the approximation <span class="math display">\[
\hat{\mathcal{G}}^*_r(u) := \sum_{j=1}^{r} w^*_r(u)b_r = Bw^*(u) \in \mathbb{R}^p. \tag{6}
\]</span> This approximation no longer requires running the full simulator, since evaluating <span class="math inline">\(\hat{\mathcal{G}}^*_r(u)\)</span> just requires (1) computing the emulator prediction at <span class="math inline">\(u\)</span>; and (2) applying <span class="math inline">\(B\)</span> to the emulator prediction. It is worth emphasizing how this approach compares to the direct emulation method. In place of directly trying to approximate the map from <span class="math inline">\(u\)</span> to the model outputs <span class="math inline">\(\mathcal{G}(u)\)</span>, we are now considering approximating the map from <span class="math inline">\(u\)</span> to inner products of <span class="math inline">\(\mathcal{G}(u)\)</span> with a small number of basis vectors. The hope is that these inner products are sufficient to capture the majority of information in the model response.</p>
</section>
<section id="the-general-emulation-model" class="level2">
<h2 class="anchored" data-anchor-id="the-general-emulation-model">The General Emulation Model</h2>
In the preceding subsections, we introduced the idea of representing the output space (range) of <span class="math inline">\(\mathcal{G}\)</span> with respect to a low-dimensional basis in order to facilitate emulation of the map <span class="math inline">\(u \mapsto \mathcal{G}(u)\)</span>. For concreteness, we considered an orthogonal basis, whereby approximations of <span class="math inline">\(\mathcal{G}(u)\)</span> take the form of orthogonal projections onto the basis vectors. In this section, we take a step back and define the general model, which encompasses basis methods beyond the orthogonal projection framework.
<blockquote class="blockquote">
<p>
<strong>The Basis Function Emulation Model.</strong> Given a set of vectors <span class="math inline">\(\{b_1, \dots, b_r\} \subset \mathbb{R}^{p}\)</span>, we refer to a decomposition of the form <span class="math display">\[
  \mathcal{G}(u) = \sum_{j=1}^{r} w_j(u) b_j + \epsilon(u) \tag{7}
  \]</span> as the basis function GP emulation model. We write <span class="math inline">\(w^*_j(u)\)</span> to denote an emulator model that approximates the map <span class="math inline">\(w_j(u)\)</span>.
</p>
</blockquote>
<p>The basis function emulation model decomposes the computer model output <span class="math inline">\(\mathcal{G}(u)\)</span> into 1. a piece that can be explained by a linear combination of <span class="math inline">\(r\)</span> basis functions that are independent of <span class="math inline">\(u\)</span>. 2. the residual <span class="math inline">\(\epsilon(u)\)</span>, representing all variation unaccounted for by the basis functions.</p>
<p>We emphasize that the basis functions are independent of the input <span class="math inline">\(u\)</span>; the effect of the inputs is restricted to the coefficients <span class="math inline">\(w_j(u)\)</span>, with unaccounted for <span class="math inline">\(u\)</span>-dependence absorbed by the residual term <span class="math inline">\(\epsilon(u)\)</span>. As noted in the previous section, if we opt for an orthogonal projection approach, then the true weights assume the form <span class="math display">\[
w_j(u) = \langle \mathcal{G}(u), b_j \rangle, \tag{8}
\]</span> but the general model (7) allows for other approaches as well. Under different decomposition strategies, the true weights may not be given by the inner products (8). Nonetheless, we can still consider applying statistical models to approximate the underlying weight maps <span class="math inline">\(u \mapsto w_j(u)\)</span>, regardless of what form these maps may take. {% endkatexmm %}</p>
</section>
<section id="concrete-details" class="level1">
<h1>Concrete Details</h1>
<p>Having laid out the general model, we now provide concrete details for specific choices of the basis construction and the emulator model. For the former we consider the popular PCA/SVD approach, which we have already hinted at above. For the latter, we consider the use of Gaussian processes (GPs). The combination of these two choices was first presented in the seminal paper {% cite HigdonBasisEmulator%}. I have a whole in-depth <a href="https://arob5.github.io/blog/2023/12/15/PCA/">post</a> on PCA, so I will assume general background knowledge on this topic.</p>
<p>{% katexmm %} ## Constructing the PCA Basis Let us consider the construction of the basis vectors <span class="math inline">\(b_j\)</span> using a principal components analysis (PCA) approach. Depending on the field, this strategy might also be termed singular value decomposition (SVD), proper orthogonal decomposition (POD), or empirical orthogonal functions (EOF).</p>
<section id="initial-design" class="level3">
<h3 class="anchored" data-anchor-id="initial-design">Initial Design</h3>
<p>In the absence of prior knowledge about the <span class="math inline">\(b_j\)</span>, PCA takes a purely empirical approach; the simulator <span class="math inline">\(\mathcal{G}(u)\)</span> is evaluated at a variety of different inputs <span class="math inline">\(u_i\)</span> in order to understand the patterns of variability induced in the model outputs. We refer to the resulting input-output pairs <span class="math display">\[
\{u_i, \mathcal{G}(u_i)\}, \qquad i = 1, \dots, n \tag{9}
\]</span> as the <em>design</em> or <em>design points</em>. In practice, the availability of parallel computing resources typically means that model simulations can be run in parallel, thus reducing the computational cost of generating the design. The input design points <span class="math inline">\(u_i\)</span> are often chosen to vary “uniformly” over the input space <span class="math inline">\(\mathcal{U}\)</span> in some sense. More generally, we might assume that the inputs are governed by some prior distribution <span class="math display">\[
u \sim \rho, \tag{10}
\]</span> which might encode prior knowledge about model parameters or serve to place higher weight on regions of the input space that are deemed more important. In this case, the design inputs might be sampled independently according to <span class="math inline">\(\rho\)</span>. In any case, the spread of the inputs ought to be chosen so that the set of corresponding outputs <span class="math inline">\(\mathcal{G}(u_i)\)</span> is representative of variation in the simulator output under typical use cases.</p>
</section>
<section id="pca" class="level3">
<h3 class="anchored" data-anchor-id="pca">PCA</h3>
We denote the design outputs by <span class="math inline">\(g_i := \mathcal{G}(u_i)\)</span>, <span class="math inline">\(i = 1, \dots, n\)</span> and consider stacking these vectors into the rows of a matrix <span class="math inline">\(G \in \mathbb{R}^{n \times p}\)</span>. Define the empirical mean <span class="math display">\[
\overline{g} := \frac{1}{n} \sum_{i=1}^{n} g_i. \tag{10}
\]</span> Since PCA is typically defined with respect to a centered data matrix, let <span class="math inline">\(G^c \in \mathbb{R}^{n \times p}\)</span> denote the analog of <span class="math inline">\(G\)</span> constructed using the centered outputs <span class="math inline">\(g_i^c := g_i - \overline{g}\)</span>. In other words, <span class="math inline">\(G^c\)</span> results from subtracting <span class="math inline">\(\overline{g}\)</span> from each row of <span class="math inline">\(G\)</span>. The rows of <span class="math inline">\(G^c\)</span> provide a summary of the variation in the simulator outputs due to variation in the inputs. Each column of <span class="math inline">\(G^c\)</span> represents a single dimension in the output space. We seek to identify a small number of vectors <span class="math inline">\(b_j \in \mathbb{R}^p\)</span> such that the observed outputs <span class="math inline">\(g_i\)</span> can be explained as a linear combination of these vectors. PCA produces these vectors by computing an eigendecomposition of the (unnormalized) empirical covariance matrix <span class="math display">\[
\hat{C}_g
:= \sum_{i=1}^{n} (g_i - \overline{g})(g_i - \overline{g})^\top
= (G^c)^\top G^c, \tag{11}
\]</span> and defining <span class="math inline">\(B\)</span> to consist of the <span class="math inline">\(r\)</span> dominant eigenvectors of <span class="math inline">\(\hat{C}_g\)</span>. We denote the eigendecomposition by <span class="math display">\[
\hat{C}_g = V \Lambda V^\top, \tag{12}
\]</span> where <span class="math inline">\(\Lambda := \text{diag}\{\lambda_1, \dots, \lambda_p\}\)</span> contains the eigenvalues sorted in decreasing order of their magnitude, with <span class="math inline">\(V\)</span> storing the respective normalized eigenvectors <span class="math inline">\(v_1, \dots, v_p\)</span> as columns. If we truncate to the dominant <span class="math inline">\(r\)</span> eigenvectors, <span class="math display">\[
\hat{C}_g \approx V_r \Lambda_r V_r^\top, \tag{13}
\]</span> then we define our basis vectors by <span class="math display">\[\begin{align}
&amp;B := V_r, &amp;&amp;b_j := v_j, \qquad j = 1, \dots, r. \tag{14}
\end{align}\]</span> Note that <span class="math inline">\(\hat{C}_g\)</span> is positive semidefinite so <span class="math inline">\(\lambda_j \geq 0\)</span> for all <span class="math inline">\(j\)</span>. We summarize these ideas below.
<blockquote class="blockquote">
<p>
<strong>PCA Approximation.</strong> Given design <span class="math inline">\(\{u_i, \mathcal{G}(u_i)\}_{i=1}^{n}\)</span>, the PCA-induced approximation to <span class="math inline">\(\mathcal{G}(u)\)</span> is given by <span class="math display">\[
  \hat{\mathcal{G}}_r(u)
  := \overline{g} + \sum_{j=1}^{r} \langle \mathcal{G}(u)-\overline{g}, v_j\rangle v_j + \epsilon(u), \tag{15}
  \]</span> where <span class="math inline">\(\overline{g}\)</span> is defined in (10) and <span class="math inline">\(v_1, \dots, v_p\)</span> denote the normalized eigenvectors of the matrix defined in (11). The residual term is given by <span class="math display">\[
  \epsilon(u) = \sum_{j=r+1}^{p} \langle \mathcal{G}(u)-\overline{g}, v_j\rangle v_j. \tag{16}
  \]</span>
</p>
</blockquote>
</section>
<section id="gaussian-process-emulators" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-process-emulators">Gaussian Process Emulators</h2>
<p>The above section shows that the PCA approach yields the basis vectors <span class="math inline">\(b_j = v_j\)</span> with associated weights <span class="math inline">\(w_j(u) = \langle \mathcal{G}(u)-\overline{g}, v_j\rangle\)</span>. In this section we consider approximating the maps <span class="math inline">\(u \mapsto w_j(u)\)</span> with Gaussian processes (GPs). The specific features of the PCA basis construction provide useful information in designing a reasonable GP model. In particular, we consider why it is typically reasonable emulate the maps <span class="math inline">\(u \mapsto w_j(u)\)</span> separately using a set of independent GPs. We then discuss how the independent GP approach induces a multi-output GP emulator for <span class="math inline">\(\mathcal{G}(u)\)</span> with a particular covariance structure.</p>
<section id="independent-gps" class="level3">
<h3 class="anchored" data-anchor-id="independent-gps">Independent GPs</h3>
<p>We first consider the typical choice to model each <span class="math inline">\(w_j(u)\)</span> separately as an independent GP. For this, we point to a result proved in the post <a href="https://arob5.github.io/blog/2023/12/15/PCA/">post</a>; namely, if we define <span class="math inline">\(w^i := [w_1(u_i), \dots, w_r(u_i)]^\top \in \mathbb{R}^r\)</span>, then the vectors <span class="math inline">\(w^1, \dots, w^n\)</span> have empirical covariance <span class="math display">\[
\hat{C}_w := \frac{1}{n-1} \sum_{i=1}^{n} w^i(w^i)^\top = \frac{1}{n-1}\Lambda_r, \tag{17}
\]</span> with <span class="math inline">\(\Lambda_r\)</span> given in (13). In words, the weight vectors have zero sample covariance across dimensions. This seems to provide some justification for the independent GP approach, thought we should be careful not to overstate the claim. We can view <span class="math inline">\(\hat{C}_w\)</span> as an estimate of <span class="math display">\[
C_w := \mathbb{E}_{u \sim \rho} (w(u) - \mathbb{E}[w(u)])(w(u) - \mathbb{E}[w(u)])^\top, \tag{18}
\]</span> where <span class="math inline">\(w(u) = (w_1(u), \dots, w_r(u))\)</span>. Even if we knew that <span class="math inline">\(C_w\)</span> was actually diagonal, we should take care to note that this means the <span class="math inline">\(w_j(u)\)</span> are uncorrelated <em>on average</em> with respect to <span class="math inline">\(u \sim \rho\)</span>. The assumption to fit independent GPs entails assuming a probabilistic model for <span class="math inline">\(w(u)\)</span> such that the <span class="math inline">\(w_j(u)\)</span> are pairwise independent <em>conditional</em> on each <span class="math inline">\(u\)</span>. Thus, diagonal structure in <span class="math inline">\(C_w\)</span> does not necessarily justify this assumption. In practice, the independent GP approach is often reasonable, and makes things much easier.</p>
</section>
<section id="prior-mean-specification" class="level3">
<h3 class="anchored" data-anchor-id="prior-mean-specification">Prior Mean Specification</h3>
<p>We next consider the specification of the GP prior for each independent GP. Start by noting that the sample mean of the vectors <span class="math inline">\(\{w_j(u_i)\}_{i=1}^{n}\)</span> is zero for each <span class="math inline">\(j = 1, \dots, p\)</span>. Indeed, <span class="math display">\[
\frac{1}{n} \sum_{i=1}^{n} w_j(u_i)
= \frac{1}{n} \sum_{i=1}^{n} \langle g^c_i,v_j \rangle
= \left\langle \frac{1}{n} \sum_{i=1}^{n} g^c_i,v_j \right\rangle
= \langle 0,v_j \rangle = 0. \tag{19}
\]</span> Similar to above, we can view this as an estimate of <span class="math inline">\(\mathbb{E}_{u \sim \rho}[w_j(u)]\)</span>. It is tempting to conclude that a constant zero prior mean assumption is thus justified for each GP. Again, this isn’t quite right. If <span class="math inline">\(\mathbb{E}_{u \sim \rho}[w_j(u)] \approx 0\)</span>, then this tells us that on average, over the whole input space, the weight <span class="math inline">\(w_j(u)\)</span> is approximately zero. This does <em>not</em> guarantee the absence of a trend in the map <span class="math inline">\(u \mapsto w_j(u)\)</span>. For example, this map might look linear, but is centered around zero so that the positive and negative values average out. The behavior of the weight maps will depend on the simulator and the basis vectors. If the simulator output is somewhat stationary, then the <span class="math inline">\(w_j(u)\)</span> will typically also look stationary, and so the zero constant mean assumption is probably reasonable. In other cases, one might want to consider using some sort of trend for the prior mean (e.g., polynomial basis functions). Standard GP model checking procedures should be used to determine what is best for a particular application. In the application presented in {% cite HigdonBasisEmulator %}, the zero mean assumption is regarded as reasonable.</p>
</section>
<section id="prior-covariance-specification" class="level3">
<h3 class="anchored" data-anchor-id="prior-covariance-specification">Prior Covariance Specification</h3>
<p>The covariance function used for each GP is also a modeling choice that must be specified on a case-by-case basis. If any trends in the functions <span class="math inline">\(w_j(u)\)</span> have been adequately addressed in the prior mean, then it is often reasonable to utilize a stationary covariance function; e.g., one of the form <span class="math display">\[
\text{Cov}[w_j(u), w_j(u^\prime)] = \alpha^2 c(u-u^\prime). \tag{20}
\]</span> Typical choices are covariances of the Gaussian or Matérn classes. In {% cite HigdonBasisEmulator %}, the authors consider “fully Bayesian” GPs, whereby priors are placed on the marginal variance <span class="math inline">\(\alpha^2\)</span> and other hyperparameters characterizing the correlation function <span class="math inline">\(c(\cdot)\)</span>. A common alternative is to opt for an empirical Bayes approach and fix these parameters at their optimized values.</p>
</section>
<section id="error-term" class="level3">
<h3 class="anchored" data-anchor-id="error-term">Error Term</h3>
<p>Next, we consider a statistical model for the error term <span class="math inline">\(\epsilon(u)\)</span> defined in (16). In {% cite HigdonBasisEmulator %}, it is assumed that <span class="math display">\[
\epsilon(u) \sim \mathcal{N}(0, \sigma^2 I), \tag{21}
\]</span> with <span class="math inline">\(\sigma^2\)</span> treated as an unknown parameter that is assigned an inverse Gamma prior.</p>
</section>
<section id="implied-multi-output-gp-for-the-simulator" class="level3">
<h3 class="anchored" data-anchor-id="implied-multi-output-gp-for-the-simulator">Implied Multi-Output GP for the Simulator</h3>
<p>{% endkatexmm %}</p>
</section>
</section>
</section>
<section id="other-references" class="level1">
<h1>Other References</h1>
<p>The main reference for this post is the paper {% cite HigdonBasisEmulator %}. Below are some later papers that explore similar ideas.</p>
<ul>
<li>JMS&amp;Williamson D. B. (2020). ”Efficient calibration for high-dimensional computer model output using basis methods”. arXiv preprint arXiv:1906.05758</li>
<li>JMS, Dodwell T.J., et al.&nbsp;(2021) ”A History Matching Approach to Building Full-Field Emulators in Composite Analysis”.</li>
<li>JMS, Williamson D. B., Scinocca J., and Kharin V. (2019). ”Uncertainty quantification for computer models with spatial output using calibration-optimal bases”. Journal of the American Statistical Association, 114.528, 1800-1814.</li>
<li>Chang, Won, et al.&nbsp;”Probabilistic calibration of a Greenland Ice Sheet model using spatially resolved synthetic observations: toward projections of ice mass loss with uncertainties.” Geoscientific Model Development 7.5 (2014): 1933-1943.</li>
</ul>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>