{"title":"Deriving the Metropolis-Hastings Update from the Transition Kernel","markdown":{"yaml":{"title":"Deriving the Metropolis-Hastings Update from the Transition Kernel","date":"2023-12-31","categories":["MCMC","Probability","Computational Statistics"]},"headingText":"Setup and notation","containsRefs":false,"markdown":"\n\nThe Metropolis-Hastings (MH) Markov Chain Monte Carlo (MCMC) method is typically\nintroduced in the form of a practical algorithm. In a more theoretically-oriented\ncontext, one might prove that the algorithm defines a Markov chain and derive\nthe associated transition (i.e. probability) kernel. I have found it insightful\nto also work through the derivations in the reverse order; given only the\ntransition kernel, how could one derive the well-known MH update? In other words,\nhow can you simulate the Markov chain implied by the transition kernel? In this\npost, I work through the required derivations.\n\nWe consider drawing samples from a target probability distribution $\\mu$ supported\non a state space $\\mathcal{X} \\subseteq \\mathbb{R}^D$ with Borel sigma algebra\n$\\mathcal{B}$. Let $\\pi: \\mathcal{X} \\to [0,\\infty]$ denote the Lebesgue density\nof $\\mu$, i.e.\n$$\n\\mu(A) = \\int_A \\mu(d\\mathbf{x}) = \\int_A \\pi(\\mathbf{x}) d\\mathbf{x}, \\qquad \\forall A \\in \\mathcal{B}.\n$$\nLet $Q: \\mathcal{X} \\times \\mathcal{B} \\to [0,1]$ denote the proposal kernel for the MH algorithm,\nwith $q(\\mathbf{x}, \\cdot)$ the Lebesgue density of the measure $Q(\\mathbf{x}, \\cdot)$.\nFor current state $\\mathbf{x} \\in \\mathcal{X}$ and proposal\n$\\mathbf{y} \\sim Q(\\mathbf{x}, \\cdot)$ we recall the MH acceptance probability\n$$\n\\alpha(\\mathbf{x}, \\mathbf{y})\n=\\min\\left(1, \\frac{\\pi(\\mathbf{y})q(\\mathbf{y},\\mathbf{x})}{\\pi(\\mathbf{x})q(\\mathbf{x},\\mathbf{y})} \\right).\n$$\nThroughout this post I will let $A \\in \\mathcal{B}$ denote an arbitrary Borel set.\nThe transition kernel $P:\\mathcal{X} \\times \\mathcal{B} \\to [0,1]$ implied by the MH algorithm is\nthen given by\n\\begin{align}\nP(\\mathbf{x},A)\n= \\int_A q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y} + \\delta_{\\mathbf{x}}(A) \\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})[1-\\alpha(\\mathbf{x},\\mathbf{y})] d\\mathbf{y} \\tag{1}\n\\end{align}\nwhere $\\delta_{\\mathbf{x}}(A) := \\mathbf{1}(\\mathbf{x} \\in A)$ denotes the Dirac\nmeasure. The first term in the kernel is the probability of accepting a proposal in\nthe set $A$, while the second accounts for the probability of rejection in the\ncase that the current state $\\mathbf{x}$ is already in $A$. I will denote the\noverall probability of acceptance by\n$$\n\\overline{a}(\\mathbf{x})\n:= \\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y}. \\tag{2}\n$$\n\n## Mixture of Kernels\nSuppose we don't already know the MH algorithm, and are given the task of writing\nan algorithm to draw a sample from the distribution $P(\\mathbf{x},\\cdot)$ defined in (1).\nA reasonable place to start is to try\nwriting $P(\\mathbf{x},\\cdot)$ as a mixture of kernels from which we already\nknow how to sample. Let's first try this idea, attempting to write\n$$\nP(\\mathbf{x},A) = wP_1(\\mathbf{x},A) + (1-w)P_2(\\mathbf{x},A), \\qquad w \\in [0,1], \\tag{3}\n$$\nwith $P_1$, $P_2$ transition kernels we already know how to sample from. If\nwe are able to do this, then we could easily sample from $P(\\mathbf{x},\\cdot)$\nvia the following simple algorithm:\n1. Select $P_1$ with probability $w$, else select $P_2$.\n2. Sample from the selected kernel.\n\nTo this end, let's manipulate (1) to write it in the form of a kernel mixture. We have\n\\begin{align}\nP(\\mathbf{x},A)\n&= \\int_A q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y} + \\delta_{\\mathbf{x}}(A) \\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})[1-\\alpha(\\mathbf{x},\\mathbf{y})] d\\mathbf{y} \\newline\n&= \\overline{a}(\\mathbf{x}) \\int_A \\frac{q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})}{\\overline{a}(\\mathbf{x})}d\\mathbf{y} + \\left[1-\\overline{a}(\\mathbf{x})\\right] \\delta_{\\mathbf{x}}(A) \\tag{4}\n\\end{align}\nwhich is a kernel mixture of the form (3) with\n\\begin{align}\nw = \\overline{a}(\\mathbf{x}), \\qquad P_1(\\mathbf{x},A)=\\int_A \\frac{q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})}{\\overline{a}(\\mathbf{x})}d\\mathbf{y},\n\\qquad P_2(\\mathbf{x},A)=\\delta_{\\mathbf{x}}(A).\n\\end{align}\nAll we did here was to multiply and divide by the acceptance probability $\\overline{a}(\\mathbf{x})$\nin the first term (under typical assumptions on $Q$ this will be non-zero when $\\mathbf{y}$\nis in the support of $\\pi$) and to rearrange the second term using the fact that\n$q(\\mathbf{x},\\cdot)$ is a probability density; hence,\n$$\n\\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y} = 1.\n$$\nNote that the mixture weight $\\overline{a}(\\mathbf{x})$ is the overall acceptance probability,\nand is thus in $[0,1]$ as required. Moreover, $P_2$ is the Dirac measure centered at $\\mathbf{x}$\nand is thus a valid kernel. To check that $P_1(\\mathbf{x},\\cdot)$ is a valid probability\nmeasure, we recall the form of $\\overline{a}(\\mathbf{x})$ from (2) to verify that\n\\begin{align}\nP_1(\\mathbf{x},A)\n&= \\int_{\\mathcal{X}} \\frac{q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})}{\\overline{a}(\\mathbf{x})}d\\mathbf{y} \\newline\n&= \\frac{\\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y}}{\\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y}} \\newline\n&= 1.\n\\end{align}\nThe other required properties (countable additivity and non-negativity) are similarly\nverified. Thus, $P_1(\\mathbf{x},\\cdot)$ is a probability measure with Lebesgue density\nproportional to $q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})$; the overall\nacceptance probability $\\overline{a}(\\mathbf{x})$ is the normalizing constant for this\ndensity.\n\nThis representation of the MH kernel as a mixture distribution is conceptually useful,\nbut it does not directly help us determine a sampling algorithm. Indeed, we cannot\nimplement the simple mixture sampling algorithm described above since (i.) computing\nthe mixture weight $\\overline{a}(\\mathbf{x})$ requires evaluating an intractable\nintegral, and (ii.) we don't know how to directly sample from the probability distribution\nwith density proportional to $q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})$.\nWhile this approach seems to be a dead end from a practical point of view,\nwe should keep in mind that the MH algorithm derived below does sample from the\nmixture (4), but does so in a way that avoids having to compute the mixture weight\nor to directly sample from $P_1(\\mathbf{x},\\cdot)$.\n\n## Marginalized Mixture of Kernels\nIn the previous section, we feigned ignorance of the MH algorithm in order to\napproach the problem of simulating (1) as a generic sampling problem.\nWe found that $P$ can indeed be written as a mixture of kernels, but the problem\nof sampling from the resulting mixture was also intractable. To take a step in the\nright direction, it is useful to cheat a little bit and recall some of the mechanics\nof the MH algorithm. The proposal $\\mathbf{y} \\sim Q(\\mathbf{x},\\cdot)$\nis accepted with probability $\\alpha(\\mathbf{x},\\mathbf{y})$; if rejected, the\nnext state is set to the current state $\\mathbf{x}$. Thus, it seems that we should\nbe looking for a mixture of kernels of the form\n$$\n\\alpha(\\mathbf{x},\\mathbf{y})\\delta_{\\mathbf{y}}(\\cdot) + [1-\\alpha(\\mathbf{x},\\mathbf{y})]\\delta_{\\mathbf{x}}(\\cdot). \\tag{5}\n$$\nOf course, this can't represent the whole picture since the mixture weight in (5)\ndepends on $\\mathbf{y}$ and the proposal kernel $Q$ is completely missing from the\nexpression. The key insight is that the MH kernel $P(\\mathbf{x},\\cdot)$ can be\nviewed as the expectation of (5) averaged with respect to $Q(\\mathbf{x},\\cdot)$;\ni.e., $P(\\mathbf{x},\\cdot)$ is derived by *marginalizing* the mixture (5)\nwith respect to $\\mathbf{y} \\sim Q(\\mathbf{x},\\cdot)$. To show this,\nwe return to the original expression (1) for the MH kernel. We have\n\\begin{align}\nP(\\mathbf{x},A)\n&= \\int_A q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y} + \\delta_{\\mathbf{x}}(A) \\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})[1-\\alpha(\\mathbf{x},\\mathbf{y})] d\\mathbf{y} \\newline\n&= \\int_{\\mathcal{X}} \\alpha(\\mathbf{x},\\mathbf{y}) \\mathbf{1}(\\mathbf{y} \\in A) q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y} +  \\int_{\\mathcal{X}} [1-\\alpha(\\mathbf{x},\\mathbf{y})] \\delta_{\\mathbf{x}}(A) q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y} \\newline\n&= \\int_{\\mathcal{X}} \\alpha(\\mathbf{x},\\mathbf{y}) \\delta_{\\mathbf{y}}(A) q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y} +  \\int_{\\mathcal{X}} [1-\\alpha(\\mathbf{x},\\mathbf{y})] \\delta_{\\mathbf{x}}(A) q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y} \\newline\n&= \\int_{\\mathcal{X}} \\left[\\alpha(\\mathbf{x},\\mathbf{y})\\delta_{\\mathbf{y}}(A) + [1-\\alpha(\\mathbf{x},\\mathbf{y})] \\delta_{\\mathbf{x}}(A) \\right] q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y}, \\tag{6}\n\\end{align}\nwhich is precisely the mixture (5) averaged (in $\\mathbf{y}$) with respect to the weights\n$q(\\mathbf{x},\\mathbf{y})$. We now have three different representations of the MH\ntransition kernel $P$: (1) is the most natural to derive when starting from the MH\nalgorithm, (4) represents $P$ as a mixture of two distributions, and (6) represents\n$P$ as a marginalized mixture of two distributions. It is this final representation which\nproves useful for developing a practical simulation algorithm.\n\nAll that remains is to recall how to sample from a marginalized distribution. First\nnote that $\\mathbf{x}$ is essentially a fixed parameter in the integral (6); the\naveraging is done with respect to $\\mathbf{y}$. Now, if we condition on a fixed\n $\\mathbf{y}$ as well, then the expression\n$$\n\\alpha(\\mathbf{x},\\mathbf{y})\\delta_{\\mathbf{y}}(A) + [1-\\alpha(\\mathbf{x},\\mathbf{y})] \\delta_{\\mathbf{x}}(A)\n$$\nis simply a mixture of two distributions, which we know how to sample from. Thus,\na sample can be drawn from $P(\\mathbf{x},\\cdot)$ via the following algorithm:\n1. Sample $\\mathbf{y} \\sim Q(\\mathbf{x}, \\cdot)$.\n2. Conditional on $\\mathbf{y}$, sample from $\\alpha(\\mathbf{x},\\mathbf{y})\\delta_{\\mathbf{y}} + [1-\\alpha(\\mathbf{x},\\mathbf{y})] \\delta_{\\mathbf{x}}$.\n\nFor the second step, the mixture can be sampled from using the simple algorithm\ndiscussed in the previous section: randomly select one of the two kernels with\nprobabilities equal to their mixture weights, then return a sample from the\nselected kernel. Since sampling from the Dirac measure $\\delta_{\\mathbf{x}}$ simply\nmeans returning the value $\\mathbf{x}$ (and similarly for $\\delta_{\\mathbf{y}}$)\nthen this step will simply return $\\mathbf{x}$ or $\\mathbf{y}$ according to\ntheir respective probabilities $\\alpha(\\mathbf{x},\\mathbf{y})$ and\n$1-\\alpha(\\mathbf{x},\\mathbf{y})$. This is precisely the MH accept-reject\nmechanism!\n\nIt might be helpful to make this more concrete by letting $\\mathbf{X}_k$ denote the\nvalue of the MCMC algorithm at iteration $k$ and letting\n $\\mathbf{Y}|\\mathbf{X}_k \\sim Q(\\mathbf{X}_k, \\cdot)$ be the random variable\n representing the proposal. Then the above mixture corresponds to the probability\n$\\mathbb{P}\\left(\\mathbf{X}_{k+1} \\in A | \\mathbf{X}_k=\\mathbf{x}, \\mathbf{Y}=\\mathbf{y}\\right)$\nso we can re-write (6) as\n$$\nP(\\mathbf{x},A)\n= \\int_{\\mathcal{X}} \\mathbb{P}\\left(\\mathbf{X}_{k+1} \\in A | \\mathbf{X}_k=\\mathbf{x}, \\mathbf{Y}=\\mathbf{y}\\right) q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y}.\n$$\nOnce we condition on $\\mathbf{X}_k$ and $\\mathbf{Y}$, the only remaining randomness in\nthe probability above is coming from the selection of one of the two kernels.\n\n## Conclusion\nWhile all of this might appear to be overcomplicating the very simple MH algorithm,\nI have found it a quite worthwhile exercise to contemplate some different perspectives\non the method, as well as to get some practice manipulating expressions involving\nprobability kernels and thinking through sampling schemes. The MH transition kernel\n(1) can easily be derived by thinking through the mechanics of the MH algorithm.\nIn this post, I showed in (4) how the kernel can be re-written as mixture of two\ndistributions and in (6) as a marginalized mixture of two distributions. It is\nthis final expression which provides the basis for a tractable simulation algorithm.\n","srcMarkdownNoYaml":"\n\nThe Metropolis-Hastings (MH) Markov Chain Monte Carlo (MCMC) method is typically\nintroduced in the form of a practical algorithm. In a more theoretically-oriented\ncontext, one might prove that the algorithm defines a Markov chain and derive\nthe associated transition (i.e. probability) kernel. I have found it insightful\nto also work through the derivations in the reverse order; given only the\ntransition kernel, how could one derive the well-known MH update? In other words,\nhow can you simulate the Markov chain implied by the transition kernel? In this\npost, I work through the required derivations.\n\n## Setup and notation\nWe consider drawing samples from a target probability distribution $\\mu$ supported\non a state space $\\mathcal{X} \\subseteq \\mathbb{R}^D$ with Borel sigma algebra\n$\\mathcal{B}$. Let $\\pi: \\mathcal{X} \\to [0,\\infty]$ denote the Lebesgue density\nof $\\mu$, i.e.\n$$\n\\mu(A) = \\int_A \\mu(d\\mathbf{x}) = \\int_A \\pi(\\mathbf{x}) d\\mathbf{x}, \\qquad \\forall A \\in \\mathcal{B}.\n$$\nLet $Q: \\mathcal{X} \\times \\mathcal{B} \\to [0,1]$ denote the proposal kernel for the MH algorithm,\nwith $q(\\mathbf{x}, \\cdot)$ the Lebesgue density of the measure $Q(\\mathbf{x}, \\cdot)$.\nFor current state $\\mathbf{x} \\in \\mathcal{X}$ and proposal\n$\\mathbf{y} \\sim Q(\\mathbf{x}, \\cdot)$ we recall the MH acceptance probability\n$$\n\\alpha(\\mathbf{x}, \\mathbf{y})\n=\\min\\left(1, \\frac{\\pi(\\mathbf{y})q(\\mathbf{y},\\mathbf{x})}{\\pi(\\mathbf{x})q(\\mathbf{x},\\mathbf{y})} \\right).\n$$\nThroughout this post I will let $A \\in \\mathcal{B}$ denote an arbitrary Borel set.\nThe transition kernel $P:\\mathcal{X} \\times \\mathcal{B} \\to [0,1]$ implied by the MH algorithm is\nthen given by\n\\begin{align}\nP(\\mathbf{x},A)\n= \\int_A q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y} + \\delta_{\\mathbf{x}}(A) \\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})[1-\\alpha(\\mathbf{x},\\mathbf{y})] d\\mathbf{y} \\tag{1}\n\\end{align}\nwhere $\\delta_{\\mathbf{x}}(A) := \\mathbf{1}(\\mathbf{x} \\in A)$ denotes the Dirac\nmeasure. The first term in the kernel is the probability of accepting a proposal in\nthe set $A$, while the second accounts for the probability of rejection in the\ncase that the current state $\\mathbf{x}$ is already in $A$. I will denote the\noverall probability of acceptance by\n$$\n\\overline{a}(\\mathbf{x})\n:= \\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y}. \\tag{2}\n$$\n\n## Mixture of Kernels\nSuppose we don't already know the MH algorithm, and are given the task of writing\nan algorithm to draw a sample from the distribution $P(\\mathbf{x},\\cdot)$ defined in (1).\nA reasonable place to start is to try\nwriting $P(\\mathbf{x},\\cdot)$ as a mixture of kernels from which we already\nknow how to sample. Let's first try this idea, attempting to write\n$$\nP(\\mathbf{x},A) = wP_1(\\mathbf{x},A) + (1-w)P_2(\\mathbf{x},A), \\qquad w \\in [0,1], \\tag{3}\n$$\nwith $P_1$, $P_2$ transition kernels we already know how to sample from. If\nwe are able to do this, then we could easily sample from $P(\\mathbf{x},\\cdot)$\nvia the following simple algorithm:\n1. Select $P_1$ with probability $w$, else select $P_2$.\n2. Sample from the selected kernel.\n\nTo this end, let's manipulate (1) to write it in the form of a kernel mixture. We have\n\\begin{align}\nP(\\mathbf{x},A)\n&= \\int_A q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y} + \\delta_{\\mathbf{x}}(A) \\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})[1-\\alpha(\\mathbf{x},\\mathbf{y})] d\\mathbf{y} \\newline\n&= \\overline{a}(\\mathbf{x}) \\int_A \\frac{q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})}{\\overline{a}(\\mathbf{x})}d\\mathbf{y} + \\left[1-\\overline{a}(\\mathbf{x})\\right] \\delta_{\\mathbf{x}}(A) \\tag{4}\n\\end{align}\nwhich is a kernel mixture of the form (3) with\n\\begin{align}\nw = \\overline{a}(\\mathbf{x}), \\qquad P_1(\\mathbf{x},A)=\\int_A \\frac{q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})}{\\overline{a}(\\mathbf{x})}d\\mathbf{y},\n\\qquad P_2(\\mathbf{x},A)=\\delta_{\\mathbf{x}}(A).\n\\end{align}\nAll we did here was to multiply and divide by the acceptance probability $\\overline{a}(\\mathbf{x})$\nin the first term (under typical assumptions on $Q$ this will be non-zero when $\\mathbf{y}$\nis in the support of $\\pi$) and to rearrange the second term using the fact that\n$q(\\mathbf{x},\\cdot)$ is a probability density; hence,\n$$\n\\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y} = 1.\n$$\nNote that the mixture weight $\\overline{a}(\\mathbf{x})$ is the overall acceptance probability,\nand is thus in $[0,1]$ as required. Moreover, $P_2$ is the Dirac measure centered at $\\mathbf{x}$\nand is thus a valid kernel. To check that $P_1(\\mathbf{x},\\cdot)$ is a valid probability\nmeasure, we recall the form of $\\overline{a}(\\mathbf{x})$ from (2) to verify that\n\\begin{align}\nP_1(\\mathbf{x},A)\n&= \\int_{\\mathcal{X}} \\frac{q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})}{\\overline{a}(\\mathbf{x})}d\\mathbf{y} \\newline\n&= \\frac{\\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y}}{\\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y}} \\newline\n&= 1.\n\\end{align}\nThe other required properties (countable additivity and non-negativity) are similarly\nverified. Thus, $P_1(\\mathbf{x},\\cdot)$ is a probability measure with Lebesgue density\nproportional to $q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})$; the overall\nacceptance probability $\\overline{a}(\\mathbf{x})$ is the normalizing constant for this\ndensity.\n\nThis representation of the MH kernel as a mixture distribution is conceptually useful,\nbut it does not directly help us determine a sampling algorithm. Indeed, we cannot\nimplement the simple mixture sampling algorithm described above since (i.) computing\nthe mixture weight $\\overline{a}(\\mathbf{x})$ requires evaluating an intractable\nintegral, and (ii.) we don't know how to directly sample from the probability distribution\nwith density proportional to $q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})$.\nWhile this approach seems to be a dead end from a practical point of view,\nwe should keep in mind that the MH algorithm derived below does sample from the\nmixture (4), but does so in a way that avoids having to compute the mixture weight\nor to directly sample from $P_1(\\mathbf{x},\\cdot)$.\n\n## Marginalized Mixture of Kernels\nIn the previous section, we feigned ignorance of the MH algorithm in order to\napproach the problem of simulating (1) as a generic sampling problem.\nWe found that $P$ can indeed be written as a mixture of kernels, but the problem\nof sampling from the resulting mixture was also intractable. To take a step in the\nright direction, it is useful to cheat a little bit and recall some of the mechanics\nof the MH algorithm. The proposal $\\mathbf{y} \\sim Q(\\mathbf{x},\\cdot)$\nis accepted with probability $\\alpha(\\mathbf{x},\\mathbf{y})$; if rejected, the\nnext state is set to the current state $\\mathbf{x}$. Thus, it seems that we should\nbe looking for a mixture of kernels of the form\n$$\n\\alpha(\\mathbf{x},\\mathbf{y})\\delta_{\\mathbf{y}}(\\cdot) + [1-\\alpha(\\mathbf{x},\\mathbf{y})]\\delta_{\\mathbf{x}}(\\cdot). \\tag{5}\n$$\nOf course, this can't represent the whole picture since the mixture weight in (5)\ndepends on $\\mathbf{y}$ and the proposal kernel $Q$ is completely missing from the\nexpression. The key insight is that the MH kernel $P(\\mathbf{x},\\cdot)$ can be\nviewed as the expectation of (5) averaged with respect to $Q(\\mathbf{x},\\cdot)$;\ni.e., $P(\\mathbf{x},\\cdot)$ is derived by *marginalizing* the mixture (5)\nwith respect to $\\mathbf{y} \\sim Q(\\mathbf{x},\\cdot)$. To show this,\nwe return to the original expression (1) for the MH kernel. We have\n\\begin{align}\nP(\\mathbf{x},A)\n&= \\int_A q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y} + \\delta_{\\mathbf{x}}(A) \\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})[1-\\alpha(\\mathbf{x},\\mathbf{y})] d\\mathbf{y} \\newline\n&= \\int_{\\mathcal{X}} \\alpha(\\mathbf{x},\\mathbf{y}) \\mathbf{1}(\\mathbf{y} \\in A) q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y} +  \\int_{\\mathcal{X}} [1-\\alpha(\\mathbf{x},\\mathbf{y})] \\delta_{\\mathbf{x}}(A) q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y} \\newline\n&= \\int_{\\mathcal{X}} \\alpha(\\mathbf{x},\\mathbf{y}) \\delta_{\\mathbf{y}}(A) q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y} +  \\int_{\\mathcal{X}} [1-\\alpha(\\mathbf{x},\\mathbf{y})] \\delta_{\\mathbf{x}}(A) q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y} \\newline\n&= \\int_{\\mathcal{X}} \\left[\\alpha(\\mathbf{x},\\mathbf{y})\\delta_{\\mathbf{y}}(A) + [1-\\alpha(\\mathbf{x},\\mathbf{y})] \\delta_{\\mathbf{x}}(A) \\right] q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y}, \\tag{6}\n\\end{align}\nwhich is precisely the mixture (5) averaged (in $\\mathbf{y}$) with respect to the weights\n$q(\\mathbf{x},\\mathbf{y})$. We now have three different representations of the MH\ntransition kernel $P$: (1) is the most natural to derive when starting from the MH\nalgorithm, (4) represents $P$ as a mixture of two distributions, and (6) represents\n$P$ as a marginalized mixture of two distributions. It is this final representation which\nproves useful for developing a practical simulation algorithm.\n\nAll that remains is to recall how to sample from a marginalized distribution. First\nnote that $\\mathbf{x}$ is essentially a fixed parameter in the integral (6); the\naveraging is done with respect to $\\mathbf{y}$. Now, if we condition on a fixed\n $\\mathbf{y}$ as well, then the expression\n$$\n\\alpha(\\mathbf{x},\\mathbf{y})\\delta_{\\mathbf{y}}(A) + [1-\\alpha(\\mathbf{x},\\mathbf{y})] \\delta_{\\mathbf{x}}(A)\n$$\nis simply a mixture of two distributions, which we know how to sample from. Thus,\na sample can be drawn from $P(\\mathbf{x},\\cdot)$ via the following algorithm:\n1. Sample $\\mathbf{y} \\sim Q(\\mathbf{x}, \\cdot)$.\n2. Conditional on $\\mathbf{y}$, sample from $\\alpha(\\mathbf{x},\\mathbf{y})\\delta_{\\mathbf{y}} + [1-\\alpha(\\mathbf{x},\\mathbf{y})] \\delta_{\\mathbf{x}}$.\n\nFor the second step, the mixture can be sampled from using the simple algorithm\ndiscussed in the previous section: randomly select one of the two kernels with\nprobabilities equal to their mixture weights, then return a sample from the\nselected kernel. Since sampling from the Dirac measure $\\delta_{\\mathbf{x}}$ simply\nmeans returning the value $\\mathbf{x}$ (and similarly for $\\delta_{\\mathbf{y}}$)\nthen this step will simply return $\\mathbf{x}$ or $\\mathbf{y}$ according to\ntheir respective probabilities $\\alpha(\\mathbf{x},\\mathbf{y})$ and\n$1-\\alpha(\\mathbf{x},\\mathbf{y})$. This is precisely the MH accept-reject\nmechanism!\n\nIt might be helpful to make this more concrete by letting $\\mathbf{X}_k$ denote the\nvalue of the MCMC algorithm at iteration $k$ and letting\n $\\mathbf{Y}|\\mathbf{X}_k \\sim Q(\\mathbf{X}_k, \\cdot)$ be the random variable\n representing the proposal. Then the above mixture corresponds to the probability\n$\\mathbb{P}\\left(\\mathbf{X}_{k+1} \\in A | \\mathbf{X}_k=\\mathbf{x}, \\mathbf{Y}=\\mathbf{y}\\right)$\nso we can re-write (6) as\n$$\nP(\\mathbf{x},A)\n= \\int_{\\mathcal{X}} \\mathbb{P}\\left(\\mathbf{X}_{k+1} \\in A | \\mathbf{X}_k=\\mathbf{x}, \\mathbf{Y}=\\mathbf{y}\\right) q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y}.\n$$\nOnce we condition on $\\mathbf{X}_k$ and $\\mathbf{Y}$, the only remaining randomness in\nthe probability above is coming from the selection of one of the two kernels.\n\n## Conclusion\nWhile all of this might appear to be overcomplicating the very simple MH algorithm,\nI have found it a quite worthwhile exercise to contemplate some different perspectives\non the method, as well as to get some practice manipulating expressions involving\nprobability kernels and thinking through sampling schemes. The MH transition kernel\n(1) can easily be derived by thinking through the mechanics of the MH algorithm.\nIn this post, I showed in (4) how the kernel can be re-written as mixture of two\ndistributions and in (6) as a marginalized mixture of two distributions. It is\nthis final expression which provides the basis for a tractable simulation algorithm.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"MH-update-from-kernel.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","theme":["cosmo","brand"],"title-block-banner":true,"title":"Deriving the Metropolis-Hastings Update from the Transition Kernel","date":"2023-12-31","categories":["MCMC","Probability","Computational Statistics"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}