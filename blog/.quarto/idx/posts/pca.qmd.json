{"title":"Principal Components Analysis","markdown":{"yaml":{"title":"Principal Components Analysis","description":"A deep dive into PCA.","date":"2023-12-15","categories":["Statistics","Linear-Algebra"]},"headingText":"Part 1: Formulating and Solving the PCA Optimization Problem","containsRefs":false,"markdown":"\n\n\n## Setup and Notation\nSuppose that we have data $\\mathbf{x}_1, \\dots, \\mathbf{x}_N \\in \\mathbb{R}^D$, stacked into the\nrows of a matrix $X \\in \\mathbb{R}^{N \\times D}$. Our task is to find a subspace\nof smaller dimension $R < D$ such that the projection of the data points onto the\nsubspace retains as much information as possible. By restricting our attention to\northonormal bases for the low-dimensional subspace, we reduce the problem to finding\na set of orthonormal basis vectors\n$$\n\\begin{align}\n&\\mathbf{b}_1, \\dots, \\mathbf{b}_R \\in \\mathbb{R}^D,\n&&\\langle \\mathbf{b}_r, \\mathbf{b}_s \\rangle = \\delta_{r,s}.\n\\end{align}\n$$\nDefine $B \\in \\mathbb{R}^{D \\times R}$ to be the matrix with $r^{\\text{th}}$ column\nequal to $\\mathbf{b}_r$. The subspace generated by the basis $B$ is given by\n$$\n\\text{span}(B) := \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R).\n$$\nThroughout this post I will abuse notation by referring to the matrix $B$ when actually\ntalking about the set of vectors $\\{\\mathbf{b}_1, \\dots, \\mathbf{b}_R\\}$. Since there\nis no a priori reason to assume that the data is centered, we should also allow for\nthe subspace to be shifted by some intercept $\\mathbf{w}_0 \\in \\mathbb{R}^D$,\nresulting in the affine space\n$$\n\\mathbf{w}_0 + \\text{span}(B) = \\left\\{\\mathbf{w}_0 +\n\\sum_{r=1}^{R} w_r \\mathbf{b}_r : w_1, \\dots, w_R \\in \\mathbb{R} \\right\\}.\n$$\nLoosely speaking, the task is to find the basis\n$B$, intercept $\\mathbf{w}_0$, and pointwise weights\n$\\mathbf{w}_1, \\dots, \\mathbf{w}_N \\in \\mathbb{R}^R$ such that\n$$\n\\begin{align}\n\\mathbf{x}_n &\\approx \\mathbf{w}_0 + \\sum_{r=1}^{R} (\\mathbf{w}_n)_r \\mathbf{b}_r &&\\forall n=1,\\dots,N \\\\\n&= \\mathbf{w}_0 + B\\mathbf{w}_n.\n\\end{align}\n$$\nTo formalize this notion, PCA measures the error in the above approximation using\nEuclidean distance, averaged over the $N$ data points. To further simplify notation,\nwe stack the $\\mathbf{w}_n$ in the columns of a matrix $W \\in \\mathbb{R}^{R \\times N}$.\nWith all of this notation established, we can state that PCA solves the optimization\nproblem\n$$\n\\text{argmin}_{B, W, \\mathbf{w}_0} \\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - (\\mathbf{w}_0 + B\\mathbf{w}_n) \\rVert_2^2, \\tag{1}\n$$\nwhere the basis $B$ is constrained to be orthonormal.\nAs we will see, this optimization naturally breaks down into two distinct problems\nwhich can be solved sequentially:\n1. Given the basis $B$ and intercept $\\mathbf{w}_0$, find the optimal basis coefficients\n$\\mathbf{w}_n$ corresponding to each data point $\\mathbf{x}_n$.\n2. Find the optimal basis and intercept.\n\nPart of the popularity of PCA stems from the fact that both problems can be solved in\nclosed-form. Let us consider both problems in turn.\n\n## Optimizing the Basis Coefficients\nLet us first consider $\\mathbf{w}_0$ and $B$ to be fixed, meaning that we are fixing\nan affine subspace of dimension $R$. We seek to find the optimal way to represent\nthe data $X$ in this lower-dimensional space. As we will show, the Euclidean objective\nused by PCA implies that this problem reduces to straightforward orthogonal projection.\nFor now, let $\\mathbf{x}^c_n := \\mathbf{x}_n - \\mathbf{w}_0$ denote the centered\ndata points (we will deal with the intercept shortly). We are thus considering\nthe problem\n$$\n\\text{argmin}_{W} \\sum_{n=1}^{N} \\lVert \\mathbf{x}^c_n - B\\mathbf{w}_n \\rVert_2^2 \\tag{2}\n$$\nObserve that $\\mathbf{w}_n$ only appears in the $n^{\\text{th}}$ term of the sum,\nmeaning that we can consider each summand independently,\n$$\n\\text{argmin}_{\\mathbf{w}_n} \\lVert \\mathbf{x}^c_n - B\\mathbf{w}_n \\rVert_2^2.\n$$\nIn words, we seek the linear combination of the basis vectors $B$ that results\nin minimal Euclidean distance from $\\mathbf{x}^c_n$; this is a standard orthogonal\nprojection problem from linear algebra. Since the basis vectors are orthonormal,\nthe optimal projection coefficients are given by\n$$\n\\begin{align}\n&(\\mathbf{w}_n)_r = \\langle \\mathbf{x}_n^c, \\mathbf{b}_r \\rangle,\n&&\\mathbf{w}_n = B^\\top \\mathbf{x}_n^c\n\\end{align}\n$$\nwhich can be written succinctly for all data points by stacking the $\\mathbf{w}_n^\\top$\nas rows in a matrix $W$; i.e.,\n$$\nW := X^c B,\n$$\nwith $X^c$ denoting the centered data matrix with rows set to the\n$(\\mathbf{x}^c_n)^\\top$.\n\n## Optimizing the Basis\nIn the previous section, we saw that for a fixed basis and intercept, optimizing\nthe basis weights reduced to an orthogonal projection problem. In this section\nwe show that with the weights fixed at their optimal values, optimizing the basis\nreduces to solving a sequence of eigenvalue problems.\nTo be clear, we are now considering the problem\n$$\n\\text{argmin}_{B} \\sum_{n=1}^{N} \\lVert \\mathbf{x}^c_n - B\\mathbf{w}^*_n \\rVert_2^2, \\tag{3}\n$$\nwhere the $\\mathbf{w}^*_n$ are now fixed at the optimal values satisfying (2);\ni.e., $\\mathbf{w}^*_n = B^\\top \\mathbf{x}^c_n$. However, in the derivations\nbelow we will just write $\\mathbf{w}_n = \\mathbf{w}^*_n$ to keep the notation\nlighter. Note that we are still treating $\\mathbf{w}_0$ as fixed for the time being.\nWe will make another notational simplification in this section by writing\n$\\mathbf{x}_n = \\mathbf{x}_n^c$. Just keep in mind that\nthroughout this section, $\\mathbf{x}_n$ should be interpreted as $\\mathbf{x}_n - \\mathbf{w}_0$.\n\nThis problem is also referred to as minimizing the *reconstruction error*, since\n$\\lVert \\mathbf{x}_n - \\mathbf{\\hat{x}}_n \\rVert_2 := \\lVert \\mathbf{x}_n - B\\mathbf{w}_n \\rVert_2$\nis the error between the original\ndata point $\\mathbf{x}_n$ and the $D$-dimensional vector $\\mathbf{\\hat{x}}_n$ which\ncan be thought of as an approximation to $\\mathbf{x}_n$ that has been\n*reconstructed* from its lower-dimensional representation $\\mathbf{w}_n$. The key\nhere is to re-write this objective function so that this optimization problem\ntakes the form of an eigenvalue problem, which is something that we already know\nhow to solve (see the appendix, A1).\n\nTo start, we extend the orthonormal set $\\mathbf{b}_1, \\dots, \\mathbf{b}_R$\nto an orthonormal basis $\\mathbf{b}_1, \\dots, \\mathbf{b}_D$ for $\\mathbb{R}^D$.\nNow we can write the original data point $\\mathbf{x}_n$ and its approximation\n$\\mathbf{\\hat{x}}_n$ with respect to this basis as\n$$\n\\begin{align}\n&\\mathbf{x}_n = \\sum_{r=1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r,\n&&\\mathbf{\\hat{x}}_n = \\sum_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r\n\\end{align}\n$$\n\nand hence the residual $\\mathbf{x}_n - \\mathbf{\\hat{x}}_n$ is given by\n$$\n\\begin{align}\n\\mathbf{x}_n - \\mathbf{\\hat{x}}_n &= \\sum_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r.\n\\end{align}\n$$\n\nThus, the objective function in (3) can be written as\n$$\n\\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{\\hat{x}}_n \\rVert_2^2\n= \\sum_{n=1}^{N} \\bigg\\lVert \\sum_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r \\bigg\\rVert_2^2\n= \\sum_{n=1}^{N} \\sum_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2 \\lVert \\mathbf{b}_r \\rVert_2^2\n= \\sum_{n=1}^{N} \\sum_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2,\n$$\n\nwhere the second and third equalities use the facts that the $\\mathbf{b}_r$ are\northogonal and of unit norm, respectively.\n\nWe could continue working with this formulation, but at this point it is convenient to\nre-write the minimization problem we have been working with as an equivalent maximization\nproblem. Note that the above residual calculation is of the form\n$\\mathbf{\\hat{e}}_n = \\mathbf{x}_n - \\mathbf{\\hat{x}}_n$ (and summed over $n$).\nSince $\\mathbf{x}_n$ is fixed, then minimizing the residual (i.e., the\nreconstruction error) is equivalent to maximizing $\\mathbf{\\hat{x}}_n$. More\nrigorously, we have\n\n$$\n\\begin{align}\n\\text{argmin}_B \\sum_{n=1}^{N} \\sum_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\n&= \\text{argmin}_B \\sum_{n=1}^{N}\n\\left(\\sum_{r=1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2 - \\sum\\_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\\right) \\\\\n&= \\text{argmin}_B \\sum_{n=1}^{N}\n\\left(\\lVert \\mathbf{x}_n \\rVert_2^2 - \\sum_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\\right) \\\\\n&= \\text{argmax}_B \\sum_{n=1}^{N} \\sum_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2. \\tag{4}\n\\end{align}\n$$\n\nWe can now re-write the squared inner product to obtain\n$$\n\\begin{align}\n\\sum_{n=1}^{N} \\sum_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\n&= \\sum_{n=1}^{N} \\sum_{r=1}^{R} \\mathbf{b}_r^\\top \\mathbf{x}_n \\mathbf{x}_n^\\top \\mathbf{b}_r\n= \\sum_{r=1}^{R} \\mathbf{b}_r^\\top \\left(\\sum_{n=1}^{N}\\mathbf{x}_n \\mathbf{x}_n^\\top\\right) \\mathbf{b}_r\n= \\sum_{r=1}^{R} \\mathbf{b}_r^\\top (X^\\top X) \\mathbf{b}_r^\\top,\n\\end{align}\n$$\n\nwhere the final step uses [this](https://gregorygundersen.com/blog/2020/07/17/matmul/)\nfact. We have managed to re-write (3) as\n$$\n\\text{argmax}_{B} \\sum_{r=1}^{D} \\mathbf{b}_r^\\top (X^\\top X) \\mathbf{b}_r^\\top, \\tag{5}\n$$\nwhere we recall that this is also subject to the constraint that $B$ is orthogonal.\n\nBefore proceeding, we note that $X^\\top X$ is a positive semi-definite matrix,\nwhose eigenvalues we denote $\\lambda_1, \\dots, \\lambda_D$, sorted in decreasing\norder. Note that the eigenvalues are all non-negative due to the positive\ndefiniteness. Let $\\mathbf{e}_1, \\dots, \\mathbf{e}_D$ denote the respective\neigenvectors, normalized to have unit norm. These vectors are guaranteed to be\northogonal by the Spectral Theorem.\n\nWe now notice in (5) that the objective function has been decomposed into $R$ different\nterms, each of which only depends on a single $\\mathbf{b}_r$. However, these\ndo not constitute $R$ independent optimization problems, as they are all coupled\nthrough the orthogonality constraint. We will thus consider solving them in\na recursive fashion, beginning with the first term,\n$$\n\\text{argmax}_{\\lVert \\mathbf{b}_1 \\rVert_2=1} \\mathbf{b}_1^\\top (X^\\top X) \\mathbf{b}_1^\\top\n= \\text{argmax}_{\\lVert \\mathbf{b}_1 \\rVert_2=1} \\lVert X \\mathbf{b}_1 \\rVert_2^2.\n$$\nThis is an eigenvalue problem! It is precisely of the form (A4) (see appendix)\nand so we apply that result to conclude that the optimal argument is\n$\\mathbf{b}_1 = \\mathbf{e}_1$ with associated optimal value $\\lambda_1$ (note\nthe objective here is the *squared* norm, in contrast to the statement in the appendix). Taking\nthis as the base case, we now proceed inductively. Assume that at the\n$r^{\\text{th}}$ problem in the sequence, the solution is given by\n$(\\mathbf{b}_1, \\dots, \\mathbf{b}_r) = (\\mathbf{e}_1, \\dots, \\mathbf{e}_r)$.\nWe must show the solution to the $(r+1)^{\\text{st}}$ problem is\n$\\mathbf{e}_{r+1}$. Under the inductive hypothesis, this problem is constrained\nso that $\\mathbf{b}_{r+1}$ is orthogonal to each of $\\mathbf{e}_1, \\dots, \\mathbf{e}_r$;\ni.e., we require $\\mathbf{b}_{r+1} \\perp \\text{span}(\\mathbf{e}_1, \\dots, \\mathbf{e}_r)$.\nIf we denote $\\mathcal{E}_{r} := \\text{span}(\\mathbf{e}_1, \\dots, \\mathbf{e}_r)$\nand $\\mathcal{E}^{\\perp}_{r}$ the orthogonal complement of $\\mathcal{E}_{r}$,\nthen a succinct way to write the orthogonality constraint is that $\\mathbf{b}_{r+1} \\in \\mathcal{E}^{\\perp}_r$.\nThe problem can thus be written as\n$$\n\\begin{align}\n\\text{argmax}_{\\mathbf{b}_{r+1} \\in \\mathcal{E}^{\\perp}_{r}, \\lVert \\mathbf{b}_{r+1} \\rVert_2=1} \\lVert X \\mathbf{b}_{r+1} \\rVert_2^2, \\tag{6}\n\\end{align}\n$$\nwhich is another eigenvalue problem, precisely of the form (A3). Using this\nresult from the appendix, we conclude that this is solved by\n$\\mathbf{b}_{r+1} = \\mathbf{e}_{r+1}$, with the maximal objective value $\\lambda_{r+1}$.\n\nThat was a lot, so before moving on let's briefly summarize. First of all, recall\nthat I have been abusing notation by writing $\\mathbf{x}_n$ where I should be writing\n$\\mathbf{x}_n^c = \\mathbf{x}_n - \\mathbf{w}_0$. In summarizing the result here I will make this correction.\nHere we have considered the problem of finding the optimal orthonormal basis $B$,\nfor any fixed $\\mathbf{w}_0 \\in \\mathbb{R}^D$, but with the $\\mathbf{w}_n$ set to\ntheir optimal values satisfying (2); i.e., $\\mathbf{w}_n = B^\\top \\mathbf{x}^c_n$.\nGiven this, we showed that the reconstruction error (5) is minimized by setting\n$B$ equal to the matrix with columns given by the dominant $R$\n(normalized) eigenvectors of $(X^c)^\\top X^c$. We arrived at this solution by showing\nthat the error minimization problem (5) could be viewed as a sequence of $R$\neigenvalue problems.\n\n## Optimizing the Intercept\nThe last ingredient we are missing to solve (1) is the optimal value of $\\mathbf{w}_0$,\nwhich has henceforth been viewed as fixed in the above derivations. At first\nglance, this problem might seem like somewhat of an afterthought,\nbut there are some subtleties that are worth exploring here.\n\nThe problem we are now considering is\n$$\n\\text{argmin}_{\\mathbf{w}_0} \\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{w}_0 - B\\mathbf{w}^*_n \\rVert_2^2, \\tag{7}\n$$\nwith $\\mathbf{w}^*_n$ denoting the optimal weights $\\mathbf{w}^*_n = B^\\top \\mathbf{x}_n$\nderived above (these derivations will go through with any orthonormal basis $B$).\nPlugging in this expression for $\\mathbf{w}^*_n$ gives\n$$\n\\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{w}_0 - B\\mathbf{w}^*_n \\rVert_2^2\n= \\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{w}_0 - BB^\\top \\mathbf{x}_n \\rVert_2^2\n= \\sum_{n=1}^{N} \\lVert (I - BB^\\top)(\\mathbf{x}_n - \\mathbf{w}_0) \\rVert_2^2.\n$$\nComputing the gradient of this expression with respect to $\\mathbf{w}_0$ and setting\nit equal to zero yields the optimality condition\n$$\n\\sum_{n=1}^{N} (I - BB^\\top)(\\mathbf{x}_n - \\mathbf{w}_0) = 0,\n$$\nwhere we have used the fact that $(I - BB^\\top)^2 = (I - BB^\\top)$ (since\n$I - BB^\\top$ is a projection matrix; see appendix). By linearity we then have\n$$\n\\sum_{n=1}^{N} (I - BB^\\top)(\\mathbf{x}_n - \\mathbf{w}_0)\n= (I - BB^\\top) \\sum_{n=1}^{N} (\\mathbf{x}_n - \\mathbf{w}_0)\n= (I - BB^\\top)(N \\bar{\\mathbf{x}} - N\\mathbf{w}_0), \\tag{7}\n$$\nwhere we have defined\n$$\n\\bar{\\mathbf{x}} := \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}_n,\n$$\nthe empirical mean of the data. Since $\\mathbf{w}_0$ is optimal when (8)\nis equal to zero, this leads to the condition\n$$\n(I - BB^\\top)(\\bar{\\mathbf{x}} - \\mathbf{w}_0) = 0,\n$$\nor equivalently,\n$$\n\\bar{\\mathbf{x}} - \\mathbf{w}_0 \\in \\text{Null}(I - BB^\\top).\n$$\nNoting that the null space is non-trivial, since\n$\\text{Null}(I - BB^\\top) = \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R)$\n(again, see the appendix section on projection matrices),\nwe then conclude that there are infinitely many optimal solutions!\nUsing the basis $\\mathbf{b}_1, \\dots, \\mathbf{b}_R$ for the null space, we can\ncharacterize the set of optimal $\\mathbf{w}_0$ as those satisfying\n$$\n\\bar{\\mathbf{x}} - \\mathbf{w}_0 \\in \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R),\n$$\nor, more explicitly, all vectors within the affine space\n$$\n\\mathbf{w}_0 \\in \\bar{\\mathbf{x}} + \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R). \\tag{9}\n$$\nWhile we have an infinity of optimal solutions we could choose from, the obvious\nchoice $\\mathbf{w}_0^* := \\bar{\\mathbf{x}}$ stands out. Indeed, this is the choice\nthat is essentially always made in practice, so much so that many PCA tutorials\nwill begin by assuming that the data points have all been centered by subtracting\noff their empirical mean. I find it more insightful to include the intercept as a variable\nin the PCA optimization problem, and then show that the choice to set it equal\nto $\\bar{\\mathbf{x}}$ is actually justified.\n\nMoreover, it is quite interesting that the mean is not actually the unique optimal\nchoice here. Why is this? The characterization (9) says that we can add any\nvector lying in the span of the orthonormal basis to $\\bar{\\mathbf{x}}$ and still\nmaintain optimality. So the key requirement is that, after shifting the data by\nsubtracting off $\\mathbf{w}_0$, the resulting shifted points must \"lie along\" the\nlower-dimensional subspace $\\text{span}(B)$.\nSince, $\\text{span}(B)$ defines a hyperplane,\nthe data must lie somewhere along this plane; from the perspective of the\noptimization problem, it doesn't matter whether it lies around the origin or somewhere\nvery far away, so long as it is clustered around this plane.\nA picture is worth a thousand words here, and I will\ntry to add one once I have time.\n\nFinally, note that the specific choice of $\\bar{\\mathbf{x}}$ has various other\npractical benefits. It leads to projections that are clustered around the origin,\nthus keeping numbers relatively small. It also leads to a nice statistical\ninterpretation of the eigenvalue problems discussed in the previous subsection;\ne.g. the basis vector $\\mathbf{b}_1$ can be viewed as the direction along which\nthe empirical variance of the projected data is maximized. This maximum variance\nperspective is discussed in more detail below.\n\n\n# Part 2: Interpreting PCA\n\n## Minimum Error or Maximum Variance?\nWhile the derivations in the preceding section are somewhat lengthy, recall\nthat this was all in the pursuit of solving the optimization problem (1). In words,\nwe derived the best $R$-dimensional affine subspace to represent the data $X$,\nwhere \"best\" is defined as minimizing the average Euclidean error between the data points\nand their projections onto the subspace. We showed that this error minimization problem\ncould be re-written as a sequence of $R$ maximization problems of the form (6).\nWe now show that these maximization problems have a very nice statistical\ninterpretation.\n\n### Sample covariance of the $\\mathbf{x}_n$\nWe first recall that the empirical covariance matrix\nof the data points $\\mathbf{x}_1, \\dots, \\mathbf{x}_N$ is defined to be\n$$\n\\hat{C} := \\frac{1}{N-1} \\sum_{n=1}^{N} (\\mathbf{x}_n - \\bar{\\mathbf{x}}) (\\mathbf{x}_n - \\bar{\\mathbf{x}})^\\top,\n$$\nwhich can be re-written as\n$$\n\\hat{C} = \\frac{1}{N-1} \\sum_{n=1}^{N} \\mathbf{x}^c_n (\\mathbf{x}^c_n)^\\top = \\frac{1}{N-1} X^c (X^c)^\\top, \\tag{10}\n$$\nwhere the superscript *c* indicates that the observations have been centered by\nsubtracting off their empirical mean.\n\nRecall that solving the maximization\nproblems (6) revealed that the optimal basis vectors are given by the dominant\neigenvectors of the matrix $X^c (X^c)^\\top$, which is the (unscaled) covariance (10)!\nThe scaling factor does not affect the optimal basis vectors, it simply scales\nthe objective function. Specifically,\n$$\n\\begin{align}\n\\text{argmax}_{\\mathbf{b}_{r+1} \\in \\mathcal{E}^{\\perp}_{r}, \\lVert \\mathbf{b}_{r+1} \\rVert_2=1}\n\\left(\\mathbf{b}_{r+1}^\\top X^c (X^c)^\\top \\mathbf{b}_{r+1}\\right)\n= \\text{argmax}_{\\mathbf{b}_{r+1} \\in \\mathcal{E}^{\\perp}_{r}, \\lVert \\mathbf{b}_{r+1} \\rVert_2=1}\n\\left(\\mathbf{b}_{r+1}^\\top \\hat{C} \\mathbf{b}_{r+1}\\right). \\tag{11}\n\\end{align}\n$$\nWe haven't changed anything from (6) here, other than noting that a re-scaling\nof the objective function allows us involve $\\hat{C}$ in the expression.\n\n### Sample covariance of the $\\mathbf{w}_n$\nGiven that the sample covariance matrix of the data\n$\\mathbf{x}_1, \\dots, \\mathbf{x}_N \\in \\mathbb{R}^D$ is given by $\\hat{C}$,\nit is natural to also consider the empirical covariance of\n$\\mathbf{w}_1, \\dots, \\mathbf{w}_N \\in \\mathbb{R}^R$. We begin by computing the\nsample mean\n$$\n\\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{w}_n\n= \\frac{1}{N} \\sum_{n=1}^{N} B\\mathbf{x}^c_n\n= B \\left[\\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}^c_n \\right] = 0,\n$$\nusing the fact that the empirical mean of the centered data points is $0$.\nRecalling that the row vectors $\\mathbf{w}_n^\\top$ are stored in the rows of\nthe matrix $W$, it follows that the empirical covariance matrix of\n$\\mathbf{w}_1, \\dots, \\mathbf{w}_N$ is given by\n$$\n\\hat{C}_w := \\frac{1}{N-1} W^\\top W, \\tag{12}\n$$\nwhich follows from the calculation (10) with $W$ in place of $X^c$. Since\n$W = XB$, we have\n$$\n\\hat{C}_w = \\frac{1}{N-1} (XB)^\\top (XB) = B^\\top \\hat{C} B,\n$$\nwhich allows us to write the covariance of the $\\mathbf{w}_n$ as a function\nof the covariance of the $\\mathbf{x}_n$. We can say even more since\nwe know that $B$ is given by $V_R$, the truncated set of eigenvectors obtained\nfrom the eigendecomposition $X^\\top X = V \\Lambda V^\\top$.\nWe thus have\n$$\n\\hat{C}_w\n= \\frac{1}{N-1} B^\\top (X^\\top X) B\n= \\frac{1}{N-1} V_R^\\top (X^\\top X) V_R\n= \\frac{1}{N-1} V_R^\\top V_R \\Lambda_R\n= \\frac{1}{N-1} \\Lambda_R,  \\tag{13}\n$$\nwhere $(X^\\top X) V_R = V_R \\Lambda_R$ follows from the fact that the columns\nof $V_R$ store the first $R$ eigenvectors of $X^\\top X$. The conclusion here\nis that $\\hat{C}_w$ is diagonal, with variances equal to the eigenvalues\nof $\\hat{C}$. In other words, PCA computes a change-of-basis that diagonalizes\nthe empirical covariance of the data.\n\n### PCA as Variance Maximization\nWe now return to the goal of providing a statistical interpretation of the\nobjective function $\\mathbf{b}_{r}^\\top \\hat{C} \\mathbf{b}_r$ in (11).\nGiven the derivation of $\\hat{C}_w$ in (13), we see the empirical variance\nof $(\\mathbf{w}_1)_r, \\dots, (\\mathbf{w}_N)_r$ (i.e., the values in the\n$r^{\\text{th}}$ column of $W$) is equal to\n$$\n[\\hat{C}_w]_{rr} = \\mathbf{b}_r^\\top \\hat{C} \\mathbf{b}_r,\n$$\nwhich is precisely the objective being maximized. To interpret this quantity\nmore clearly, we consider the projection of $\\mathbf{x}_n^c$ onto the\nspan of $\\mathbf{b}_r$,\n$$\n\\text{proj}_{\\mathbf{b}_r} \\mathbf{x}^c_n\n:= \\langle \\mathbf{x}^c_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r\n= (\\mathbf{w}_n)_r \\mathbf{b}_r,\n$$\nwhich implies that $(\\mathbf{w}_n)_r$ is the magnitude of the projection. Therefore,\nwe conclude that $\\mathbf{b}_r^\\top \\hat{C} \\mathbf{b}_r$ is\nthe sample variance of the magnitude of the projections onto the subspace\n$\\text{span}(\\mathbf{b}_r)$; loosely speaking, the variance of the projection\nalong the $r^{\\text{th}}$ basis vector. Combining all of these equivalent\nexpressions yields the chain of equalities,\n$$\n\\text{Tr}(\\hat{C}_w)\n= \\frac{1}{N-1} \\sum_{r=1}^{R} \\lambda_r\n= \\sum_{n=1}^{N} \\sum_{r=1}^{R} W_{nr}^2.\n= \\sum_{r=1}^{R} \\mathbf{b}_r^\\top \\hat{C} \\mathbf{b}_r \\tag{15}\n$$\nThe trace $\\text{Tr}(\\hat{C}_w)$ thus represents the total variance of the projection,\nsummed over all of the basis vectors. The total variance is equivalently given\nby the sum of the eigenvalues of $\\hat{C}$ or by the squared Frobenius norm\n$\\lVert W \\rVert_F^2$.\n\nThe final term in (15) provides an alternative\ninterpretation of the objective function in (5); namely, that PCA seeks\nthe basis $B$ which results in the maximal projected total variance. The\nresulting sequence of constrained problems, as in (11), are interpreted similarly.\nIn particular,\n$$\n\\text{argmax}_{\\mathbf{b}_{r+1} \\in \\mathcal{E}^{\\perp}_{r}, \\lVert \\mathbf{b}_{r+1} \\rVert_2=1}\n\\left(\\mathbf{b}_{r+1}^\\top \\hat{C} \\mathbf{b}_{r+1}\\right)\n$$\ncan be viewed as seeking the direction along which the variance of the\nprojections is maximized, subject to the constraint that the direction be\northogonal to the previous $r$ directions. The optimal solution is the direction\ncorresponding to the $(r+1)^{\\text{st}}$ eigenvector of the empirical\ncovariance $\\hat{C}$, and the resulting maximal variance in this direction is\ngiven by the associated eigenvalue.\n\n## The Singular Value Decomposition\n\n## A Matrix Approximation Problem: The Eckart-Young Theorem\nIgnoring the intercept (or assuming that the $\\mathbf{x}_n$ have already been\ncentered), we can re-write the reconstruction error as\n$$\n\\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - B\\mathbf{w}_n \\rVert_2^2\n= \\lVert X - WB^\\top \\rVert_F^2,\n$$\nwhere $\\lVert \\cdot \\rVert_F$ denotes the\n[Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm).\nThe PCA optimization problem can then be written as the\nmatrix approximation problem\n$$\n\\text{argmin}_{B, W} \\lVert X - WB^\\top \\rVert_F^2, \\tag{10}\n$$\nwhere $B$ is constrained to be an orthogonal matrix. We can equivalently write this\nas\n$$\n\\begin{align}\n&\\text{argmin}_{\\hat{X} \\in \\mathcal{M}} \\lVert X - \\hat{X} \\rVert_F^2,\n&&\\mathcal{M} = \\{\\hat{X} \\in \\mathbb{R}^{N \\times D} : \\hat{X}=WB^\\top, B \\in \\mathbb{R}^{D \\times R} \\text{ is orthogonal}\\}, \\tag{11}\n\\end{align}\n$$\nwhich makes it even more clear that PCA can generically be viewed as the problem\nof approximating the data matrix $X$ with another matrix $\\hat{X}$ that is\nconstrained to lie in a subset $\\mathcal{M}$ of all $N \\times D$ matrices.\nWe can phrase this even more succinctly by noticing that $\\mathcal{M}$ is\nprecisely the set of all $N \\times D$ matrices with rank at most $R$.\nIndeed, if $\\hat{X} \\in \\mathcal{M}$ then\n$\\text{rank}(\\hat{X}) = \\text{rank}(W B^\\top) \\leq R$ since $W$ has $R$ columns\nand thus the rank of this matrix product cannot exceed $R$. Conversely, if\nwe let $\\hat{X}$ be an arbirary $N \\times D$ matrix of rank $r \\leq R$ then $\\hat{X}$\ncan be expanded\nusing the [compact SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition#Reduced_SVDs)\nas\n$$\n\\hat{X} = U\\tilde{\\Sigma}V^\\top = (U\\tilde{\\Sigma})V^\\top,\n$$\nwhere $U \\in \\mathbb{R}^{N \\times r}$, $V \\in \\mathbb{R}^{D \\times r}$ are\northogonal and $\\tilde{\\Sigma} \\in \\mathbb{R}^{r \\times r}$ is diagonal.\nBy setting $W := U\\tilde{\\Sigma}$ and $B := V$ we have almost written things\nin the form required by (11), but (11) restricts $B$ to be orthogonal with\nexactly $R$ columns. Thus, we can simply extend $V$ to have $R$ orthonormal columns\nby appending columns $B = [V|\\mathbf{v}_{r+1}, \\dots, \\mathbf{v}_{R}]$ (this is\njustified by Gram-Schmidt) and then set $W = [U\\tilde{\\Sigma}|\\boldsymbol{0}]$.\nThus, $\\hat{X} = (U\\tilde{\\Sigma})V^\\top = WB^\\top$ with $B$ now of the required\nform.\n\nWe have thus verified that (11) can equivalently be written as the low-rank\nmatrix approximation problem\n$$\n\\begin{align}\n&\\text{argmin}_{\\hat{X} \\in \\mathcal{M}} \\lVert X - \\hat{X} \\rVert_F^2,\n&&\\mathcal{M} = \\{\\hat{X} \\in \\mathbb{R}^{N \\times D} : \\text{rank}(\\hat{X}) \\leq R \\}. \\tag{12}\n\\end{align}\n$$\n\nThis is precisely the problem\nconsidered by the celebrated [Eckart-Young theorem](https://en.wikipedia.org/wiki/Low-rank_approximation).\nThe theorem concludes that the optimal solution to (12) is given by the truncated\nSVD $X = U_R \\Sigma_R V_R^\\top$, which is precisely the PCA solution computed\nusing SVD as discussed in the previous section.\n\n## Regression with an optimal basis\n\n## Statistical/Probabalistic Perspectives\nThere are various ways we might attach a statistical or probabilistic perspective\nto PCA - we briefly discuss a few of them here. Throughout this section we\nwill take\n$$\n\\text{argmin}_{W,B} \\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - B\\mathbf{w}_n \\rVert_2^2  \\tag{13}\n$$\nas the jumping off point for a probabilistic generalization; that is, we are\nimplicitly assuming the data is centered so that we can ignore the intercept.\nNote that, as always, $B$ is constrained to be orthogonal.\n\n### A Special Case of the Karhunen-Loeve Expansion\nWe start by noting that the objective function in (13) looks like a sample\naverage of the quantities $\\lVert \\mathbf{x}_n - B\\mathbf{w}_n \\rVert_2^2$.\nIt is therefore natural to consider some underlying true expectation that this\nsample average is approximating. To this end, let us view $\\mathbf{x} = \\mathbf{x}(\\omega)$\nand $\\mathbf{w} = \\mathbf{w}(\\omega)$ as random vectors, defined over some probability space\n$(\\Omega, \\mathcal{A}, \\mathbb{P})$. We can then consider\n$$\n\\text{argmin}_{\\mathbf{w},B} \\mathbb{E} \\lVert \\mathbf{x} - B\\mathbf{w} \\rVert_2^2\n= \\text{argmin}_{\\mathbf{w},B} \\int_{\\Omega} \\left\\lVert \\mathbf{x}(\\omega) - \\sum_{r=1}^{R} \\mathbf{w}_r(\\omega) \\mathbf{b}_r \\right\\rVert_2^2 \\mathbb{P}(d\\omega), \\tag{14}\n$$\nwhich can be viewed as the population analog of the sample approximation in\n(13). We note that the second expression above shows that the low-rank\napproximation of the random vector $\\mathbf{x}$ takes the form of a linear\ncombination of (non-random) basis vectors, with random coefficients.\n\nWith $B$ fixed, the optimization in $\\mathbf{w}$ is just as easy as before.\nIndeed, for any fixed $\\omega \\in \\Omega$,\n$$\n\\text{argmin}_{\\mathbf{w}(\\omega)} \\lVert \\mathbf{x}(\\omega) - B\\mathbf{w}(\\omega) \\rVert_2^2 = B^\\top \\mathbf{x}(\\omega),\n$$\nusing the same exact orthogonal projection reasoning as before. We have thus\noptimized for $\\mathbf{w}(\\omega)$ on an $\\omega$-by-$\\omega$ basis. The same\nresult thus holds in expectation:\n$$\n\\text{argmin}_{\\mathbf{w}} \\mathbb{E} \\lVert \\mathbf{x} - B\\mathbf{w} \\rVert_2^2 = B^\\top \\mathbf{x}.\n$$\nIt is important to note that we have found the optimal *random vector* $\\mathbf{w}$,\nwhich was shown to be a linear transformation of the random vector $\\mathbf{x}$.\n\nOptimizing for $B$ with the optimal $\\mathbf{w}$ fixed also follows similarly\nfrom previous derivations. Using some of the results already derived above\n(see the section on optimizing the basis), we have\n\n$$\n\\begin{align}\n\\text{argmin}_{B} \\mathbb{E}\\lVert \\mathbf{x} - B\\mathbf{w} \\rVert_2^2\n&= \\text{argmin}_{B} \\mathbb{E} \\sum_{r=R+1}^{D} \\langle \\mathbf{x}, \\mathbf{b}_r \\rangle^2 \\newline\n&= \\text{argmin}_{B} \\mathbb{E} \\left[\\lVert \\mathbf{x} \\rVert_2^2 - \\sum_{r=1}^{R} \\langle \\mathbf{x}, \\mathbf{b}_r \\rangle^2 \\right] \\newline\n&= \\text{argmax}_{B} \\mathbb{E} \\sum_{r=1}^{R} \\langle \\mathbf{x}, \\mathbf{b}_r \\rangle^2 \\newline\n&= \\text{argmax}_{B} \\mathbb{E} \\sum_{r=1}^{R} \\mathbf{b}_r^\\top \\mathbf{x}\\mathbf{x}^\\top \\mathbf{b}_r \\newline\n&= \\text{argmax}_{B} \\sum_{r=1}^{R} \\mathbf{b}_r^\\top \\mathbb{E}\\left[\\mathbf{x}\\mathbf{x}^\\top\\right] \\mathbf{b}_r \\newline\n&= \\text{argmax}_{B} \\sum_{r=1}^{R} \\mathbf{b}_r^\\top C \\mathbf{b}_r,\n\\end{align}\n$$\n\nwhere $C := \\text{Cov}[\\mathbf{x}]$. Here we have used the centering assumption\n$\\mathbb{E}[\\mathbf{x}] = 0$, as well as the implicit assumption that the\nrandom vector $\\mathbf{x}$ has a well-defined covariance matrix. At this point,\nthe derivations go through exactly as before, with $C$\nreplacing the empirical covariance $\\hat{C}$; that is, the optimal basis $B$ is\nobtained by extracting the first $R$ columns of $V$, where $C = V\\Lambda V^\\top$.\nUnsurprisingly, the results that held for the sample covariance hold\nanalogously in this setting. For example, since $\\mathbf{x}$ has zero mean then\n$$\n\\mathbb{E}[\\mathbf{w}_r] = \\mathbb{E}[\\mathbf{v}_r^\\top \\mathbf{x}] = 0.\n$$\nMoreover,\n$$\n\\text{Cov}[\\mathbf{w}_r, \\mathbf{w}_s]\n= \\text{Cov}[\\mathbf{v}_r^\\top \\mathbf{x}, \\mathbf{v}_s^\\top \\mathbf{x}]\n= \\mathbf{v}_r^\\top C \\mathbf{v}_s\n= \\mathbf{v}_r^\\top V \\Lambda V^\\top \\mathbf{v}_s\n= \\lambda_r 1[r = s],\n$$\nwhich says that the weights $\\mathbf{w}_r$ are uncorrelated, with variance equal\nto their respective eigenvalues. It is common, therefore, to normalize these\nrandom variables to have unit variance\n$$\n\\mathbf{\\tilde{w}}_r := \\frac{1}{\\sqrt{\\lambda_r}} \\mathbf{w}_r,\n$$\nwhich means the optimal rank $R$ approximation to the random vector $\\mathbf{x}$\nmay be expressed as\n$$\n\\mathbf{\\hat{x}}(\\omega) := \\sum_{r=1}^{R} \\sqrt{\\lambda_r} \\mathbf{\\tilde{w}}_r(\\omega) \\mathbf{v}_r,\n$$\nwhere we have included the $\\omega$ argument to emphasize which quantities are\nrandom here. The following result summarizes our findings, extending to the\ncase with non-zero mean.\n<blockquote>\n  <p><strong>Proposition.</strong>\n  Let $\\mathbf{x}$ be a square-integrable $D$-dimensional random vector defined on\n  $(\\Omega, \\mathcal{A}, \\mathbb{P})$. Among all such random vectors constrained\n  to take values in an $R$-dimensional subspace of $\\mathbb{R}^D$, the random\n  vector\n  $$\n  \\mathbf{\\hat{x}}(\\omega) = \\mathbb{E}[\\mathbf{x}] + \\sum_{r=1}^{R} \\sqrt{\\lambda_r} \\mathbf{\\tilde{w}}_r(\\omega) \\mathbf{v}_r, \\tag{15}\n  $$\n  provides an optimal approximation to $\\mathbf{x}$, in the expected Euclidean\n  distance sense (14). Moreover, the random weights $\\mathbf{\\tilde{w}}_r$ are\n  uncorrelated, have zero mean and unit variance.\n  </p>\n</blockquote>\n\nNote that random vectors can conceptually be thought of as stochastic processes\nwith finite-dimensional index sets. A similar decomposition to (15) can\nbe constructed for more general stochastic processes with potentially\nuncountable index sets, with the modification that the sum in (15) will require\na countably infinite number of terms. This result is generally known as the\n**Karhunen-Loeve expansion**. Thus, from this perspective PCA can be thought of\nas the special finite-dimensional case of the Karhunen-Loeve expansion.\n\n#### Gaussian Random Vectors\nThe above derivations did not make any assumptions regarding the distribution\nof $\\mathbf{x}$, just that its covariance exists. Consequently, the main result\n(15) does not give the distribution of the weights $\\mathbf{\\tilde{w}}_r$,\nonly that they are uncorrelated, have zero mean, and unit variance. If we add\nthe distributional assumption $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{m}, C)$ then\nwe are able to say more. Indeed, recalling that the (unscaled) weights are\ngiven by projections $\\mathbf{w}_r = \\langle \\mathbf{x} - \\mathbf{m}, \\mathbf{v}_r \\rangle$,\nthen we find that\n$$\n\\mathbf{w} = V^\\top (\\mathbf{x} - \\mathbf{m}) \\sim \\mathcal{N}(0, V^\\top C V).\n$$\nThe key point here are that the $\\mathbf{w}_r$ are *jointly* Gaussian, and hence\ntheir uncorrelatedness implies that they are in fact independent. The weights\ninherit Gaussianity from $\\mathbf{x}$. Thus, under this stronger assumption\nwe are able to characterize the distribution of the weights exactly.\n\n\n### Maximum Likelihood Estimation with Gaussian Noise\n### Probabilistic PCA\nhttps://stats.stackexchange.com/questions/190308/why-does-probabilistic-pca-use-gaussian-prior-over-latent-variables\n\n# Part 3: Using PCA\n1. Decorrelating\n2. Dimensionality reduction\n\n# Part 4: Computing PCA\n1. Eigendecomposition\n2. SVD\n\n# Part 5: Application and Code\n\n\n# Appendix\n\n## Eigenvalue Problems\nIn this section, I briefly discuss the spectral norm and eigenvalue problems in\nfinite-dimensional vector spaces, which I utilize above when optimizing the basis\n$B$ in the PCA derivation. Consider a matrix $A \\in \\mathbb{R}^{N \\times D}$,\nwhich represents a linear transformation from $\\mathbb{R}^{D}$ to $\\mathbb{R}^N$.\nWe define the\n**spectral norm** of $A$ as the largest factor by which the map\n$A$ can \"stretch\" a vector $\\mathbf{u} \\in \\mathbb{R}^{D}$,\n$$\n\\lVert A \\rVert_2 := \\max_{\\lVert \\mathbf{u} \\rVert_2 \\neq \\boldsymbol{0}}\n\\frac{\\lVert A\\mathbf{u} \\rVert_2}{\\lVert \\mathbf{u} \\rVert_2}.\n$$\nUsing the linearity of $A$, one can show that we need only consider vectors of\nunit length; that is,\n$$\n\\lVert A \\rVert_2 = \\max_{\\lVert \\mathbf{u}_2 \\rVert=1} \\lVert A\\mathbf{u} \\rVert_2. \\tag{A1}\n$$\nOptimization problems of this type are called **eigenvalue problems** for reasons that will\nshortly become clear. For the purposes of the PCA derivation, we will require\nconsideration of a slightly more general eigenvalue problem. To define this problem, first\nnote that the matrix $A^\\top A$ is symmetric, positive semi-definite since\n$$\n\\begin{align}\n&(A^\\top A)^\\top = A^\\top (A^\\top)^\\top = A^\\top A,\n&\\mathbf{u}^\\top (A^\\top A)\\mathbf{u} = \\lVert A \\mathbf{u} \\rVert_2^2 \\geq 0.\n\\end{align}\n$$\nThus, by the spectral theorem $A^\\top A$ has $D$ orthogonal eigenvectors, which\nwe will denote by $\\mathbf{e}_1, \\dots, \\mathbf{e}_D$ and assume that they\nhave been normalized to have unit norm. By positive definiteness the respective\neigenvalues $\\lambda_1, \\dots, \\lambda_D$ (sorted in decreasing order) are\nall non-negative. For $d = 1, \\dots, D$, let\n$\\mathcal{E}_d := \\text{span}(\\mathbf{e}_1, \\dots, \\mathbf{e}_d)$, with\n$\\mathcal{E}^{\\perp}_d := \\text{span}(\\mathbf{e}_{d+1}, \\dots, \\mathbf{e}_D)$\nits orthogonal complement. We now consider the eigenvalue problem\n\n$$\n\\begin{align}\n\\max_{\\mathbf{u} \\in \\mathcal{E}^{\\perp}_d, \\lVert \\mathbf{u} \\rVert_2=1} \\lVert A\\mathbf{u} \\rVert_2. \\tag{A2}\n\\end{align}\n$$\n\nWe have generalized (A1) by adding an orthogonality constraint. Taking as a\nconvention $\\mathcal{E}_0 := \\{\\mathbf{0}\\}$ we then have\n$\\mathcal{E}^{\\perp}_0 = \\mathbb{R}^D$, which means that setting $d = 0$ in\n(A2) recovers the original problem (A1) as a special case. We will prove the\nfollowing result.\n\n<blockquote>\n  <p><strong>Proposition.</strong>\n  Let $A \\in \\mathbb{R}^{N \\times D}$ be a matrix. Let\n  $\\mathbf{e}_1, \\dots, \\mathbf{e}_D$ denote the orthonormal\n  eigenbasis of $A^\\top A$, with respective eigenvalues\n  $\\lambda_1, \\dots, \\lambda_D$ sorted in descending\n  order by magnitude. For $d = 1, \\dots, D$ define\n  $\\mathcal{E}_d := \\text{span}(\\mathbf{e}_1, \\dots, \\mathbf{e}_d)$, with\n  $\\mathcal{E}^{\\perp}_d := \\text{span}(\\mathbf{e}_{d+1}, \\dots, \\mathbf{e}_D)$\n  its orthogonal complement. Then\n  $$\n  \\mathbf{e}_{d+1} = \\text{argmax}_{\\mathbf{u} \\in \\mathcal{E}^{\\perp}_d, \\mathbf{u}=1} \\lVert A\\mathbf{u} \\rVert_2. \\tag{A3}\n  $$\n  with the maximal value  equal to $\\sqrt{\\lambda_{d+1}}$. In particular, we have\n  $$\n  \\mathbf{e}_1 = \\text{argmax}_{\\lVert \\mathbf{u} \\rVert_2=1} \\lVert A\\mathbf{u} \\rVert_2, \\tag{A4}\n  $$\n  with maximal value $\\sqrt{\\lambda_1}$.\n  </p>\n</blockquote>\n\n**Proof.**\nLet $\\mathbf{u} \\in \\mathcal{E}^{\\perp}_d$\nbe an arbitrary vector of unit length. This vector may be represented with\nrespect to the eigenbasis as\n$$\n\\mathbf{u} = \\sum_{r=d+1}^{D} u_r \\mathbf{e}_r, \\qquad u_r := \\langle \\mathbf{u}, \\mathbf{e}_r \\rangle\n$$\nWe will use this representation to show that\n1. $\\lVert A\\mathbf{u} \\rVert_2$ is upper bounded by $\\sqrt{\\lambda_{d+1}}$.\n2. The upper bound is achieved by some $\\mathbf{u} \\in \\mathcal{E}^{\\perp}_d$,\n\nwhich together imply the claimed result. We will actually work with the squared\nnorm instead, which allows us to leverage the inner product. We have\n$$\n\\begin{align}\n\\lVert A\\mathbf{u} \\rVert^2_2\n= \\langle A\\mathbf{u}, A\\mathbf{u} \\rangle\n= \\langle A^\\top A\\mathbf{u}, \\mathbf{u} \\rangle\n&= \\left\\langle A^\\top A \\sum_{r=d+1}^{D} u_r \\mathbf{e}_r,\n\\sum_{r=d+1}^{D} u_r \\mathbf{e}_r \\right\\rangle \\newline\n&= \\left\\langle \\sum_{r=d+1}^{D} u_r (A^\\top A \\mathbf{e}_r),\n\\sum_{r=d+1}^{D} u_r \\mathbf{e}_r \\right\\rangle \\newline\n&= \\left\\langle \\sum_{r=d+1}^{D} u_r \\lambda_r \\mathbf{e}_r,\n\\sum_{r=d+1}^{D} u_r \\mathbf{e}_r \\right\\rangle,\n\\end{align}\n$$\nhaving used the fact the $\\mathbf{e}_r$ are eigenvectors of $A^\\top A$.\nNow we can take advantage of the fact that the $\\mathbf{e}_r$ are orthonormal\nto obtain\n$$\n\\begin{align}\n\\left\\langle \\sum_{r=d+1}^{D} u_r \\lambda_r \\mathbf{e}_r,\n\\sum_{r=d+1}^{D} u_r \\mathbf{e}_r \\right\\rangle\n= \\sum_{r=d+1}^{D} u_r^2 \\lambda_r \\lVert \\mathbf{e}_r \\rVert^2_2\n= \\sum_{r=d+1}^{D} u_r^2 \\lambda_r\n\\leq \\sum_{r=d+1}^{D} u_r^2 \\lambda_{d+1}\n= \\lambda_{d+1} \\lVert \\mathbf{u} \\rVert_2^2\n= \\lambda_{d+1}\n\\end{align}\n$$\nwhere the inequality follows from the fact that the eigenvalues are sorted\nin descending order. This verifies the upper bound\n$\\lVert A\\mathbf{u} \\rVert_2 \\leq \\sqrt{\\lambda_{d+1}}$. To show that the bound\nis achieved, we consider setting $\\mathbf{u} = \\mathbf{e}_{d+1}$. Then,\n\n$$\n\\begin{align}\n\\lVert A\\mathbf{e}_{d+1} \\rVert^2_2\n= \\langle A\\mathbf{e}_{d+1}, A\\mathbf{e}_{d+1} \\rangle\n= \\langle A^\\top A\\mathbf{e}_{d+1}, \\mathbf{e}_{d+1} \\rangle\n= \\langle \\lambda_{d+1} \\mathbf{e}_{d+1}, \\mathbf{e}_{d+1} \\rangle\n= \\lambda_{d+1} \\lVert \\mathbf{e}_{d+1} \\rVert^2_2\n= \\lambda_{d+1},\n\\end{align}\n$$\n\nso we have indeed verified that the equality\n$\\lVert A\\mathbf{u} \\rVert_2 = \\sqrt{\\lambda_{d+1}}$ is achieved for\nsome unit-norm vector $\\mathbf{u} \\in \\mathcal{E}^{\\perp}_{d}$.\nThe claim is thus proved. $\\qquad \\blacksquare$\n\n## Projections\n\n## Truncated SVD and Eigendecomposition\n","srcMarkdownNoYaml":"\n\n# Part 1: Formulating and Solving the PCA Optimization Problem\n\n## Setup and Notation\nSuppose that we have data $\\mathbf{x}_1, \\dots, \\mathbf{x}_N \\in \\mathbb{R}^D$, stacked into the\nrows of a matrix $X \\in \\mathbb{R}^{N \\times D}$. Our task is to find a subspace\nof smaller dimension $R < D$ such that the projection of the data points onto the\nsubspace retains as much information as possible. By restricting our attention to\northonormal bases for the low-dimensional subspace, we reduce the problem to finding\na set of orthonormal basis vectors\n$$\n\\begin{align}\n&\\mathbf{b}_1, \\dots, \\mathbf{b}_R \\in \\mathbb{R}^D,\n&&\\langle \\mathbf{b}_r, \\mathbf{b}_s \\rangle = \\delta_{r,s}.\n\\end{align}\n$$\nDefine $B \\in \\mathbb{R}^{D \\times R}$ to be the matrix with $r^{\\text{th}}$ column\nequal to $\\mathbf{b}_r$. The subspace generated by the basis $B$ is given by\n$$\n\\text{span}(B) := \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R).\n$$\nThroughout this post I will abuse notation by referring to the matrix $B$ when actually\ntalking about the set of vectors $\\{\\mathbf{b}_1, \\dots, \\mathbf{b}_R\\}$. Since there\nis no a priori reason to assume that the data is centered, we should also allow for\nthe subspace to be shifted by some intercept $\\mathbf{w}_0 \\in \\mathbb{R}^D$,\nresulting in the affine space\n$$\n\\mathbf{w}_0 + \\text{span}(B) = \\left\\{\\mathbf{w}_0 +\n\\sum_{r=1}^{R} w_r \\mathbf{b}_r : w_1, \\dots, w_R \\in \\mathbb{R} \\right\\}.\n$$\nLoosely speaking, the task is to find the basis\n$B$, intercept $\\mathbf{w}_0$, and pointwise weights\n$\\mathbf{w}_1, \\dots, \\mathbf{w}_N \\in \\mathbb{R}^R$ such that\n$$\n\\begin{align}\n\\mathbf{x}_n &\\approx \\mathbf{w}_0 + \\sum_{r=1}^{R} (\\mathbf{w}_n)_r \\mathbf{b}_r &&\\forall n=1,\\dots,N \\\\\n&= \\mathbf{w}_0 + B\\mathbf{w}_n.\n\\end{align}\n$$\nTo formalize this notion, PCA measures the error in the above approximation using\nEuclidean distance, averaged over the $N$ data points. To further simplify notation,\nwe stack the $\\mathbf{w}_n$ in the columns of a matrix $W \\in \\mathbb{R}^{R \\times N}$.\nWith all of this notation established, we can state that PCA solves the optimization\nproblem\n$$\n\\text{argmin}_{B, W, \\mathbf{w}_0} \\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - (\\mathbf{w}_0 + B\\mathbf{w}_n) \\rVert_2^2, \\tag{1}\n$$\nwhere the basis $B$ is constrained to be orthonormal.\nAs we will see, this optimization naturally breaks down into two distinct problems\nwhich can be solved sequentially:\n1. Given the basis $B$ and intercept $\\mathbf{w}_0$, find the optimal basis coefficients\n$\\mathbf{w}_n$ corresponding to each data point $\\mathbf{x}_n$.\n2. Find the optimal basis and intercept.\n\nPart of the popularity of PCA stems from the fact that both problems can be solved in\nclosed-form. Let us consider both problems in turn.\n\n## Optimizing the Basis Coefficients\nLet us first consider $\\mathbf{w}_0$ and $B$ to be fixed, meaning that we are fixing\nan affine subspace of dimension $R$. We seek to find the optimal way to represent\nthe data $X$ in this lower-dimensional space. As we will show, the Euclidean objective\nused by PCA implies that this problem reduces to straightforward orthogonal projection.\nFor now, let $\\mathbf{x}^c_n := \\mathbf{x}_n - \\mathbf{w}_0$ denote the centered\ndata points (we will deal with the intercept shortly). We are thus considering\nthe problem\n$$\n\\text{argmin}_{W} \\sum_{n=1}^{N} \\lVert \\mathbf{x}^c_n - B\\mathbf{w}_n \\rVert_2^2 \\tag{2}\n$$\nObserve that $\\mathbf{w}_n$ only appears in the $n^{\\text{th}}$ term of the sum,\nmeaning that we can consider each summand independently,\n$$\n\\text{argmin}_{\\mathbf{w}_n} \\lVert \\mathbf{x}^c_n - B\\mathbf{w}_n \\rVert_2^2.\n$$\nIn words, we seek the linear combination of the basis vectors $B$ that results\nin minimal Euclidean distance from $\\mathbf{x}^c_n$; this is a standard orthogonal\nprojection problem from linear algebra. Since the basis vectors are orthonormal,\nthe optimal projection coefficients are given by\n$$\n\\begin{align}\n&(\\mathbf{w}_n)_r = \\langle \\mathbf{x}_n^c, \\mathbf{b}_r \\rangle,\n&&\\mathbf{w}_n = B^\\top \\mathbf{x}_n^c\n\\end{align}\n$$\nwhich can be written succinctly for all data points by stacking the $\\mathbf{w}_n^\\top$\nas rows in a matrix $W$; i.e.,\n$$\nW := X^c B,\n$$\nwith $X^c$ denoting the centered data matrix with rows set to the\n$(\\mathbf{x}^c_n)^\\top$.\n\n## Optimizing the Basis\nIn the previous section, we saw that for a fixed basis and intercept, optimizing\nthe basis weights reduced to an orthogonal projection problem. In this section\nwe show that with the weights fixed at their optimal values, optimizing the basis\nreduces to solving a sequence of eigenvalue problems.\nTo be clear, we are now considering the problem\n$$\n\\text{argmin}_{B} \\sum_{n=1}^{N} \\lVert \\mathbf{x}^c_n - B\\mathbf{w}^*_n \\rVert_2^2, \\tag{3}\n$$\nwhere the $\\mathbf{w}^*_n$ are now fixed at the optimal values satisfying (2);\ni.e., $\\mathbf{w}^*_n = B^\\top \\mathbf{x}^c_n$. However, in the derivations\nbelow we will just write $\\mathbf{w}_n = \\mathbf{w}^*_n$ to keep the notation\nlighter. Note that we are still treating $\\mathbf{w}_0$ as fixed for the time being.\nWe will make another notational simplification in this section by writing\n$\\mathbf{x}_n = \\mathbf{x}_n^c$. Just keep in mind that\nthroughout this section, $\\mathbf{x}_n$ should be interpreted as $\\mathbf{x}_n - \\mathbf{w}_0$.\n\nThis problem is also referred to as minimizing the *reconstruction error*, since\n$\\lVert \\mathbf{x}_n - \\mathbf{\\hat{x}}_n \\rVert_2 := \\lVert \\mathbf{x}_n - B\\mathbf{w}_n \\rVert_2$\nis the error between the original\ndata point $\\mathbf{x}_n$ and the $D$-dimensional vector $\\mathbf{\\hat{x}}_n$ which\ncan be thought of as an approximation to $\\mathbf{x}_n$ that has been\n*reconstructed* from its lower-dimensional representation $\\mathbf{w}_n$. The key\nhere is to re-write this objective function so that this optimization problem\ntakes the form of an eigenvalue problem, which is something that we already know\nhow to solve (see the appendix, A1).\n\nTo start, we extend the orthonormal set $\\mathbf{b}_1, \\dots, \\mathbf{b}_R$\nto an orthonormal basis $\\mathbf{b}_1, \\dots, \\mathbf{b}_D$ for $\\mathbb{R}^D$.\nNow we can write the original data point $\\mathbf{x}_n$ and its approximation\n$\\mathbf{\\hat{x}}_n$ with respect to this basis as\n$$\n\\begin{align}\n&\\mathbf{x}_n = \\sum_{r=1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r,\n&&\\mathbf{\\hat{x}}_n = \\sum_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r\n\\end{align}\n$$\n\nand hence the residual $\\mathbf{x}_n - \\mathbf{\\hat{x}}_n$ is given by\n$$\n\\begin{align}\n\\mathbf{x}_n - \\mathbf{\\hat{x}}_n &= \\sum_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r.\n\\end{align}\n$$\n\nThus, the objective function in (3) can be written as\n$$\n\\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{\\hat{x}}_n \\rVert_2^2\n= \\sum_{n=1}^{N} \\bigg\\lVert \\sum_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r \\bigg\\rVert_2^2\n= \\sum_{n=1}^{N} \\sum_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2 \\lVert \\mathbf{b}_r \\rVert_2^2\n= \\sum_{n=1}^{N} \\sum_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2,\n$$\n\nwhere the second and third equalities use the facts that the $\\mathbf{b}_r$ are\northogonal and of unit norm, respectively.\n\nWe could continue working with this formulation, but at this point it is convenient to\nre-write the minimization problem we have been working with as an equivalent maximization\nproblem. Note that the above residual calculation is of the form\n$\\mathbf{\\hat{e}}_n = \\mathbf{x}_n - \\mathbf{\\hat{x}}_n$ (and summed over $n$).\nSince $\\mathbf{x}_n$ is fixed, then minimizing the residual (i.e., the\nreconstruction error) is equivalent to maximizing $\\mathbf{\\hat{x}}_n$. More\nrigorously, we have\n\n$$\n\\begin{align}\n\\text{argmin}_B \\sum_{n=1}^{N} \\sum_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\n&= \\text{argmin}_B \\sum_{n=1}^{N}\n\\left(\\sum_{r=1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2 - \\sum\\_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\\right) \\\\\n&= \\text{argmin}_B \\sum_{n=1}^{N}\n\\left(\\lVert \\mathbf{x}_n \\rVert_2^2 - \\sum_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\\right) \\\\\n&= \\text{argmax}_B \\sum_{n=1}^{N} \\sum_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2. \\tag{4}\n\\end{align}\n$$\n\nWe can now re-write the squared inner product to obtain\n$$\n\\begin{align}\n\\sum_{n=1}^{N} \\sum_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\n&= \\sum_{n=1}^{N} \\sum_{r=1}^{R} \\mathbf{b}_r^\\top \\mathbf{x}_n \\mathbf{x}_n^\\top \\mathbf{b}_r\n= \\sum_{r=1}^{R} \\mathbf{b}_r^\\top \\left(\\sum_{n=1}^{N}\\mathbf{x}_n \\mathbf{x}_n^\\top\\right) \\mathbf{b}_r\n= \\sum_{r=1}^{R} \\mathbf{b}_r^\\top (X^\\top X) \\mathbf{b}_r^\\top,\n\\end{align}\n$$\n\nwhere the final step uses [this](https://gregorygundersen.com/blog/2020/07/17/matmul/)\nfact. We have managed to re-write (3) as\n$$\n\\text{argmax}_{B} \\sum_{r=1}^{D} \\mathbf{b}_r^\\top (X^\\top X) \\mathbf{b}_r^\\top, \\tag{5}\n$$\nwhere we recall that this is also subject to the constraint that $B$ is orthogonal.\n\nBefore proceeding, we note that $X^\\top X$ is a positive semi-definite matrix,\nwhose eigenvalues we denote $\\lambda_1, \\dots, \\lambda_D$, sorted in decreasing\norder. Note that the eigenvalues are all non-negative due to the positive\ndefiniteness. Let $\\mathbf{e}_1, \\dots, \\mathbf{e}_D$ denote the respective\neigenvectors, normalized to have unit norm. These vectors are guaranteed to be\northogonal by the Spectral Theorem.\n\nWe now notice in (5) that the objective function has been decomposed into $R$ different\nterms, each of which only depends on a single $\\mathbf{b}_r$. However, these\ndo not constitute $R$ independent optimization problems, as they are all coupled\nthrough the orthogonality constraint. We will thus consider solving them in\na recursive fashion, beginning with the first term,\n$$\n\\text{argmax}_{\\lVert \\mathbf{b}_1 \\rVert_2=1} \\mathbf{b}_1^\\top (X^\\top X) \\mathbf{b}_1^\\top\n= \\text{argmax}_{\\lVert \\mathbf{b}_1 \\rVert_2=1} \\lVert X \\mathbf{b}_1 \\rVert_2^2.\n$$\nThis is an eigenvalue problem! It is precisely of the form (A4) (see appendix)\nand so we apply that result to conclude that the optimal argument is\n$\\mathbf{b}_1 = \\mathbf{e}_1$ with associated optimal value $\\lambda_1$ (note\nthe objective here is the *squared* norm, in contrast to the statement in the appendix). Taking\nthis as the base case, we now proceed inductively. Assume that at the\n$r^{\\text{th}}$ problem in the sequence, the solution is given by\n$(\\mathbf{b}_1, \\dots, \\mathbf{b}_r) = (\\mathbf{e}_1, \\dots, \\mathbf{e}_r)$.\nWe must show the solution to the $(r+1)^{\\text{st}}$ problem is\n$\\mathbf{e}_{r+1}$. Under the inductive hypothesis, this problem is constrained\nso that $\\mathbf{b}_{r+1}$ is orthogonal to each of $\\mathbf{e}_1, \\dots, \\mathbf{e}_r$;\ni.e., we require $\\mathbf{b}_{r+1} \\perp \\text{span}(\\mathbf{e}_1, \\dots, \\mathbf{e}_r)$.\nIf we denote $\\mathcal{E}_{r} := \\text{span}(\\mathbf{e}_1, \\dots, \\mathbf{e}_r)$\nand $\\mathcal{E}^{\\perp}_{r}$ the orthogonal complement of $\\mathcal{E}_{r}$,\nthen a succinct way to write the orthogonality constraint is that $\\mathbf{b}_{r+1} \\in \\mathcal{E}^{\\perp}_r$.\nThe problem can thus be written as\n$$\n\\begin{align}\n\\text{argmax}_{\\mathbf{b}_{r+1} \\in \\mathcal{E}^{\\perp}_{r}, \\lVert \\mathbf{b}_{r+1} \\rVert_2=1} \\lVert X \\mathbf{b}_{r+1} \\rVert_2^2, \\tag{6}\n\\end{align}\n$$\nwhich is another eigenvalue problem, precisely of the form (A3). Using this\nresult from the appendix, we conclude that this is solved by\n$\\mathbf{b}_{r+1} = \\mathbf{e}_{r+1}$, with the maximal objective value $\\lambda_{r+1}$.\n\nThat was a lot, so before moving on let's briefly summarize. First of all, recall\nthat I have been abusing notation by writing $\\mathbf{x}_n$ where I should be writing\n$\\mathbf{x}_n^c = \\mathbf{x}_n - \\mathbf{w}_0$. In summarizing the result here I will make this correction.\nHere we have considered the problem of finding the optimal orthonormal basis $B$,\nfor any fixed $\\mathbf{w}_0 \\in \\mathbb{R}^D$, but with the $\\mathbf{w}_n$ set to\ntheir optimal values satisfying (2); i.e., $\\mathbf{w}_n = B^\\top \\mathbf{x}^c_n$.\nGiven this, we showed that the reconstruction error (5) is minimized by setting\n$B$ equal to the matrix with columns given by the dominant $R$\n(normalized) eigenvectors of $(X^c)^\\top X^c$. We arrived at this solution by showing\nthat the error minimization problem (5) could be viewed as a sequence of $R$\neigenvalue problems.\n\n## Optimizing the Intercept\nThe last ingredient we are missing to solve (1) is the optimal value of $\\mathbf{w}_0$,\nwhich has henceforth been viewed as fixed in the above derivations. At first\nglance, this problem might seem like somewhat of an afterthought,\nbut there are some subtleties that are worth exploring here.\n\nThe problem we are now considering is\n$$\n\\text{argmin}_{\\mathbf{w}_0} \\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{w}_0 - B\\mathbf{w}^*_n \\rVert_2^2, \\tag{7}\n$$\nwith $\\mathbf{w}^*_n$ denoting the optimal weights $\\mathbf{w}^*_n = B^\\top \\mathbf{x}_n$\nderived above (these derivations will go through with any orthonormal basis $B$).\nPlugging in this expression for $\\mathbf{w}^*_n$ gives\n$$\n\\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{w}_0 - B\\mathbf{w}^*_n \\rVert_2^2\n= \\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{w}_0 - BB^\\top \\mathbf{x}_n \\rVert_2^2\n= \\sum_{n=1}^{N} \\lVert (I - BB^\\top)(\\mathbf{x}_n - \\mathbf{w}_0) \\rVert_2^2.\n$$\nComputing the gradient of this expression with respect to $\\mathbf{w}_0$ and setting\nit equal to zero yields the optimality condition\n$$\n\\sum_{n=1}^{N} (I - BB^\\top)(\\mathbf{x}_n - \\mathbf{w}_0) = 0,\n$$\nwhere we have used the fact that $(I - BB^\\top)^2 = (I - BB^\\top)$ (since\n$I - BB^\\top$ is a projection matrix; see appendix). By linearity we then have\n$$\n\\sum_{n=1}^{N} (I - BB^\\top)(\\mathbf{x}_n - \\mathbf{w}_0)\n= (I - BB^\\top) \\sum_{n=1}^{N} (\\mathbf{x}_n - \\mathbf{w}_0)\n= (I - BB^\\top)(N \\bar{\\mathbf{x}} - N\\mathbf{w}_0), \\tag{7}\n$$\nwhere we have defined\n$$\n\\bar{\\mathbf{x}} := \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}_n,\n$$\nthe empirical mean of the data. Since $\\mathbf{w}_0$ is optimal when (8)\nis equal to zero, this leads to the condition\n$$\n(I - BB^\\top)(\\bar{\\mathbf{x}} - \\mathbf{w}_0) = 0,\n$$\nor equivalently,\n$$\n\\bar{\\mathbf{x}} - \\mathbf{w}_0 \\in \\text{Null}(I - BB^\\top).\n$$\nNoting that the null space is non-trivial, since\n$\\text{Null}(I - BB^\\top) = \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R)$\n(again, see the appendix section on projection matrices),\nwe then conclude that there are infinitely many optimal solutions!\nUsing the basis $\\mathbf{b}_1, \\dots, \\mathbf{b}_R$ for the null space, we can\ncharacterize the set of optimal $\\mathbf{w}_0$ as those satisfying\n$$\n\\bar{\\mathbf{x}} - \\mathbf{w}_0 \\in \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R),\n$$\nor, more explicitly, all vectors within the affine space\n$$\n\\mathbf{w}_0 \\in \\bar{\\mathbf{x}} + \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R). \\tag{9}\n$$\nWhile we have an infinity of optimal solutions we could choose from, the obvious\nchoice $\\mathbf{w}_0^* := \\bar{\\mathbf{x}}$ stands out. Indeed, this is the choice\nthat is essentially always made in practice, so much so that many PCA tutorials\nwill begin by assuming that the data points have all been centered by subtracting\noff their empirical mean. I find it more insightful to include the intercept as a variable\nin the PCA optimization problem, and then show that the choice to set it equal\nto $\\bar{\\mathbf{x}}$ is actually justified.\n\nMoreover, it is quite interesting that the mean is not actually the unique optimal\nchoice here. Why is this? The characterization (9) says that we can add any\nvector lying in the span of the orthonormal basis to $\\bar{\\mathbf{x}}$ and still\nmaintain optimality. So the key requirement is that, after shifting the data by\nsubtracting off $\\mathbf{w}_0$, the resulting shifted points must \"lie along\" the\nlower-dimensional subspace $\\text{span}(B)$.\nSince, $\\text{span}(B)$ defines a hyperplane,\nthe data must lie somewhere along this plane; from the perspective of the\noptimization problem, it doesn't matter whether it lies around the origin or somewhere\nvery far away, so long as it is clustered around this plane.\nA picture is worth a thousand words here, and I will\ntry to add one once I have time.\n\nFinally, note that the specific choice of $\\bar{\\mathbf{x}}$ has various other\npractical benefits. It leads to projections that are clustered around the origin,\nthus keeping numbers relatively small. It also leads to a nice statistical\ninterpretation of the eigenvalue problems discussed in the previous subsection;\ne.g. the basis vector $\\mathbf{b}_1$ can be viewed as the direction along which\nthe empirical variance of the projected data is maximized. This maximum variance\nperspective is discussed in more detail below.\n\n\n# Part 2: Interpreting PCA\n\n## Minimum Error or Maximum Variance?\nWhile the derivations in the preceding section are somewhat lengthy, recall\nthat this was all in the pursuit of solving the optimization problem (1). In words,\nwe derived the best $R$-dimensional affine subspace to represent the data $X$,\nwhere \"best\" is defined as minimizing the average Euclidean error between the data points\nand their projections onto the subspace. We showed that this error minimization problem\ncould be re-written as a sequence of $R$ maximization problems of the form (6).\nWe now show that these maximization problems have a very nice statistical\ninterpretation.\n\n### Sample covariance of the $\\mathbf{x}_n$\nWe first recall that the empirical covariance matrix\nof the data points $\\mathbf{x}_1, \\dots, \\mathbf{x}_N$ is defined to be\n$$\n\\hat{C} := \\frac{1}{N-1} \\sum_{n=1}^{N} (\\mathbf{x}_n - \\bar{\\mathbf{x}}) (\\mathbf{x}_n - \\bar{\\mathbf{x}})^\\top,\n$$\nwhich can be re-written as\n$$\n\\hat{C} = \\frac{1}{N-1} \\sum_{n=1}^{N} \\mathbf{x}^c_n (\\mathbf{x}^c_n)^\\top = \\frac{1}{N-1} X^c (X^c)^\\top, \\tag{10}\n$$\nwhere the superscript *c* indicates that the observations have been centered by\nsubtracting off their empirical mean.\n\nRecall that solving the maximization\nproblems (6) revealed that the optimal basis vectors are given by the dominant\neigenvectors of the matrix $X^c (X^c)^\\top$, which is the (unscaled) covariance (10)!\nThe scaling factor does not affect the optimal basis vectors, it simply scales\nthe objective function. Specifically,\n$$\n\\begin{align}\n\\text{argmax}_{\\mathbf{b}_{r+1} \\in \\mathcal{E}^{\\perp}_{r}, \\lVert \\mathbf{b}_{r+1} \\rVert_2=1}\n\\left(\\mathbf{b}_{r+1}^\\top X^c (X^c)^\\top \\mathbf{b}_{r+1}\\right)\n= \\text{argmax}_{\\mathbf{b}_{r+1} \\in \\mathcal{E}^{\\perp}_{r}, \\lVert \\mathbf{b}_{r+1} \\rVert_2=1}\n\\left(\\mathbf{b}_{r+1}^\\top \\hat{C} \\mathbf{b}_{r+1}\\right). \\tag{11}\n\\end{align}\n$$\nWe haven't changed anything from (6) here, other than noting that a re-scaling\nof the objective function allows us involve $\\hat{C}$ in the expression.\n\n### Sample covariance of the $\\mathbf{w}_n$\nGiven that the sample covariance matrix of the data\n$\\mathbf{x}_1, \\dots, \\mathbf{x}_N \\in \\mathbb{R}^D$ is given by $\\hat{C}$,\nit is natural to also consider the empirical covariance of\n$\\mathbf{w}_1, \\dots, \\mathbf{w}_N \\in \\mathbb{R}^R$. We begin by computing the\nsample mean\n$$\n\\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{w}_n\n= \\frac{1}{N} \\sum_{n=1}^{N} B\\mathbf{x}^c_n\n= B \\left[\\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}^c_n \\right] = 0,\n$$\nusing the fact that the empirical mean of the centered data points is $0$.\nRecalling that the row vectors $\\mathbf{w}_n^\\top$ are stored in the rows of\nthe matrix $W$, it follows that the empirical covariance matrix of\n$\\mathbf{w}_1, \\dots, \\mathbf{w}_N$ is given by\n$$\n\\hat{C}_w := \\frac{1}{N-1} W^\\top W, \\tag{12}\n$$\nwhich follows from the calculation (10) with $W$ in place of $X^c$. Since\n$W = XB$, we have\n$$\n\\hat{C}_w = \\frac{1}{N-1} (XB)^\\top (XB) = B^\\top \\hat{C} B,\n$$\nwhich allows us to write the covariance of the $\\mathbf{w}_n$ as a function\nof the covariance of the $\\mathbf{x}_n$. We can say even more since\nwe know that $B$ is given by $V_R$, the truncated set of eigenvectors obtained\nfrom the eigendecomposition $X^\\top X = V \\Lambda V^\\top$.\nWe thus have\n$$\n\\hat{C}_w\n= \\frac{1}{N-1} B^\\top (X^\\top X) B\n= \\frac{1}{N-1} V_R^\\top (X^\\top X) V_R\n= \\frac{1}{N-1} V_R^\\top V_R \\Lambda_R\n= \\frac{1}{N-1} \\Lambda_R,  \\tag{13}\n$$\nwhere $(X^\\top X) V_R = V_R \\Lambda_R$ follows from the fact that the columns\nof $V_R$ store the first $R$ eigenvectors of $X^\\top X$. The conclusion here\nis that $\\hat{C}_w$ is diagonal, with variances equal to the eigenvalues\nof $\\hat{C}$. In other words, PCA computes a change-of-basis that diagonalizes\nthe empirical covariance of the data.\n\n### PCA as Variance Maximization\nWe now return to the goal of providing a statistical interpretation of the\nobjective function $\\mathbf{b}_{r}^\\top \\hat{C} \\mathbf{b}_r$ in (11).\nGiven the derivation of $\\hat{C}_w$ in (13), we see the empirical variance\nof $(\\mathbf{w}_1)_r, \\dots, (\\mathbf{w}_N)_r$ (i.e., the values in the\n$r^{\\text{th}}$ column of $W$) is equal to\n$$\n[\\hat{C}_w]_{rr} = \\mathbf{b}_r^\\top \\hat{C} \\mathbf{b}_r,\n$$\nwhich is precisely the objective being maximized. To interpret this quantity\nmore clearly, we consider the projection of $\\mathbf{x}_n^c$ onto the\nspan of $\\mathbf{b}_r$,\n$$\n\\text{proj}_{\\mathbf{b}_r} \\mathbf{x}^c_n\n:= \\langle \\mathbf{x}^c_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r\n= (\\mathbf{w}_n)_r \\mathbf{b}_r,\n$$\nwhich implies that $(\\mathbf{w}_n)_r$ is the magnitude of the projection. Therefore,\nwe conclude that $\\mathbf{b}_r^\\top \\hat{C} \\mathbf{b}_r$ is\nthe sample variance of the magnitude of the projections onto the subspace\n$\\text{span}(\\mathbf{b}_r)$; loosely speaking, the variance of the projection\nalong the $r^{\\text{th}}$ basis vector. Combining all of these equivalent\nexpressions yields the chain of equalities,\n$$\n\\text{Tr}(\\hat{C}_w)\n= \\frac{1}{N-1} \\sum_{r=1}^{R} \\lambda_r\n= \\sum_{n=1}^{N} \\sum_{r=1}^{R} W_{nr}^2.\n= \\sum_{r=1}^{R} \\mathbf{b}_r^\\top \\hat{C} \\mathbf{b}_r \\tag{15}\n$$\nThe trace $\\text{Tr}(\\hat{C}_w)$ thus represents the total variance of the projection,\nsummed over all of the basis vectors. The total variance is equivalently given\nby the sum of the eigenvalues of $\\hat{C}$ or by the squared Frobenius norm\n$\\lVert W \\rVert_F^2$.\n\nThe final term in (15) provides an alternative\ninterpretation of the objective function in (5); namely, that PCA seeks\nthe basis $B$ which results in the maximal projected total variance. The\nresulting sequence of constrained problems, as in (11), are interpreted similarly.\nIn particular,\n$$\n\\text{argmax}_{\\mathbf{b}_{r+1} \\in \\mathcal{E}^{\\perp}_{r}, \\lVert \\mathbf{b}_{r+1} \\rVert_2=1}\n\\left(\\mathbf{b}_{r+1}^\\top \\hat{C} \\mathbf{b}_{r+1}\\right)\n$$\ncan be viewed as seeking the direction along which the variance of the\nprojections is maximized, subject to the constraint that the direction be\northogonal to the previous $r$ directions. The optimal solution is the direction\ncorresponding to the $(r+1)^{\\text{st}}$ eigenvector of the empirical\ncovariance $\\hat{C}$, and the resulting maximal variance in this direction is\ngiven by the associated eigenvalue.\n\n## The Singular Value Decomposition\n\n## A Matrix Approximation Problem: The Eckart-Young Theorem\nIgnoring the intercept (or assuming that the $\\mathbf{x}_n$ have already been\ncentered), we can re-write the reconstruction error as\n$$\n\\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - B\\mathbf{w}_n \\rVert_2^2\n= \\lVert X - WB^\\top \\rVert_F^2,\n$$\nwhere $\\lVert \\cdot \\rVert_F$ denotes the\n[Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm).\nThe PCA optimization problem can then be written as the\nmatrix approximation problem\n$$\n\\text{argmin}_{B, W} \\lVert X - WB^\\top \\rVert_F^2, \\tag{10}\n$$\nwhere $B$ is constrained to be an orthogonal matrix. We can equivalently write this\nas\n$$\n\\begin{align}\n&\\text{argmin}_{\\hat{X} \\in \\mathcal{M}} \\lVert X - \\hat{X} \\rVert_F^2,\n&&\\mathcal{M} = \\{\\hat{X} \\in \\mathbb{R}^{N \\times D} : \\hat{X}=WB^\\top, B \\in \\mathbb{R}^{D \\times R} \\text{ is orthogonal}\\}, \\tag{11}\n\\end{align}\n$$\nwhich makes it even more clear that PCA can generically be viewed as the problem\nof approximating the data matrix $X$ with another matrix $\\hat{X}$ that is\nconstrained to lie in a subset $\\mathcal{M}$ of all $N \\times D$ matrices.\nWe can phrase this even more succinctly by noticing that $\\mathcal{M}$ is\nprecisely the set of all $N \\times D$ matrices with rank at most $R$.\nIndeed, if $\\hat{X} \\in \\mathcal{M}$ then\n$\\text{rank}(\\hat{X}) = \\text{rank}(W B^\\top) \\leq R$ since $W$ has $R$ columns\nand thus the rank of this matrix product cannot exceed $R$. Conversely, if\nwe let $\\hat{X}$ be an arbirary $N \\times D$ matrix of rank $r \\leq R$ then $\\hat{X}$\ncan be expanded\nusing the [compact SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition#Reduced_SVDs)\nas\n$$\n\\hat{X} = U\\tilde{\\Sigma}V^\\top = (U\\tilde{\\Sigma})V^\\top,\n$$\nwhere $U \\in \\mathbb{R}^{N \\times r}$, $V \\in \\mathbb{R}^{D \\times r}$ are\northogonal and $\\tilde{\\Sigma} \\in \\mathbb{R}^{r \\times r}$ is diagonal.\nBy setting $W := U\\tilde{\\Sigma}$ and $B := V$ we have almost written things\nin the form required by (11), but (11) restricts $B$ to be orthogonal with\nexactly $R$ columns. Thus, we can simply extend $V$ to have $R$ orthonormal columns\nby appending columns $B = [V|\\mathbf{v}_{r+1}, \\dots, \\mathbf{v}_{R}]$ (this is\njustified by Gram-Schmidt) and then set $W = [U\\tilde{\\Sigma}|\\boldsymbol{0}]$.\nThus, $\\hat{X} = (U\\tilde{\\Sigma})V^\\top = WB^\\top$ with $B$ now of the required\nform.\n\nWe have thus verified that (11) can equivalently be written as the low-rank\nmatrix approximation problem\n$$\n\\begin{align}\n&\\text{argmin}_{\\hat{X} \\in \\mathcal{M}} \\lVert X - \\hat{X} \\rVert_F^2,\n&&\\mathcal{M} = \\{\\hat{X} \\in \\mathbb{R}^{N \\times D} : \\text{rank}(\\hat{X}) \\leq R \\}. \\tag{12}\n\\end{align}\n$$\n\nThis is precisely the problem\nconsidered by the celebrated [Eckart-Young theorem](https://en.wikipedia.org/wiki/Low-rank_approximation).\nThe theorem concludes that the optimal solution to (12) is given by the truncated\nSVD $X = U_R \\Sigma_R V_R^\\top$, which is precisely the PCA solution computed\nusing SVD as discussed in the previous section.\n\n## Regression with an optimal basis\n\n## Statistical/Probabalistic Perspectives\nThere are various ways we might attach a statistical or probabilistic perspective\nto PCA - we briefly discuss a few of them here. Throughout this section we\nwill take\n$$\n\\text{argmin}_{W,B} \\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - B\\mathbf{w}_n \\rVert_2^2  \\tag{13}\n$$\nas the jumping off point for a probabilistic generalization; that is, we are\nimplicitly assuming the data is centered so that we can ignore the intercept.\nNote that, as always, $B$ is constrained to be orthogonal.\n\n### A Special Case of the Karhunen-Loeve Expansion\nWe start by noting that the objective function in (13) looks like a sample\naverage of the quantities $\\lVert \\mathbf{x}_n - B\\mathbf{w}_n \\rVert_2^2$.\nIt is therefore natural to consider some underlying true expectation that this\nsample average is approximating. To this end, let us view $\\mathbf{x} = \\mathbf{x}(\\omega)$\nand $\\mathbf{w} = \\mathbf{w}(\\omega)$ as random vectors, defined over some probability space\n$(\\Omega, \\mathcal{A}, \\mathbb{P})$. We can then consider\n$$\n\\text{argmin}_{\\mathbf{w},B} \\mathbb{E} \\lVert \\mathbf{x} - B\\mathbf{w} \\rVert_2^2\n= \\text{argmin}_{\\mathbf{w},B} \\int_{\\Omega} \\left\\lVert \\mathbf{x}(\\omega) - \\sum_{r=1}^{R} \\mathbf{w}_r(\\omega) \\mathbf{b}_r \\right\\rVert_2^2 \\mathbb{P}(d\\omega), \\tag{14}\n$$\nwhich can be viewed as the population analog of the sample approximation in\n(13). We note that the second expression above shows that the low-rank\napproximation of the random vector $\\mathbf{x}$ takes the form of a linear\ncombination of (non-random) basis vectors, with random coefficients.\n\nWith $B$ fixed, the optimization in $\\mathbf{w}$ is just as easy as before.\nIndeed, for any fixed $\\omega \\in \\Omega$,\n$$\n\\text{argmin}_{\\mathbf{w}(\\omega)} \\lVert \\mathbf{x}(\\omega) - B\\mathbf{w}(\\omega) \\rVert_2^2 = B^\\top \\mathbf{x}(\\omega),\n$$\nusing the same exact orthogonal projection reasoning as before. We have thus\noptimized for $\\mathbf{w}(\\omega)$ on an $\\omega$-by-$\\omega$ basis. The same\nresult thus holds in expectation:\n$$\n\\text{argmin}_{\\mathbf{w}} \\mathbb{E} \\lVert \\mathbf{x} - B\\mathbf{w} \\rVert_2^2 = B^\\top \\mathbf{x}.\n$$\nIt is important to note that we have found the optimal *random vector* $\\mathbf{w}$,\nwhich was shown to be a linear transformation of the random vector $\\mathbf{x}$.\n\nOptimizing for $B$ with the optimal $\\mathbf{w}$ fixed also follows similarly\nfrom previous derivations. Using some of the results already derived above\n(see the section on optimizing the basis), we have\n\n$$\n\\begin{align}\n\\text{argmin}_{B} \\mathbb{E}\\lVert \\mathbf{x} - B\\mathbf{w} \\rVert_2^2\n&= \\text{argmin}_{B} \\mathbb{E} \\sum_{r=R+1}^{D} \\langle \\mathbf{x}, \\mathbf{b}_r \\rangle^2 \\newline\n&= \\text{argmin}_{B} \\mathbb{E} \\left[\\lVert \\mathbf{x} \\rVert_2^2 - \\sum_{r=1}^{R} \\langle \\mathbf{x}, \\mathbf{b}_r \\rangle^2 \\right] \\newline\n&= \\text{argmax}_{B} \\mathbb{E} \\sum_{r=1}^{R} \\langle \\mathbf{x}, \\mathbf{b}_r \\rangle^2 \\newline\n&= \\text{argmax}_{B} \\mathbb{E} \\sum_{r=1}^{R} \\mathbf{b}_r^\\top \\mathbf{x}\\mathbf{x}^\\top \\mathbf{b}_r \\newline\n&= \\text{argmax}_{B} \\sum_{r=1}^{R} \\mathbf{b}_r^\\top \\mathbb{E}\\left[\\mathbf{x}\\mathbf{x}^\\top\\right] \\mathbf{b}_r \\newline\n&= \\text{argmax}_{B} \\sum_{r=1}^{R} \\mathbf{b}_r^\\top C \\mathbf{b}_r,\n\\end{align}\n$$\n\nwhere $C := \\text{Cov}[\\mathbf{x}]$. Here we have used the centering assumption\n$\\mathbb{E}[\\mathbf{x}] = 0$, as well as the implicit assumption that the\nrandom vector $\\mathbf{x}$ has a well-defined covariance matrix. At this point,\nthe derivations go through exactly as before, with $C$\nreplacing the empirical covariance $\\hat{C}$; that is, the optimal basis $B$ is\nobtained by extracting the first $R$ columns of $V$, where $C = V\\Lambda V^\\top$.\nUnsurprisingly, the results that held for the sample covariance hold\nanalogously in this setting. For example, since $\\mathbf{x}$ has zero mean then\n$$\n\\mathbb{E}[\\mathbf{w}_r] = \\mathbb{E}[\\mathbf{v}_r^\\top \\mathbf{x}] = 0.\n$$\nMoreover,\n$$\n\\text{Cov}[\\mathbf{w}_r, \\mathbf{w}_s]\n= \\text{Cov}[\\mathbf{v}_r^\\top \\mathbf{x}, \\mathbf{v}_s^\\top \\mathbf{x}]\n= \\mathbf{v}_r^\\top C \\mathbf{v}_s\n= \\mathbf{v}_r^\\top V \\Lambda V^\\top \\mathbf{v}_s\n= \\lambda_r 1[r = s],\n$$\nwhich says that the weights $\\mathbf{w}_r$ are uncorrelated, with variance equal\nto their respective eigenvalues. It is common, therefore, to normalize these\nrandom variables to have unit variance\n$$\n\\mathbf{\\tilde{w}}_r := \\frac{1}{\\sqrt{\\lambda_r}} \\mathbf{w}_r,\n$$\nwhich means the optimal rank $R$ approximation to the random vector $\\mathbf{x}$\nmay be expressed as\n$$\n\\mathbf{\\hat{x}}(\\omega) := \\sum_{r=1}^{R} \\sqrt{\\lambda_r} \\mathbf{\\tilde{w}}_r(\\omega) \\mathbf{v}_r,\n$$\nwhere we have included the $\\omega$ argument to emphasize which quantities are\nrandom here. The following result summarizes our findings, extending to the\ncase with non-zero mean.\n<blockquote>\n  <p><strong>Proposition.</strong>\n  Let $\\mathbf{x}$ be a square-integrable $D$-dimensional random vector defined on\n  $(\\Omega, \\mathcal{A}, \\mathbb{P})$. Among all such random vectors constrained\n  to take values in an $R$-dimensional subspace of $\\mathbb{R}^D$, the random\n  vector\n  $$\n  \\mathbf{\\hat{x}}(\\omega) = \\mathbb{E}[\\mathbf{x}] + \\sum_{r=1}^{R} \\sqrt{\\lambda_r} \\mathbf{\\tilde{w}}_r(\\omega) \\mathbf{v}_r, \\tag{15}\n  $$\n  provides an optimal approximation to $\\mathbf{x}$, in the expected Euclidean\n  distance sense (14). Moreover, the random weights $\\mathbf{\\tilde{w}}_r$ are\n  uncorrelated, have zero mean and unit variance.\n  </p>\n</blockquote>\n\nNote that random vectors can conceptually be thought of as stochastic processes\nwith finite-dimensional index sets. A similar decomposition to (15) can\nbe constructed for more general stochastic processes with potentially\nuncountable index sets, with the modification that the sum in (15) will require\na countably infinite number of terms. This result is generally known as the\n**Karhunen-Loeve expansion**. Thus, from this perspective PCA can be thought of\nas the special finite-dimensional case of the Karhunen-Loeve expansion.\n\n#### Gaussian Random Vectors\nThe above derivations did not make any assumptions regarding the distribution\nof $\\mathbf{x}$, just that its covariance exists. Consequently, the main result\n(15) does not give the distribution of the weights $\\mathbf{\\tilde{w}}_r$,\nonly that they are uncorrelated, have zero mean, and unit variance. If we add\nthe distributional assumption $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{m}, C)$ then\nwe are able to say more. Indeed, recalling that the (unscaled) weights are\ngiven by projections $\\mathbf{w}_r = \\langle \\mathbf{x} - \\mathbf{m}, \\mathbf{v}_r \\rangle$,\nthen we find that\n$$\n\\mathbf{w} = V^\\top (\\mathbf{x} - \\mathbf{m}) \\sim \\mathcal{N}(0, V^\\top C V).\n$$\nThe key point here are that the $\\mathbf{w}_r$ are *jointly* Gaussian, and hence\ntheir uncorrelatedness implies that they are in fact independent. The weights\ninherit Gaussianity from $\\mathbf{x}$. Thus, under this stronger assumption\nwe are able to characterize the distribution of the weights exactly.\n\n\n### Maximum Likelihood Estimation with Gaussian Noise\n### Probabilistic PCA\nhttps://stats.stackexchange.com/questions/190308/why-does-probabilistic-pca-use-gaussian-prior-over-latent-variables\n\n# Part 3: Using PCA\n1. Decorrelating\n2. Dimensionality reduction\n\n# Part 4: Computing PCA\n1. Eigendecomposition\n2. SVD\n\n# Part 5: Application and Code\n\n\n# Appendix\n\n## Eigenvalue Problems\nIn this section, I briefly discuss the spectral norm and eigenvalue problems in\nfinite-dimensional vector spaces, which I utilize above when optimizing the basis\n$B$ in the PCA derivation. Consider a matrix $A \\in \\mathbb{R}^{N \\times D}$,\nwhich represents a linear transformation from $\\mathbb{R}^{D}$ to $\\mathbb{R}^N$.\nWe define the\n**spectral norm** of $A$ as the largest factor by which the map\n$A$ can \"stretch\" a vector $\\mathbf{u} \\in \\mathbb{R}^{D}$,\n$$\n\\lVert A \\rVert_2 := \\max_{\\lVert \\mathbf{u} \\rVert_2 \\neq \\boldsymbol{0}}\n\\frac{\\lVert A\\mathbf{u} \\rVert_2}{\\lVert \\mathbf{u} \\rVert_2}.\n$$\nUsing the linearity of $A$, one can show that we need only consider vectors of\nunit length; that is,\n$$\n\\lVert A \\rVert_2 = \\max_{\\lVert \\mathbf{u}_2 \\rVert=1} \\lVert A\\mathbf{u} \\rVert_2. \\tag{A1}\n$$\nOptimization problems of this type are called **eigenvalue problems** for reasons that will\nshortly become clear. For the purposes of the PCA derivation, we will require\nconsideration of a slightly more general eigenvalue problem. To define this problem, first\nnote that the matrix $A^\\top A$ is symmetric, positive semi-definite since\n$$\n\\begin{align}\n&(A^\\top A)^\\top = A^\\top (A^\\top)^\\top = A^\\top A,\n&\\mathbf{u}^\\top (A^\\top A)\\mathbf{u} = \\lVert A \\mathbf{u} \\rVert_2^2 \\geq 0.\n\\end{align}\n$$\nThus, by the spectral theorem $A^\\top A$ has $D$ orthogonal eigenvectors, which\nwe will denote by $\\mathbf{e}_1, \\dots, \\mathbf{e}_D$ and assume that they\nhave been normalized to have unit norm. By positive definiteness the respective\neigenvalues $\\lambda_1, \\dots, \\lambda_D$ (sorted in decreasing order) are\nall non-negative. For $d = 1, \\dots, D$, let\n$\\mathcal{E}_d := \\text{span}(\\mathbf{e}_1, \\dots, \\mathbf{e}_d)$, with\n$\\mathcal{E}^{\\perp}_d := \\text{span}(\\mathbf{e}_{d+1}, \\dots, \\mathbf{e}_D)$\nits orthogonal complement. We now consider the eigenvalue problem\n\n$$\n\\begin{align}\n\\max_{\\mathbf{u} \\in \\mathcal{E}^{\\perp}_d, \\lVert \\mathbf{u} \\rVert_2=1} \\lVert A\\mathbf{u} \\rVert_2. \\tag{A2}\n\\end{align}\n$$\n\nWe have generalized (A1) by adding an orthogonality constraint. Taking as a\nconvention $\\mathcal{E}_0 := \\{\\mathbf{0}\\}$ we then have\n$\\mathcal{E}^{\\perp}_0 = \\mathbb{R}^D$, which means that setting $d = 0$ in\n(A2) recovers the original problem (A1) as a special case. We will prove the\nfollowing result.\n\n<blockquote>\n  <p><strong>Proposition.</strong>\n  Let $A \\in \\mathbb{R}^{N \\times D}$ be a matrix. Let\n  $\\mathbf{e}_1, \\dots, \\mathbf{e}_D$ denote the orthonormal\n  eigenbasis of $A^\\top A$, with respective eigenvalues\n  $\\lambda_1, \\dots, \\lambda_D$ sorted in descending\n  order by magnitude. For $d = 1, \\dots, D$ define\n  $\\mathcal{E}_d := \\text{span}(\\mathbf{e}_1, \\dots, \\mathbf{e}_d)$, with\n  $\\mathcal{E}^{\\perp}_d := \\text{span}(\\mathbf{e}_{d+1}, \\dots, \\mathbf{e}_D)$\n  its orthogonal complement. Then\n  $$\n  \\mathbf{e}_{d+1} = \\text{argmax}_{\\mathbf{u} \\in \\mathcal{E}^{\\perp}_d, \\mathbf{u}=1} \\lVert A\\mathbf{u} \\rVert_2. \\tag{A3}\n  $$\n  with the maximal value  equal to $\\sqrt{\\lambda_{d+1}}$. In particular, we have\n  $$\n  \\mathbf{e}_1 = \\text{argmax}_{\\lVert \\mathbf{u} \\rVert_2=1} \\lVert A\\mathbf{u} \\rVert_2, \\tag{A4}\n  $$\n  with maximal value $\\sqrt{\\lambda_1}$.\n  </p>\n</blockquote>\n\n**Proof.**\nLet $\\mathbf{u} \\in \\mathcal{E}^{\\perp}_d$\nbe an arbitrary vector of unit length. This vector may be represented with\nrespect to the eigenbasis as\n$$\n\\mathbf{u} = \\sum_{r=d+1}^{D} u_r \\mathbf{e}_r, \\qquad u_r := \\langle \\mathbf{u}, \\mathbf{e}_r \\rangle\n$$\nWe will use this representation to show that\n1. $\\lVert A\\mathbf{u} \\rVert_2$ is upper bounded by $\\sqrt{\\lambda_{d+1}}$.\n2. The upper bound is achieved by some $\\mathbf{u} \\in \\mathcal{E}^{\\perp}_d$,\n\nwhich together imply the claimed result. We will actually work with the squared\nnorm instead, which allows us to leverage the inner product. We have\n$$\n\\begin{align}\n\\lVert A\\mathbf{u} \\rVert^2_2\n= \\langle A\\mathbf{u}, A\\mathbf{u} \\rangle\n= \\langle A^\\top A\\mathbf{u}, \\mathbf{u} \\rangle\n&= \\left\\langle A^\\top A \\sum_{r=d+1}^{D} u_r \\mathbf{e}_r,\n\\sum_{r=d+1}^{D} u_r \\mathbf{e}_r \\right\\rangle \\newline\n&= \\left\\langle \\sum_{r=d+1}^{D} u_r (A^\\top A \\mathbf{e}_r),\n\\sum_{r=d+1}^{D} u_r \\mathbf{e}_r \\right\\rangle \\newline\n&= \\left\\langle \\sum_{r=d+1}^{D} u_r \\lambda_r \\mathbf{e}_r,\n\\sum_{r=d+1}^{D} u_r \\mathbf{e}_r \\right\\rangle,\n\\end{align}\n$$\nhaving used the fact the $\\mathbf{e}_r$ are eigenvectors of $A^\\top A$.\nNow we can take advantage of the fact that the $\\mathbf{e}_r$ are orthonormal\nto obtain\n$$\n\\begin{align}\n\\left\\langle \\sum_{r=d+1}^{D} u_r \\lambda_r \\mathbf{e}_r,\n\\sum_{r=d+1}^{D} u_r \\mathbf{e}_r \\right\\rangle\n= \\sum_{r=d+1}^{D} u_r^2 \\lambda_r \\lVert \\mathbf{e}_r \\rVert^2_2\n= \\sum_{r=d+1}^{D} u_r^2 \\lambda_r\n\\leq \\sum_{r=d+1}^{D} u_r^2 \\lambda_{d+1}\n= \\lambda_{d+1} \\lVert \\mathbf{u} \\rVert_2^2\n= \\lambda_{d+1}\n\\end{align}\n$$\nwhere the inequality follows from the fact that the eigenvalues are sorted\nin descending order. This verifies the upper bound\n$\\lVert A\\mathbf{u} \\rVert_2 \\leq \\sqrt{\\lambda_{d+1}}$. To show that the bound\nis achieved, we consider setting $\\mathbf{u} = \\mathbf{e}_{d+1}$. Then,\n\n$$\n\\begin{align}\n\\lVert A\\mathbf{e}_{d+1} \\rVert^2_2\n= \\langle A\\mathbf{e}_{d+1}, A\\mathbf{e}_{d+1} \\rangle\n= \\langle A^\\top A\\mathbf{e}_{d+1}, \\mathbf{e}_{d+1} \\rangle\n= \\langle \\lambda_{d+1} \\mathbf{e}_{d+1}, \\mathbf{e}_{d+1} \\rangle\n= \\lambda_{d+1} \\lVert \\mathbf{e}_{d+1} \\rVert^2_2\n= \\lambda_{d+1},\n\\end{align}\n$$\n\nso we have indeed verified that the equality\n$\\lVert A\\mathbf{u} \\rVert_2 = \\sqrt{\\lambda_{d+1}}$ is achieved for\nsome unit-norm vector $\\mathbf{u} \\in \\mathcal{E}^{\\perp}_{d}$.\nThe claim is thus proved. $\\qquad \\blacksquare$\n\n## Projections\n\n## Truncated SVD and Eigendecomposition\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"pca.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","theme":["cosmo","brand"],"title-block-banner":true,"title":"Principal Components Analysis","description":"A deep dive into PCA.","date":"2023-12-15","categories":["Statistics","Linear-Algebra"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}