{"title":"Introduction to Gaussian Process Priors and Hyperparameter Estimation","markdown":{"yaml":{"title":"Introduction to Gaussian Process Priors and Hyperparameter Estimation","subtitle":"A deep dive into hyperparameter specifications for GP mean and covariance functions, including both frequentist and Bayesian methods for hyperparameter estimation.","date":"2024-01-11","categories":["Statistics","Gaussian-Process","kernel-methods"]},"headingText":"Background","containsRefs":false,"markdown":"\n\nGaussian processes (GP) are widely utilized across various fields, each with\ntheir own preferences, terminology, and conventions. Some notable domains that\nmake significant use of GPs include\n- Spatial statistics (kriging)\n- Design and analysis of computer experiments (emulator/surrogate modeling)\n- Bayesian optimization\n- Machine learning\n\nEven if you're a GP expert in one of these domains,\nthese differences can make navigating the\nGP literature in other domains a bit tricky. The goal of this post is to\nsummarize common approaches for specifying GP distributions, and emphasize\nconventions and assumptions that tend to differ across fields. By\n\"specifying GP distributions\", what I am really talking about here is\nparameterizing the mean and covariance functions that define the GP. While\nGPs are non-parametric models in a certain sense, specifying and\nlearning the *hyperparameters* making up the mean and covariance functions\nis a crucial step to successful GP applications. I will discuss popular\nparameterizations for these functions, and different algorithms for learning\nthese parameter values from data. In the spirit of drawing connections across\ndifferent domains, I will try my best to borrow terminology from different fields,\nand will draw attention to synonymous terms by using boldface.\n\n\n### Gaussian Processes\nGaussian processes (GPs) define a probability distribution over a space of\nfunctions in such a way that they can be viewed as a generalization of\nGaussian random vectors. Just as Gaussian vectors are defined by their\nmean vector and covariance matrix, GPs are defined by a mean and covariance\n*function*. We will interchangeably refer to the latter as either the\n**covariance function** or **kernel**.\n\nWe will consider GPs defined over a space of functions of the form\n$f: \\mathcal{X} \\to \\mathbb{R}$, where $\\mathcal{X} \\subseteq \\mathbb{R}^d$.\nWe will refer to elements $x \\in \\mathcal{X}$ as **inputs** or\n**locations** and the images $f(x) \\in \\mathbb{R}$ as **outputs** or\n**responses**. If the use of the word \"locations\" seems odd, note that in\nspatial statistical settings, the inputs $x$ are often geographic coordinates.\nWe will denote the mean and covariance function defining the\nGP by $\\mu: \\mathcal{X} \\to \\mathbb{R}$\nand $k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$, respectively. The mean\nfunction is essentially unrestricted, but the covariance function $k(\\cdot, \\cdot)$\nmust be a valid positive definite kernel. If $f(\\cdot)$ is a GP with\nmean function $\\mu(\\cdot)$ and kernel $k(\\cdot, \\cdot)$ we will denote this by\n$$\n\\begin{align}\nf \\sim \\mathcal{GP}(\\mu, k). \\tag{1}\n\\end{align}\n$$\n\nThe defining property of GPs is that their finite-dimensional distributions\nare Gaussian; that is, for an arbitrary finite set of $n$ inputs\n$X := \\{x_1, \\dots, x_N\\} \\subset \\mathcal{X}$,\nthe vector $f(X) \\in \\mathbb{R}^n$ is distributed as\n$$\n\\begin{align}\nf(X) \\sim \\mathcal{N}(\\mu(X), k(X, X)). \\tag{2}\n\\end{align}\n$$\nWe are vectorizing notation here so that $[f(X)]_i := f(x_i)$,\n$[\\mu(X)]_i := \\mu(x_i)$, and $[k(X, X)]_{i,j} := k(x_i, x_j)$. When the\ntwo input sets to the kernel are equal, we lighten notation by writing\n$k(X) := k(X, X)$.\nNow suppose we have two sets of inputs\n$X$ and $\\tilde{X}$, containing $n$ and\n$m$ inputs, respectively. The defining property (2) then implies\n\n$$\n\\begin{align}\n\\begin{bmatrix} f(\\tilde{X}) \\newline f(X) \\end{bmatrix}\n&\\sim \\mathcal{N}\\left(\n  \\begin{bmatrix} \\mu(\\tilde{X}) \\newline \\mu(X) \\end{bmatrix},\n  \\begin{bmatrix}\n  k(\\tilde{X}) & k(\\tilde{X}, X) \\newline\n  k(X, \\tilde{X}) & k(X)\n  \\end{bmatrix}\n\\right). \\tag{3}\n\\end{align}\n$$\n\nThe Gaussian joint distribution (3) implies that the conditional distributions\nare also Gaussian. In particular, the distribution of $f(\\tilde{X})|f(X)$ can be\nobtained by applying the well-known Gaussian conditioning identities:\n\n$$\n\\begin{align}\nf(\\tilde{X})|f(X) &\\sim \\mathcal{N}(\\hat{\\mu}(\\tilde{X}), \\hat{k}(\\tilde{X})), \\tag{4} \\newline\n\\hat{\\mu}(\\tilde{X}) &:= \\mu(\\tilde{X}) + k(\\tilde{X}, X)k(X)^{-1} [f(X) - \\mu(X)] \\newline\n\\hat{k}(\\tilde{X}) &:= k(\\tilde{X}) - k(\\tilde{X}, X)k(X)^{-1} k(X, \\tilde{X}).\n\\end{align}\n$$\nThe fact that the result (4) holds for arbitrary finite sets of inputs $\\tilde{X}$\nimplies that the conditional $f | f(X)$ is also a GP, with mean and covariance\nfunctions $\\hat{\\mu}(\\cdot)$ and $\\hat{k}(\\cdot, \\cdot)$ defined by (4).\nOn a terminology note, the $n \\times n$ matrix $k(X)$ is often called the\n**kernel matrix**. This is the matrix containing the kernel evaluations at the\nset of $n$ *observed* locations.\n\n### Regression with GPs\nOne common application of GPs is their use as a flexible nonlinear regression\nmodel. Let's consider the basic regression setup with observed data pairs\n$(x_1, y_1), \\dots, (x_n, y_n)$. We assume that the $y_i$ are noisy observations\nof some underlying latent function output $f(x_i)$. The GP regression model arises\nby placing a GP prior distribution on the latent function $f$. We thus consider\nthe regression model\n$$\n\\begin{align}\ny(x) &= f(x) + \\epsilon(x) \\tag{5} \\newline\nf &\\sim \\mathcal{GP}(\\mu, k) \\newline\n\\epsilon(x) &\\overset{iid}{\\sim} \\mathcal{N}(0, \\sigma^2),\n\\end{align}\n$$\nwhere we have assumed a simple additive Gaussian noise model. This assumption is\nquite common in the GP regression setting due to the fact that it results in\nclosed-form conditional distributions, similar to (4). We will assume the\nerror model (5) throughout this post, but note that there are many other possibilities\nif one is willing to abandon closed-form posterior inference.\n\nThe solution of the regression problem is given by the distribution of\n$f(\\cdot)|y(X)$ or $y(\\cdot)|y(X)$, where $y(X)$ is the $n$-dimensional vector\nof observed responses. The first distribution is the posterior on the latent\nfunction $f$, while the second incorporates the observation noise as well.\nBoth distributions can be derived in the same way, so we focus on the second.\nLetting $\\tilde{X}$ denote a set of $m$ inputs at which we would like to predict the\nresponse, consider the joint distribution\n$$\n\\begin{align}\n\\begin{bmatrix} y(\\tilde{X}) \\newline y(X) \\end{bmatrix}\n&\\sim \\mathcal{N}\\left(\n  \\begin{bmatrix} \\mu(\\tilde{X}) \\newline \\mu(X) \\end{bmatrix},\n  \\begin{bmatrix}\n  k(\\tilde{X}) + \\sigma^2 I_m & k(\\tilde{X}, X) \\newline\n  k(X, \\tilde{X}) & k(X) + \\sigma^2 I_n\n  \\end{bmatrix}\n\\right). \\tag{6}\n\\end{align}\n$$\nThis is quite similar to (3), but now takes into account the noise term\n$\\epsilon$. This does not affect the mean vector since $\\epsilon$ is mean-zero;\nnor does it affect the off-diagonal elements of the covariance matrix since\n$\\epsilon$ and $f$ were assumed independent. Applying the Gaussian conditioning\nidentities (4) yields the posterior distribution\n$$\n\\begin{align}\ny(\\tilde{X})|y(X) &\\sim \\mathcal{N}(\\hat{\\mu}(\\tilde{X}), \\hat{k}(\\tilde{X})), \\tag{7} \\newline\n\\hat{\\mu}(\\tilde{X}) &:= \\mu(\\tilde{X}) + k(\\tilde{X}, X)[k(X) + \\sigma^2 I_n]^{-1} [f(X) - \\mu(X)] \\newline\n\\hat{k}(\\tilde{X}) &:= \\sigma^2 I_m + k(\\tilde{X}) - k(\\tilde{X}, X)[k(X) + \\sigma^2 I_n]^{-1} k(X, \\tilde{X}).\n\\end{align}\n$$\nWe will refer to (7) as the GP **posterior**, **predictive**, or generically\n**conditional**, distribution.\nWe observe that these equations are identical to (4), modulo the appearance\nof $\\sigma^2$ in the predictive mean and covariance equations. The distribution\n$f(\\tilde{X})|y(X)$ is identical to (7), except that the $\\sigma^2 I_m$ is removed in\nthe predictive covariance. Again, this reflects the subtle distinction between\ndoing inference on the latent function $f$ versus on the observation process $y$.\n\n### Noise, Nuggets, and Jitters\nObserve that this whole regression procedure is only slightly different from the\nnoiseless GP setting explored in the previous section (thanks to the Gaussian\nlikelihood assumption). Indeed, the conditional distribution of\n$f(\\tilde{X})|y(X)$ is derived from $f(\\tilde{X})|f(X)$ by simply replacing\n$k(X)$ with $k(X) + \\sigma^2 I_n$ (obtaining the distribution\n$y(\\tilde{X})|y(X)$ requires the one additional step of adding $\\sigma^2 I_m$\nto the predictive covariance). In other words, we have simply applied standard\nGP conditioning using the modified kernel matrix\n$$\n\\begin{align}\nC(X) := k(X) + \\sigma^2 I_n. \\tag{8}\n\\end{align}\n$$\nWe thus might reasonably wonder if the model (5) admits an alternative\nequivalent representation by defining a GP directly on the observation process $y$.\nDefining such a model would require\ndefining a kernel $c: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ that\nis consistent with (8). This route is fraught with difficulties and subtleties, which I will\ndo my best to describe clearly here. At first glance, it seems like the right\nchoice is\n$$\n\\begin{align}\nc(x, x^\\prime) := k(x, x^\\prime) + \\sigma^2 \\delta(x, x^\\prime), \\tag{9}\n\\end{align}\n$$\nwhere $\\delta(x, x^\\prime) := 1[x = x^\\prime]$ is sometimes called the\n**stationary white noise kernel**. Why isn't this quite right? Notice in (9)\nthat $\\sigma^2$ is added whenever the inputs $x = x^\\prime$ are equal. However,\nsuppose we observe multiple independent realizations of the process at the\nsame inputs $X$. In the regression model (9) the errors $\\epsilon(x)$ are\nindependent across these realizations, *even at the same locations*. However,\nthis will not hold true in the model under (9), since $\\delta(x, x^\\prime)$ only\nsees the values of the inputs, and has no sense of distinction across realizations.\nWe might try to fix this by writing something like\n$$\n\\begin{align}\nc(x_i, x_j) := k(x_i, x_j) + \\sigma^2 \\delta_{ij}, \\tag{10}\n\\end{align}\n$$\nwhere the Delta function now depends on the labels $i, j$ instead of the values\nof the inputs. In the spatial statistics literature,\nit is not uncommon to see a covariance function defined like (10), but this is\nbasically a notational hack. A kernel is a function of two inputs from\n$\\mathcal{X}$ - we can't have it also depending on some side information like\nthe labels $i, j$. At the end of the day, (9) and (10) are attempts to\nincorporate some concept of **white noise** inside the kernel itself, rather\nthan via a hierarchical model like (5). I would just stick with the hierarchical\nmodel, which is easily rigorously defined and much more intuitive.\n\nNonetheless, one should not be surprised if expressions like (10) pop up,\nespecially in the spatial statistics literature. Spatial statisticians refer\nto the noise term $\\epsilon(x)$ as the **nugget**, and $\\sigma^2$ the\n**nugget variance** (sometimes these terms are conflated). In this context,\ninstead of representing observation noise, $\\sigma^2$ is often thought of\nas representing some unresolved small-scale randomness in the spatial field\nitself. If you imagine sampling a field to determine the concentration of some\nmineral across space, then you would hope that repeated measurements (taken around\nthe same time) would yield the same values. Naturally, they may not, and the\nintroduction of the nugget is one way to account for this.\n\nWhile this discussion may seem needlessly abstract, we recall that the\neffect of incorporating the noise term (however you want to interpret it) is to\nsimply replace the kernel matrix $k(X)$ with the new matrix $c(X) = k(X) + \\sigma^2 I_n$.\nConfusingly, there is one more reason (having nothing to do with observation error\nor nuggets) that people use a matrix of the form $c(X)$ in place of $k(X)$:\nnumerical stability. Indeed, even though $k(X)$ is theoretically positive definite,\nin practice its numerical instantiation may fail to have this property. A simple\napproach to deal with this is to add a small, fixed constant $\\sigma^2$ to the\ndiagonal of the kernel matrix. In this context, $\\sigma^2$ is often called the\n**jitter**. While computationally its effect is the same as the nugget, note that\nits introduction is motivated very differently. The jitter is not stemming from\nsome sort of random white noise; it is purely a computational hack to improve\nthe conditioning of the kernel matrix. Check out\n[this](https://discourse.mc-stan.org/t/adding-gaussian-process-covariance-functions/237/67)\nthread for some entertaining debates on the use of the nugget and jitter concepts.\n\n### Parameterized Means and Kernels\nEverything we have discussed this far assumes fixed mean and covariance\nfunctions. In practice, suitable choices for these quantities are not typically\nknown. Thus, the usual approach is to specify some parametric families\n$\\mu = \\mu_{\\psi}$ and $k = k_{\\phi}$ and learn their parameters from data.\nThe parameters $\\psi$ and $\\phi$ are often referred to as **hyperparameters**,\nsince they are not the primary parameters of interest in the GP regression model.\nRecalling from (5) that the GP acts as a prior distribution on the latent\nfunction, we see that $\\psi$ and $\\phi$ control the specification of this\nprior distribution. In addition to $\\psi$ and $\\phi$, the parameter\n$\\sigma^2$ is also typically not known. I will not wade back into the previous\nsection's debate in arguing whether this should be classified as\na \"hyperparameter\" or not. In any case, let's let\n$\\theta := \\{\\psi, \\phi, \\sigma^2 \\}$ denote the full set of\n(hyper)parameters that must be learned from data.\n\n#### Mean Functions\nThe machine learning community commonly uses the simplest possible form for the\nmean function: $\\mu(x) \\equiv 0$. This zero-mean assumption is less restrictive\nthan it seems, since GPs mainly derive their expressivity from the kernel.\nA slight generalization is to allow a constant, non-zero mean\n$\\mu(x) \\equiv \\beta_0$, where $\\beta_0 \\in \\mathbb{R}$.\nHowever, constant (including zero-mean) GP priors can have some undesirable properties;\ne.g., in the context of extrapolation. Sometimes one wants more flexibility, and\nin these cases it is quite common to consider some sort of linear regression\nmodel\n$$\n\\begin{align}\n\\mu(x) = h(x)^\\top \\beta, \\tag{11}\n\\end{align}\n$$\nwhere $h: \\mathcal{X} \\to \\mathbb{R}^p$ is some feature map and $\\beta \\in \\mathbb{R}^p$\nthe associated coefficient vector. For example, $h(x) = [1, x^\\top]^\\top$\nwould yield a standard linear model, and\n$h(x) = [1, x_1, \\dots, x_d, x_1^2, \\dots, x_d^2]^\\top$ would allow for a quadratic\ntrend.\n\n#### Kernels\nThe positive definite restriction makes defining valid covariance functions\nmuch more difficult than defining mean functions. Thus, one typically falls back\non one of a few popular choices of known parametric kernel families (though\nnote that kernels can be combined in various ways to give a large variety of\noptions). While the goal of this post is not to explore specific kernels, in order to have\na concrete example in mind consider the following parameterization:\n  $$\n\\begin{align}\nk(x, \\tilde{x}) = \\alpha^2 \\sum_{j=1}^{d} \\left(-\\frac{\\lvert x^{j} - \\tilde{x}^j \\rvert}{\\ell^j}\\right)^2.\n\\tag{12}\n\\end{align}\n$$\nNote that I'm using superscripts to index vector entries here.\nThis kernel goes by many names, including **exponentiated quadratic**,\n**squared exponential**, **Gaussian**, **radial basis function**, and\n**automatic relevance determination**. The parameter $\\alpha^2$ is sometimes called\nthe **marginal variance**, or just the **scale parameter**. The parameters\n$\\ell^1, \\dots, \\ell^d$ are often called **lengthscale**, **smoothness**, or\n**range** parameters, since they control the smoothness of the GP realizations\nalong each coordinate direction. Other popular kernels (e.g., MatÃ©rn) have\nanalogous parameters controlling similar features. Note that in this example\nwe have $\\phi = \\{\\alpha^2, \\ell^1, \\dots, \\ell^d \\}$. Also note that people\nchoose to parameterize the Gaussian kernel in many different ways; for example,\nit's not uncommon to see a $1/2$ factor included inside the exponential to make\nthe kernel align with the typical parameterization of the Gaussian probability\ndensity function. Knowing which parameterization you're working with is important\nfor interpreting the hyperparameters, specifying bounds, defining priors, etc.\n\nIt is quite common in the\nspatial statistics (and sometimes the computer experiments) literature to see\nkernels written like $\\alpha^2 k(\\cdot, \\cdot)$; in these cases $k(\\cdot, \\cdot)$\ntypically represents a *correlation* function, which becomes the covariance function\nafter multiplying by the marginal variance $\\alpha^2$. There is an advantage in\ndecomposing the kernel this way when it comes to estimating the hyperparameters,\nwhich we will discuss shortly.\n\n### The GP (Marginal) Likelihood Function\nLet's first recall the GP regression model (5)\n$$\n\\begin{align}\ny(x) &= f(x) + \\epsilon(x) \\newline\nf &\\sim \\mathcal{GP}(\\mu_{\\psi}, k_{\\phi}) \\newline\n\\epsilon &\\overset{iid}{\\sim} \\mathcal{N}(0, \\sigma^2),\n\\end{align}\n$$\nwhere we have now explicitly added the dependence on $\\psi$ and $\\phi$.\nThis model is defined for any $x \\in \\mathcal{X}$. However, when estimating\nhyperparameters, we will naturally be restricting the model to $X$, the finite\nset of locations at which we actually have observations. Restricting to\n$X$ reduces the above model to the standard (finite-dimensional)\nBayesian regression model\n$$\n\\begin{align}\ny(X)|f(X), \\theta &\\sim \\mathcal{N}(f(X), \\sigma^2 I_n) \\tag{13} \\newline\nf(X)|\\theta &\\sim \\mathcal{N}(\\mu_{\\psi}(X), k_{\\phi}(X)).\n\\end{align}\n$$\nWe could consider completing the Bayesian specification by defining a prior\non $\\theta$, but we'll hold off on this for now.\nNotice that the model (13) defines a joint distribution over\n$[y(X), f(X)] | \\theta$, with $y(X)|f(X), \\theta$ representing the\nlikelihood of the observations at the observed input locations $X$. At present\neverything is conditional on a fixed $\\theta$.\nNow, if we marginalize the likelihood $y(X)|f(X), \\theta$ with\nrespect to $f(X)$ then we obtain the distribution $y(X) | \\theta$. This is often\ncalled the **marginal likelihood**, due to the fact that $f$ was marginalized\nout. In particular, the distribution $y(X) | \\theta$ has implicitly marginalized\nthe function values $f(\\tilde{x})$ at all location $\\tilde{x}$ other than $X$.\nThis same logic and terminology applies in the noiseless setting with $\\sigma^2 = 0$,\nin which case the marginal likelihood is given by $f(X) | \\theta$. In the noiseless\nsetting we are marginalizing both over the unobserved function values and\nthe noise $\\epsilon$.\nThanks to all the Gaussian assumptions here, the marginal likelihood\nis available in closed-form. One could approach the derivation using (13) as\nthe starting point, but it's much easier to consider the model written out using\nrandom variables,\n$$\n\\begin{align}\ny(X) &= f(X) + \\epsilon(X).\n\\end{align}\n$$\nSince $f(X)$ and $\\epsilon(X)$ are independent Gaussians, then their sum is also\nGaussian with mean and covariance given by\n$$\n\\begin{align}\n\\mathbb{E}[y(X)|\\theta]\n&= \\mathbb{E}[f(X)|\\theta] + \\mathbb{E}[\\epsilon(X)|\\theta] = \\mu_{\\psi}(X) \\newline\n\\text{Cov}[y(X)|\\theta]\n&= \\text{Cov}[f(X)|\\theta] + \\text{Cov}[\\epsilon(X)|\\theta]\n= k_{\\phi}(X) + \\sigma^2 I_n.\n\\end{align}\n$$\nWe have thus found that\n$$\n\\begin{align}\ny(X)|\\theta \\sim \\mathcal{N}\\left(\\mu_{\\psi}(X), C_{\\phi, \\sigma^2}(X)\\right), \\tag{14}\n\\end{align}\n$$\nrecalling the definition $C_{\\phi, \\sigma^2}(X) := k_{\\phi}(X) + \\sigma^2 I_n$.\nWe will let $\\mathcal{L}(\\theta)$ denote the log density of this Gaussian\ndistribution; i.e. the log **marginal likelihood**:\n$$\n\\begin{align}\n\\mathcal{L}(\\theta)\n&:= -\\frac{1}{2} \\log \\text{det}\\left(2\\pi C_{\\phi, \\sigma^2}(X) \\right) -\n\\frac{1}{2} (y(X) - \\mu_{\\psi}(X))^\\top C_{\\phi, \\sigma^2}(X)^{-1} (y(X) - \\mu_{\\psi}(X)) \\tag{15}\n\\end{align}\n$$\nThe function $\\mathcal{L}(\\theta)$ plays a central role in the typical approach\nto hyperparameter optimization, as we will explore below. Also note that\nthe above derivations also apply to the noiseless setting\n(i.e., $y(X) = f(X)$) by setting $\\sigma^2 = 0$. In this case, the marginal\nlikelihood is simply the GP distribution restricted to the inputs $X$.\n\nI have henceforth been a bit verbose with the notation in (15) to make very explicit\nthe dependence on the inputs $X$ and the hyperparameters. To lighten notation a\nbit, we define $y_n := y(X)$, $\\mu_{\\psi} := \\mu_{\\psi}(X)$, and\n$C_{\\phi, \\sigma^2} := C_{\\phi, \\sigma^2}(X)$, allowing us to rewrite (15) as\n$$\n\\begin{align}\n\\mathcal{L}(\\theta)\n&:= -\\frac{1}{2} \\log \\text{det}\\left(2\\pi C_{\\phi, \\sigma^2} \\right) -\n\\frac{1}{2} (y_n - \\mu_{\\psi})^\\top C_{\\phi, \\sigma^2}^{-1} (y_n - \\mu_{\\psi}). \\tag{16}\n\\end{align}\n$$\nWe have simply suppressed the explicit dependence on $X$ in the notation.\n\n# Hyperparameter Optimization\nWe now begin to turn out attention to methods for learning the values of the\nhyperparameters from data. This section starts with the most popular approach:\noptimizing the marginal likelihood.\n\n## Maximum Marginal Likelihood, or Empirical Bayes\nRecall that (16) gives the expression for the log marginal likelihood $\\mathcal{L}(\\theta)$, which is just the log density of $y(X)|\\theta$ viewed as a function of $\\theta$.\nA natural approach is to set the hyperparameters $\\theta$ to their values\nthat maximize $\\mathcal{L}(\\theta)$:\n$$\n\\begin{align}\n\\hat{\\theta} := \\text{argmax} \\ \\mathcal{L}(\\theta). \\tag{17}\n\\end{align}\n$$\nAt first glance, the Gaussian form of $\\mathcal{L}(\\theta)$ might look quite\nfriendly to closed-form optimization.\nAfter all, maximum likelihood estimates of the mean and covariance of Gaussian\nvectors are indeed available in closed-form. However, upon closer inspection notice\nthat the covariance is not being directly optimized; we are optimizing $\\phi$, and\nthe covariance $C_{\\phi, \\sigma^2}$ is a *nonlinear* function of this\nparameter. Thus, in general some sort of iterative numerical scheme is\nis used for the optimization. Typically, gradient-based approaches are preferred,\nmeaning we must be able to calculate quantities like\n$\\frac{\\partial}{\\partial \\phi} C_{\\phi, \\sigma^2}$.\nThe exact gradient calculations will thus depend on the choice of kernel; specifics\non kernels and optimization schemes are not the focus of this post. We will instead\nfocus on the high level ideas here. The general approach to GP regression\nthat we have outlined so far can be summarized as:\n1. Solve the optimization problem (17) and fix the hyperparameters at their\noptimized values $\\hat{\\theta}$. The hyperparameters will be fixed from\nthis point onward.\n2. Use the GP predictive equations (7) to perform inference at a set of locations\nof interest $\\tilde{X}$.\n\nOne might object to the fact that we are estimating the hyperparameters from\ndata, and then neglecting the uncertainty in $\\hat{\\theta}$ during the\nprediction step. It is true that this uncertainty is being ignored, but it is\nalso very computationally convenient to do so.\nWe will discuss alternatives later\non, but this simple approach is probably the most commonly used\nin practice today. One way to think about this strategy is in an\n**empirical Bayes** context; that is, we can view this approach as an approximation\nto a fully Bayesian hierarchical model, which would involve equipping the\nhyperparameters with their own priors. Instead of marginalizing the hyperparameters,\nwe instead fix them at their most likely values with respect to the\nobserved data. We are using the data to \"fine tune\" the GP prior distribution.\nIn the literature you will see this general hyperparameter optimization strategy\nreferred to as either **empirical Bayes**, **maximum marginal likelihood**, or\neven just **maximum likelihood**.\n\n## Special Case Closed-Form Solutions: Mean Function\nAs mentioned above, in general the maximization of $\\mathcal{L}(\\theta)$ requires\nnumerical methods. However, in certain cases elements of $\\theta$ can be optimized\nin closed-form, meaning that numerical optimization may only be required for\na subset of the hyperparameters. We start by considering closed form optimizers\nfor the parameters defining the mean functions.\n\n### Constant Mean: Plug-In MLE\nWith the choice of constant mean $\\mu_{\\psi}(x) \\equiv \\beta_0$ the log marginal\nlikelihood becomes\n\n$$\n\\begin{align}\n\\mathcal{L}(\\theta)\n&:= -\\frac{1}{2} \\log \\text{det}\\left(2\\pi C_{\\phi, \\sigma^2} \\right) -\n\\frac{1}{2} (y_n - \\beta_0 1_n)^\\top C_{\\phi, \\sigma^2}(X)^{-1} (y_n - \\beta_0 1_n),\n\\end{align}\n$$\n\nwith $1_n \\in \\mathbb{R}^n$ denoting a vector of ones. We now consider optimizing\n$\\mathcal{L}(\\theta)$ as a function of $\\beta_0$ only. The partial derivative\nwith respect to the constant mean equals\n$$\n\\begin{align}\n\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\beta_0}\n&= y_n^\\top C_{\\phi, \\sigma^2}^{-1}1_n - \\beta_0 1_n^\\top C_{\\phi, \\sigma^2}^{-1} 1_n.    \\tag{18}\n\\end{align}\n$$\nSetting (18) equal to zero and solving for $\\beta_0$ gives the optimum\n$$\n\\begin{align}\n\\hat{\\beta}_0(\\phi, \\sigma^2) = \\frac{y_n^\\top C_{\\phi, \\sigma^2}^{-1} 1_n}{1_n^\\top C_{\\phi, \\sigma^2}^{-1} 1_n}. \\tag{19}\n\\end{align}\n$$\nNotice that $\\hat{\\beta}_0(\\phi, \\sigma^2)$ depends on the values of the other hyperparameters\n$\\phi$ and $\\sigma^2$. Therefore, while this does not give us the outright value\nfor the mean, we can plug $\\hat{\\beta}_0(\\phi, \\sigma^2)$ in place of $\\beta_0$ in the marginal\nlikelihood. This yields the **profile likelihood** (aka the **concentrated likelihood**),\nwhich is no longer a function of $\\beta_0$ and hence the dimensionality of the subsequent numerical optimization problem has been reduced.\n\n### Linear Model Coefficients: Plug-In MLE\nLet's try to do the same thing with the mean function $\\mu_{\\psi}(x) = h(x)^\\top \\beta$.\nThe constant mean function is actually just a special case of this more general\nsetting, but its common enough that it warranted its own section.\nIf we denote by $H \\in \\mathbb{R}^{n \\times p}$ the feature matrix with rows equal\nto $h(x_i)^\\top$, $i = 1, \\dots, n$ then the marginal likelihood becomes\n$$\n\\begin{align}\n\\mathcal{L}(\\theta)\n&:= -\\frac{1}{2} \\log \\text{det}\\left(2\\pi C_{\\phi, \\sigma^2} \\right) -\n\\frac{1}{2} (y_n - H\\beta)^\\top C_{\\phi, \\sigma^2}^{-1} (y_n - H\\beta), \\tag{20}\n\\end{align}\n$$\n\nwith gradient\n$$\n\\begin{align}\n\\nabla_{\\beta} \\mathcal{L}(\\theta)\n&= H^\\top C_{\\phi, \\sigma^2}^{-1}y_n - (H^\\top C_{\\phi, \\sigma^2}^{-1} H)\\beta. \\tag{21}\n\\end{align}\n$$\nSetting the gradient equal to zero and solving for $\\beta$ yields the optimality\ncondition\n$$\n\\begin{align}\n\\left(H^\\top C_{\\phi, \\sigma^2}^{-1} H\\right)\\hat{\\beta} &= H^\\top C_{\\phi, \\sigma^2}^{-1}y_n. \\tag{22}\n\\end{align}\n$$\nA unique solution for $\\hat{\\beta}$ thus exists when $H^\\top C_{\\phi, \\sigma^2}^{-1} H$\nis invertible. When does this happen? First note that this matrix is positive\nsemidefinite, since\n$$\n\\begin{align}\n\\beta^\\top \\left(H^\\top C_{\\phi, \\sigma^2}^{-1} H\\right) \\beta\n&= \\beta^\\top (H^\\top [LL^\\top]^{-1} H) \\beta\n= \\lVert L^{-1} H\\beta \\rVert_2^2 \\geq 0,\n\\end{align}\n$$\nwhere we have used the fact that $C_{\\phi, \\sigma^2}$ is positive definite and\nhence admits a decomposition $LL^\\top$. The matrix $H^\\top C_{\\phi, \\sigma^2}^{-1} H$\nis thus positive definite when $L^{-1}H$ has linearly independent columns; i.e., when\nit is full rank. We already know that $L^{-1}$ is full rank. If we assume that\n$H$ is also full rank and $n \\geq p$ then we can conclude that $L^{-1}H$ is\nfull rank; see [this](https://math.stackexchange.com/questions/272049/rank-of-matrix-ab-when-a-and-b-have-full-rank) post for a quick proof. Thus, under these assumptions we conclude\nthat $H^\\top C_{\\phi, \\sigma^2}^{-1} H$ is invertible and so\n$$\n\\begin{align}\n\\hat{\\beta}(\\phi, \\sigma^2) &= \\left(H^\\top C_{\\phi, \\sigma^2}^{-1} H\\right)^{-1} H^\\top C_{\\phi, \\sigma^2}^{-1}y_n.\n\\tag{23}\n\\end{align}\n$$\nNotice that (23) is simply a [generalized least squares](https://en.wikipedia.org/wiki/Generalized_least_squares) estimator. As with the constant mean, we can plug\n$\\hat{\\beta}(\\phi, \\sigma^2)$ into the marginal likelihood to concentrate out\nthe parameter $\\beta$. The resulting concentrated likelihood can then be numerically\noptimized as a function of the remaining hyperparameters.\n\n### Linear Model Coefficients: Closed-Form Marginalization\nThe above section showed that, conditional on fixed kernel hyperparameters,\nthe coefficients of a linear mean function can be optimized in closed form.\nWe now show a similar result: if the mean coefficients are assigned a Gaussian\nprior then, conditional on fixed kernel hyperparameters, the coefficients can\nbe marginalized in closed form. To this end, we consider the same linear mean\nfunction as above, but now equip the coefficients with a Gaussian prior:\n$$\n\\begin{align}\n\\mu_{\\psi}(x) &= h(x)^\\top \\beta, &&\\beta \\sim \\mathcal{N}(b, B).\n\\end{align}\n$$\nRestricted to the model inputs $X$, the model is thus\n$$\n\\begin{align}\ny_n|\\beta &\\sim \\mathcal{N}\\left(H\\beta, C_{\\phi, \\sigma^2} \\right) \\newline\n\\beta &\\sim \\mathcal{N}(b, B).\n\\end{align}\n$$\nOur goal here is derive the marginal distribution of $y_n$. We could resort to\ncomputing the required integral by hand, but an easier approach is to notice\nthat under the above model $[y_n, \\beta]$ is joint Gaussian distributed.\nTherefore, the marginal distribution of $y_n$ must also be Gaussian. It thus\nremains to identify the mean and covariance of this distribution. We obtain\n$$\n\\begin{align}\n\\mathbb{E}[y_n]\n&= \\mathbb{E}\\mathbb{E}[y_n|\\beta] = \\mathbb{E}[H\\beta] = Hb \\newline\n\\text{Cov}[y_n]\n&= \\mathbb{E}[y_n y_n^\\top] - \\mathbb{E}[y_n]\\mathbb{E}[y_n]^\\top \\newline\n&= \\mathbb{E} \\mathbb{E}\\left[y_n y_n^\\top | \\beta\\right] - (Hb)(Hb)^\\top \\newline\n&= \\mathbb{E}\\left[\\text{Cov}[y_n|\\beta] + \\mathbb{E}[y_n|\\beta] \\mathbb{E}[y_n|\\beta]^\\top \\right] - Hbb^\\top H^\\top \\newline\n&= \\mathbb{E}\\left[C_{\\phi, \\sigma^2} + (H\\beta)(H\\beta)^\\top \\right] - Hbb^\\top H^\\top \\newline\n&= C_{\\phi, \\sigma^2} + H\\mathbb{E}\\left[\\beta \\beta^\\top \\right]H^\\top - Hbb^\\top H^\\top \\newline\n&= C_{\\phi, \\sigma^2} + H\\left[B + bb^\\top \\right]H^\\top - Hbb^\\top H^\\top \\newline\n&= C_{\\phi, \\sigma^2} + HBH^\\top,\n\\end{align}\n$$\nwhere we have used the law of total expectation and the various equivalent\ndefinitions for the covariance matrix. To summarize, we have found that the\nabove hierarchical model implies the marginal distribution\n$$\n\\begin{align}\ny_n &\\sim \\mathcal{N}\\left(Hb, C_{\\phi, \\sigma^2} + HBH^\\top \\right).\n\\end{align}\n$$\nSince this holds for any set of inputs, we obtain the analogous result for the\nGP prior:\n$$\n\\begin{align}\ny(x) &= f(x) + \\epsilon(x) \\newline\nf &\\sim \\mathcal{GP}\\left(\\mu^\\prime, k^\\prime \\right) \\newline\n\\epsilon(x) &\\overset{iid}{\\sim} \\mathcal{N}(0, \\sigma^2),\n\\end{align}\n$$\nwhere\n$$\n\\begin{align}\n\\mu^\\prime(x) &= h(x)^\\top b \\newline\nk^\\prime(x_1, x_2) &= k(x_1, x_2) + h(x_1)^\\top B h(x_2).\n\\end{align}\n$$\nAfter marginalizing, we again end up with a mean function that is linear in the\nbasis functions $h(\\cdot)$. The basis function coefficients are now given by\nthe prior mean $b$. The mean $b$ is something that we can prescribe, or we could\nagain entertain an empirical Bayes approach to set its value. Note that we have\ndescended another step in the hierarchical ladder. The kernel that appears from\nthe marginalization is now a sum of two kernels: the original kernel $k$ and\nthe kernel $h(x_1)^\\top B h(x_2)$. The latter can be viewed as a linear kernel\nin the transformed inputs $h(x_1)$, $h(x_2)$ and weighted by the positive\ndefinite matrix $B$. It serves to account for the uncertainty in the coefficients\nof the mean function.\n\n## Special Case Closed-Form Solutions: Marginal Variance\nWe now consider a closed-form plug-in estimate for the marginal variance\n$\\alpha^2$, as mentioned in (12). The takeaway from this section will be that\na closed-form estimate is only available when the covariance matrix appearing\nin the marginal likelihood (16) is of the form\n$$\n\\begin{align}\nC_{\\phi} &= \\alpha^2 C. \\tag{24}\n\\end{align}\n$$\nThis holds for any kernel of the form $\\alpha^2 k(\\cdot, \\cdot)$ provided\nthat $\\sigma^2 = 0$. For example, the exponentiated quadratic kernel in\n(12) satisfies this requirement.\nWith this assumption, the marginal likelihood is given by\n$$\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log\\left(2\\pi \\alpha^2 \\right) - \\frac{1}{2}\\log\\text{det}(C) -\n\\frac{1}{2\\alpha^2} (y_n - \\mu_{\\psi})^\\top C^{-1} (y_n - \\mu_{\\psi}). \\tag{25}\n\\end{align}\n$$\nThe analytical derivations given below go through for a log marginal likelihood\nof this form. However, this doesn't work for the common setting with an observation\nvariance $\\sigma^2 > 0$, since in this case the covariance assumes\nthe form\n$$\n\\begin{align}\nC &= \\left(\\alpha^2 k(X) + \\sigma^2 I_n \\right).\n\\end{align}\n$$\nThis can be addressed via the simple reparameterization\n$$\n\\begin{align}\n\\tilde{\\alpha}^2 C &:= \\tilde{\\alpha}^2\\left(k(X) + \\tilde{\\sigma}^2 I_n \\right).\n\\end{align}\n$$\nThis gives the required form of the covariance, and maintains the same number\nof parameters as before. The one downside is that we lose the straightforward\ninterpretation of the noise variance; the observation noise is now given by\nthe product $\\tilde{\\alpha}^2 \\tilde{\\sigma}^2$ instead of being encoded in\nthe single parameter $\\sigma^2$. This\nreparameterization is utilized in the R package [hetGP](https://cran.r-project.org/package=hetGP).\n\n\n### Plug-In MLE\nLet's consider optimizing the log marginal likelihood with respect to $\\alpha^2$.\nThe partial derivative of (25) with respect to $\\alpha^2$ is given by\n$$\n\\begin{align}\n\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\alpha^2}\n&= -\\frac{n}{2}\\frac{2\\pi}{2\\pi \\alpha^2} - \\frac{(y_n - \\mu_{\\psi})^\\top C^{-1} (y_n - \\mu_{\\psi})}{2\\alpha^4} \\newline\n&= -\\frac{n}{2\\alpha^2} - \\frac{(y_n - \\mu_{\\psi})^\\top C^{-1} (y_n - \\mu_{\\psi})}{2\\alpha^4}.\n\\end{align}\n$$\nSetting this expression equal to zero and solving for $\\alpha^2$ yields\n$$\n\\begin{align}\n\\hat{\\alpha}^2 &= \\frac{(y_n - \\mu_{\\psi})^\\top C^{-1} (y_n - \\mu_{\\psi})}{n}.\n\\end{align}\n$$\nFollowing the same procedure as before, the estimate $\\hat{\\alpha}^2$ can be\nsubbed in for $\\alpha^2$ in $\\mathcal{L}(\\theta)$ to obtain the\nconcentrated log marginal likelihood.\n\n### Closed-Form Marginalization\n\n\n## Bias Corrections\n\n# Bayesian Approaches\n\n# Computational Considerations\n## Log Marginal Likelihood\nWe start by considering the computation of the log marginal likelihood (16),\n$$\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log(2\\pi) -\\frac{1}{2} \\log \\text{det}\\left(C \\right) -\n\\frac{1}{2} (y - \\mu)^\\top C^{-1} (y - \\mu),\n\\end{align}\n$$\nwhere we now suppress all dependence on hyperparameters in the notation for\nsuccinctness.\nSince $C = k(X) + \\sigma^2 I_n$ is positive definite, we may Cholesky decompose it as\n$C = L L^\\top$. Plugging this decomposition into the log marginal likelihood yields\n$$\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log(2\\pi) - \\frac{1}{2} \\log\\text{det}\\left(C\\right) -\n\\frac{1}{2} (y_n - \\mu)^\\top \\left(LL^\\top \\right)^{-1} (y_n - \\mu).\n\\end{align}\n$$\nThe log determinant and the quadratic term can both be conveniently written in terms\nof the Cholesky factor. These terms are given respectively by\n$$\n\\begin{align}\n\\log\\text{det}\\left(LL^\\top\\right)\n&= \\log\\text{det}\\left(L\\right)^2\n= 2 \\log \\prod_{i=1}^{n} L_{ii}\n= 2 \\sum_{i=1}^{n} \\log\\left(L_{ii} \\right),\n\\end{align}\n$$\nand\n$$\n\\begin{align}\n(y_n - \\mu)^\\top \\left(LL^\\top \\right)^{-1} (y_n - \\mu)\n&= (y_n - \\mu)^\\top \\left(L^{-1}\\right)^\\top L^{-1} (y_n - \\mu)\n= \\lVert L^{-1}(y - \\mu)\\rVert_2^2.\n\\end{align}\n$$\nThe linear solve $L^{-1}(y - \\mu)$ can be computed in $\\mathcal{O}(n^2)$ by\nexploiting the fact that the linear system has lower triangular structure.\nPlugging these terms back into the log marginal\nlikelihood gives\n$$\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log(2\\pi) - \\sum_{i=1}^{n} \\log\\left(L_{ii}\\right) -\n\\frac{1}{2} \\lVert L^{-1}(y - \\mu)\\rVert_2^2.\n\\end{align}\n$$\nNote that the Cholesky factor $L$ is a function of $\\phi$ and $\\sigma^2$ and hence\nmust be re-computed whenever the kernel hyperparameters or noise variances\nchange.\n\n## Profile Log Marginal Likelihood with Linear Mean Function\nWe now consider computation of the concentrated marginal log-likelihood under\na mean function of the form (11), $\\mu(x) = h(x)^\\top \\beta$, where the generalized\nleast squares (GLS) estimator $\\hat{\\beta} = \\left(H^\\top C^{-1} H\\right)^{-1} H^\\top C^{-1}y$\n(see (23)) is inserted in place of $\\beta$. We are thus considering the profile log\nmarginal likelihood\n$$\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log(2\\pi) -\\frac{1}{2} \\log \\text{det}\\left(C \\right) -\n\\frac{1}{2} (y - H\\hat{\\beta})^\\top C^{-1} (y - H\\hat{\\beta}).\n\\end{align}\n$$\nWe will derive a numerically stable implementation of this expression in two steps,\nfirst applying a Cholesky decomposition (as in the previous section), and then\nleveraging a QR decomposition as in a typical ordinary least squares (OLS)\ncomputation. We first write $\\hat{\\beta}$ in terms of the Cholesky factor $L$,\nwhere $C = LL^\\top$:\n$$\n\\begin{align}\n\\hat{\\beta}\n&= \\left(H^\\top C^{-1} H\\right)^{-1} H^\\top C^{-1}y \\newline\n&= \\left(H^\\top \\left[LL^\\top\\right]^{-1} H\\right)^{-1} H^\\top \\left[LL^\\top\\right]^{-1}y \\newline\n&= \\left(\\left[L^{-1}H \\right]^\\top \\left[L^{-1}H \\right] \\right)^{-1} \\left[L^{-1}H \\right]^\\top\n\\left[L^{-1}y\\right].\n\\end{align}\n$$\nNotice that the GLS computation boils down to two lower-triangular linear solves:\n$L^{-1}H$ and $L^{-1}y$. However, the above expression still requires one non-triangular\nlinear solve that we will now address via the QR decomposition. The above expression\nfor $\\hat{\\beta}$ can be viewed as a standard OLS estimator with design matrix\n$L^{-1}H$ and response vector $L^{-1}y$. At this point, we could adopt a standard\nOLS technique of taking the QR decomposition of the design matrix $L^{-1}H$.\nThis was my original thought, but I found a nice alternative looking through the\ncode in the R [kergp](https://github.com/cran/kergp/blob/master/R/logLikFuns.R)\npackage (see the function `.logLikFun0` in the file `kergp/R/logLikFuns.R`). The approach\nis to compute the QR decomposition\n$$\n\\begin{align}\n\\begin{bmatrix} L^{-1}H & L^{-1}y \\end{bmatrix} &= QR = Q \\begin{bmatrix} \\tilde{R} & r \\end{bmatrix}.\n\\end{align}\n$$\nThat is, we compute QR on the matrix formed by concatenating $L^{-1}y$ as an additional\ncolumn on the design matrix $L^{-1}H$. We have written the upper triangular matrix\n$R \\in \\mathbb{R}^{(p+1) \\times (p+1)}$ as the concatenation of\n$\\tilde{R} \\in \\mathbb{R}^{(p+1) \\times p}$ and the vector\n$r \\in \\mathbb{R}^{p+1}$ so that $L^{-1}H = Q\\tilde{R}$ and $L^{-1}y = Qr$.\nWe recall the basic properties of the QR decomposition: $R$ is upper triangular\nand invertible, and $Q$ has orthonormal columns with span equal to the column space\nof $\\begin{bmatrix} L^{-1}H & L^{-1}y \\end{bmatrix}$. Taking the QR decomposition\nof this concatenated matrix leads to a very nice expression for the quadratic\nform term of the profile log marginal likelihood. But first let's rewrite $\\hat{\\beta}$\nin terms of these QR factors:\n$$\n\\begin{align}\n\\hat{\\beta}\n&= \\left(\\left[L^{-1}H \\right]^\\top \\left[L^{-1}H \\right] \\right)^{-1} \\left[L^{-1}H \\right]^\\top\n\\left[L^{-1}y\\right] \\newline\n&= \\left(\\left[Q\\tilde{R} \\right]^\\top \\left[Q\\tilde{R} \\right] \\right)^{-1} \\left[Q\\tilde{R} \\right]^\\top \\left[Qr\\right] \\newline\n&= \\left(\\tilde{R}^\\top Q^\\top Q\\tilde{R} \\right)^{-1} \\tilde{R}^\\top Q^\\top Qr \\newline\n&= \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top r,\n\\end{align}\n$$\nwhere we have used the fact that $Q^\\top Q$ is the identity since $Q$ is orthogonal.\nPlugging this into the quadratic form term of the log likelihood gives\n$$\n\\begin{align}\n(y - H\\hat{\\beta})^\\top C^{-1} (y - H\\hat{\\beta})\n&= (y - H\\hat{\\beta})^\\top \\left[LL^\\top \\right]^{-1} (y - H\\hat{\\beta}) \\newline\n&= \\lVert L^{-1}(y - H\\hat{\\beta}) \\rVert_2^2 \\newline\n&= \\lVert L^{-1}y - L^{-1}H\\hat{\\beta} \\rVert_2^2 \\newline\n&= \\lVert Qr - Q\\tilde{R} \\hat{\\beta} \\rVert_2^2 \\newline\n&= \\left\\lVert Qr - Q\\tilde{R} \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top r \\right\\rVert_2^2 \\newline\n&= \\left\\lVert Q\\left[r - \\tilde{R} \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top r \\right] \\right\\rVert_2^2 \\newline\n&= \\left\\lVert r - \\tilde{R} \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top \\right\\rVert_2^2 \\newline\n&= \\left\\lVert \\left[I - \\tilde{R} \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top \\right]r \\right\\rVert_2^2,\n\\end{align}\n$$\nwhere the penultimate line follows from the fact that $Q$ is orthogonal, and hence an\nisometry. At this point, notice that the matrix\n$P := \\tilde{R} \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top$ is the standard OLS projection matrix (i.e., hat matrix)\nconstructed with the design matrix $\\tilde{R}$. Also, take care to notice that\n$\\tilde{R}$ is not invertible (it is not even square). Using standard properties\nof the projection matrix, we know that $P$ has rank $p$, since $\\tilde{R}$ has rank $p$.\nAlso, since $R$ is upper triangular, then the last row of $\\tilde{R}$ contains all zeros.\nLetting, $e_j$ denote the $j^{\\text{th}}$ standard basis vector of $\\mathbb{R}^{p+1}$,\nthis means that\n$$\n\\begin{align}\n\\mathcal{R}(P) \\perp \\text{span}(e_{p+1}),\n\\end{align}\n$$\nwhere $\\mathcal{R}(P)$ denotes the range (i.e., column space) of $P$.\nThe only subspace of $\\mathbb{R}^{p+1}$ with rank $p$ and satisfying this property\nis $\\text{span}(e_1, \\dots, e_p)$. The conclusion is that $P$ projects onto $\\text{span}(e_1, \\dots, e_p)$, and thus the annihilator $I - P$ projects onto\nthe orthogonal complement $\\text{span}(e_{p+1})$. We thus conclude,\n$$\n\\begin{align}\n\\left\\lVert \\left[I - P\\right]r \\right\\rVert_2^2\n&= \\lVert \\langle r, e_{p+1} \\rangle e_{p+1} \\rVert_2^2 \\newline\n&= \\lVert r_{p+1} e_{p+1} \\rVert_2^2 \\newline\n&= r_{p+1}^2,\n\\end{align}\n$$\nwhere $r_{p+1}$ is the last entry of $r$; i.e., the bottom right entry of $R$.\nWe finally arrive at the expression for the concentrated log marginal likelihood\n$$\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log(2\\pi) - \\sum_{i=1}^{n} \\log\\left(L_{ii}\\right) -\n\\frac{1}{2} r_{p+1}^2.\n\\end{align}\n$$\n\n# References\n- Surrogates (Gramacy)\n- Statistics or geostatistics? Sampling error or nugget effect? (Clark)\n- Michael Betencourt's very nice [post](https://betanalpha.github.io/assets/case_studies/gp_part3/part3.html#4_adding_an_informative_prior_for_the_length_scale) on setting\npriors on GP hyperparameters.\n- Large-Scale Spatial Modeling via Local Approximate Gaussian Processes in R.\n(nice discussion of setting ranges for hyperparameters in the appendix; see\nlaGP function `darg`).\n","srcMarkdownNoYaml":"\n\nGaussian processes (GP) are widely utilized across various fields, each with\ntheir own preferences, terminology, and conventions. Some notable domains that\nmake significant use of GPs include\n- Spatial statistics (kriging)\n- Design and analysis of computer experiments (emulator/surrogate modeling)\n- Bayesian optimization\n- Machine learning\n\nEven if you're a GP expert in one of these domains,\nthese differences can make navigating the\nGP literature in other domains a bit tricky. The goal of this post is to\nsummarize common approaches for specifying GP distributions, and emphasize\nconventions and assumptions that tend to differ across fields. By\n\"specifying GP distributions\", what I am really talking about here is\nparameterizing the mean and covariance functions that define the GP. While\nGPs are non-parametric models in a certain sense, specifying and\nlearning the *hyperparameters* making up the mean and covariance functions\nis a crucial step to successful GP applications. I will discuss popular\nparameterizations for these functions, and different algorithms for learning\nthese parameter values from data. In the spirit of drawing connections across\ndifferent domains, I will try my best to borrow terminology from different fields,\nand will draw attention to synonymous terms by using boldface.\n\n## Background\n\n### Gaussian Processes\nGaussian processes (GPs) define a probability distribution over a space of\nfunctions in such a way that they can be viewed as a generalization of\nGaussian random vectors. Just as Gaussian vectors are defined by their\nmean vector and covariance matrix, GPs are defined by a mean and covariance\n*function*. We will interchangeably refer to the latter as either the\n**covariance function** or **kernel**.\n\nWe will consider GPs defined over a space of functions of the form\n$f: \\mathcal{X} \\to \\mathbb{R}$, where $\\mathcal{X} \\subseteq \\mathbb{R}^d$.\nWe will refer to elements $x \\in \\mathcal{X}$ as **inputs** or\n**locations** and the images $f(x) \\in \\mathbb{R}$ as **outputs** or\n**responses**. If the use of the word \"locations\" seems odd, note that in\nspatial statistical settings, the inputs $x$ are often geographic coordinates.\nWe will denote the mean and covariance function defining the\nGP by $\\mu: \\mathcal{X} \\to \\mathbb{R}$\nand $k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$, respectively. The mean\nfunction is essentially unrestricted, but the covariance function $k(\\cdot, \\cdot)$\nmust be a valid positive definite kernel. If $f(\\cdot)$ is a GP with\nmean function $\\mu(\\cdot)$ and kernel $k(\\cdot, \\cdot)$ we will denote this by\n$$\n\\begin{align}\nf \\sim \\mathcal{GP}(\\mu, k). \\tag{1}\n\\end{align}\n$$\n\nThe defining property of GPs is that their finite-dimensional distributions\nare Gaussian; that is, for an arbitrary finite set of $n$ inputs\n$X := \\{x_1, \\dots, x_N\\} \\subset \\mathcal{X}$,\nthe vector $f(X) \\in \\mathbb{R}^n$ is distributed as\n$$\n\\begin{align}\nf(X) \\sim \\mathcal{N}(\\mu(X), k(X, X)). \\tag{2}\n\\end{align}\n$$\nWe are vectorizing notation here so that $[f(X)]_i := f(x_i)$,\n$[\\mu(X)]_i := \\mu(x_i)$, and $[k(X, X)]_{i,j} := k(x_i, x_j)$. When the\ntwo input sets to the kernel are equal, we lighten notation by writing\n$k(X) := k(X, X)$.\nNow suppose we have two sets of inputs\n$X$ and $\\tilde{X}$, containing $n$ and\n$m$ inputs, respectively. The defining property (2) then implies\n\n$$\n\\begin{align}\n\\begin{bmatrix} f(\\tilde{X}) \\newline f(X) \\end{bmatrix}\n&\\sim \\mathcal{N}\\left(\n  \\begin{bmatrix} \\mu(\\tilde{X}) \\newline \\mu(X) \\end{bmatrix},\n  \\begin{bmatrix}\n  k(\\tilde{X}) & k(\\tilde{X}, X) \\newline\n  k(X, \\tilde{X}) & k(X)\n  \\end{bmatrix}\n\\right). \\tag{3}\n\\end{align}\n$$\n\nThe Gaussian joint distribution (3) implies that the conditional distributions\nare also Gaussian. In particular, the distribution of $f(\\tilde{X})|f(X)$ can be\nobtained by applying the well-known Gaussian conditioning identities:\n\n$$\n\\begin{align}\nf(\\tilde{X})|f(X) &\\sim \\mathcal{N}(\\hat{\\mu}(\\tilde{X}), \\hat{k}(\\tilde{X})), \\tag{4} \\newline\n\\hat{\\mu}(\\tilde{X}) &:= \\mu(\\tilde{X}) + k(\\tilde{X}, X)k(X)^{-1} [f(X) - \\mu(X)] \\newline\n\\hat{k}(\\tilde{X}) &:= k(\\tilde{X}) - k(\\tilde{X}, X)k(X)^{-1} k(X, \\tilde{X}).\n\\end{align}\n$$\nThe fact that the result (4) holds for arbitrary finite sets of inputs $\\tilde{X}$\nimplies that the conditional $f | f(X)$ is also a GP, with mean and covariance\nfunctions $\\hat{\\mu}(\\cdot)$ and $\\hat{k}(\\cdot, \\cdot)$ defined by (4).\nOn a terminology note, the $n \\times n$ matrix $k(X)$ is often called the\n**kernel matrix**. This is the matrix containing the kernel evaluations at the\nset of $n$ *observed* locations.\n\n### Regression with GPs\nOne common application of GPs is their use as a flexible nonlinear regression\nmodel. Let's consider the basic regression setup with observed data pairs\n$(x_1, y_1), \\dots, (x_n, y_n)$. We assume that the $y_i$ are noisy observations\nof some underlying latent function output $f(x_i)$. The GP regression model arises\nby placing a GP prior distribution on the latent function $f$. We thus consider\nthe regression model\n$$\n\\begin{align}\ny(x) &= f(x) + \\epsilon(x) \\tag{5} \\newline\nf &\\sim \\mathcal{GP}(\\mu, k) \\newline\n\\epsilon(x) &\\overset{iid}{\\sim} \\mathcal{N}(0, \\sigma^2),\n\\end{align}\n$$\nwhere we have assumed a simple additive Gaussian noise model. This assumption is\nquite common in the GP regression setting due to the fact that it results in\nclosed-form conditional distributions, similar to (4). We will assume the\nerror model (5) throughout this post, but note that there are many other possibilities\nif one is willing to abandon closed-form posterior inference.\n\nThe solution of the regression problem is given by the distribution of\n$f(\\cdot)|y(X)$ or $y(\\cdot)|y(X)$, where $y(X)$ is the $n$-dimensional vector\nof observed responses. The first distribution is the posterior on the latent\nfunction $f$, while the second incorporates the observation noise as well.\nBoth distributions can be derived in the same way, so we focus on the second.\nLetting $\\tilde{X}$ denote a set of $m$ inputs at which we would like to predict the\nresponse, consider the joint distribution\n$$\n\\begin{align}\n\\begin{bmatrix} y(\\tilde{X}) \\newline y(X) \\end{bmatrix}\n&\\sim \\mathcal{N}\\left(\n  \\begin{bmatrix} \\mu(\\tilde{X}) \\newline \\mu(X) \\end{bmatrix},\n  \\begin{bmatrix}\n  k(\\tilde{X}) + \\sigma^2 I_m & k(\\tilde{X}, X) \\newline\n  k(X, \\tilde{X}) & k(X) + \\sigma^2 I_n\n  \\end{bmatrix}\n\\right). \\tag{6}\n\\end{align}\n$$\nThis is quite similar to (3), but now takes into account the noise term\n$\\epsilon$. This does not affect the mean vector since $\\epsilon$ is mean-zero;\nnor does it affect the off-diagonal elements of the covariance matrix since\n$\\epsilon$ and $f$ were assumed independent. Applying the Gaussian conditioning\nidentities (4) yields the posterior distribution\n$$\n\\begin{align}\ny(\\tilde{X})|y(X) &\\sim \\mathcal{N}(\\hat{\\mu}(\\tilde{X}), \\hat{k}(\\tilde{X})), \\tag{7} \\newline\n\\hat{\\mu}(\\tilde{X}) &:= \\mu(\\tilde{X}) + k(\\tilde{X}, X)[k(X) + \\sigma^2 I_n]^{-1} [f(X) - \\mu(X)] \\newline\n\\hat{k}(\\tilde{X}) &:= \\sigma^2 I_m + k(\\tilde{X}) - k(\\tilde{X}, X)[k(X) + \\sigma^2 I_n]^{-1} k(X, \\tilde{X}).\n\\end{align}\n$$\nWe will refer to (7) as the GP **posterior**, **predictive**, or generically\n**conditional**, distribution.\nWe observe that these equations are identical to (4), modulo the appearance\nof $\\sigma^2$ in the predictive mean and covariance equations. The distribution\n$f(\\tilde{X})|y(X)$ is identical to (7), except that the $\\sigma^2 I_m$ is removed in\nthe predictive covariance. Again, this reflects the subtle distinction between\ndoing inference on the latent function $f$ versus on the observation process $y$.\n\n### Noise, Nuggets, and Jitters\nObserve that this whole regression procedure is only slightly different from the\nnoiseless GP setting explored in the previous section (thanks to the Gaussian\nlikelihood assumption). Indeed, the conditional distribution of\n$f(\\tilde{X})|y(X)$ is derived from $f(\\tilde{X})|f(X)$ by simply replacing\n$k(X)$ with $k(X) + \\sigma^2 I_n$ (obtaining the distribution\n$y(\\tilde{X})|y(X)$ requires the one additional step of adding $\\sigma^2 I_m$\nto the predictive covariance). In other words, we have simply applied standard\nGP conditioning using the modified kernel matrix\n$$\n\\begin{align}\nC(X) := k(X) + \\sigma^2 I_n. \\tag{8}\n\\end{align}\n$$\nWe thus might reasonably wonder if the model (5) admits an alternative\nequivalent representation by defining a GP directly on the observation process $y$.\nDefining such a model would require\ndefining a kernel $c: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ that\nis consistent with (8). This route is fraught with difficulties and subtleties, which I will\ndo my best to describe clearly here. At first glance, it seems like the right\nchoice is\n$$\n\\begin{align}\nc(x, x^\\prime) := k(x, x^\\prime) + \\sigma^2 \\delta(x, x^\\prime), \\tag{9}\n\\end{align}\n$$\nwhere $\\delta(x, x^\\prime) := 1[x = x^\\prime]$ is sometimes called the\n**stationary white noise kernel**. Why isn't this quite right? Notice in (9)\nthat $\\sigma^2$ is added whenever the inputs $x = x^\\prime$ are equal. However,\nsuppose we observe multiple independent realizations of the process at the\nsame inputs $X$. In the regression model (9) the errors $\\epsilon(x)$ are\nindependent across these realizations, *even at the same locations*. However,\nthis will not hold true in the model under (9), since $\\delta(x, x^\\prime)$ only\nsees the values of the inputs, and has no sense of distinction across realizations.\nWe might try to fix this by writing something like\n$$\n\\begin{align}\nc(x_i, x_j) := k(x_i, x_j) + \\sigma^2 \\delta_{ij}, \\tag{10}\n\\end{align}\n$$\nwhere the Delta function now depends on the labels $i, j$ instead of the values\nof the inputs. In the spatial statistics literature,\nit is not uncommon to see a covariance function defined like (10), but this is\nbasically a notational hack. A kernel is a function of two inputs from\n$\\mathcal{X}$ - we can't have it also depending on some side information like\nthe labels $i, j$. At the end of the day, (9) and (10) are attempts to\nincorporate some concept of **white noise** inside the kernel itself, rather\nthan via a hierarchical model like (5). I would just stick with the hierarchical\nmodel, which is easily rigorously defined and much more intuitive.\n\nNonetheless, one should not be surprised if expressions like (10) pop up,\nespecially in the spatial statistics literature. Spatial statisticians refer\nto the noise term $\\epsilon(x)$ as the **nugget**, and $\\sigma^2$ the\n**nugget variance** (sometimes these terms are conflated). In this context,\ninstead of representing observation noise, $\\sigma^2$ is often thought of\nas representing some unresolved small-scale randomness in the spatial field\nitself. If you imagine sampling a field to determine the concentration of some\nmineral across space, then you would hope that repeated measurements (taken around\nthe same time) would yield the same values. Naturally, they may not, and the\nintroduction of the nugget is one way to account for this.\n\nWhile this discussion may seem needlessly abstract, we recall that the\neffect of incorporating the noise term (however you want to interpret it) is to\nsimply replace the kernel matrix $k(X)$ with the new matrix $c(X) = k(X) + \\sigma^2 I_n$.\nConfusingly, there is one more reason (having nothing to do with observation error\nor nuggets) that people use a matrix of the form $c(X)$ in place of $k(X)$:\nnumerical stability. Indeed, even though $k(X)$ is theoretically positive definite,\nin practice its numerical instantiation may fail to have this property. A simple\napproach to deal with this is to add a small, fixed constant $\\sigma^2$ to the\ndiagonal of the kernel matrix. In this context, $\\sigma^2$ is often called the\n**jitter**. While computationally its effect is the same as the nugget, note that\nits introduction is motivated very differently. The jitter is not stemming from\nsome sort of random white noise; it is purely a computational hack to improve\nthe conditioning of the kernel matrix. Check out\n[this](https://discourse.mc-stan.org/t/adding-gaussian-process-covariance-functions/237/67)\nthread for some entertaining debates on the use of the nugget and jitter concepts.\n\n### Parameterized Means and Kernels\nEverything we have discussed this far assumes fixed mean and covariance\nfunctions. In practice, suitable choices for these quantities are not typically\nknown. Thus, the usual approach is to specify some parametric families\n$\\mu = \\mu_{\\psi}$ and $k = k_{\\phi}$ and learn their parameters from data.\nThe parameters $\\psi$ and $\\phi$ are often referred to as **hyperparameters**,\nsince they are not the primary parameters of interest in the GP regression model.\nRecalling from (5) that the GP acts as a prior distribution on the latent\nfunction, we see that $\\psi$ and $\\phi$ control the specification of this\nprior distribution. In addition to $\\psi$ and $\\phi$, the parameter\n$\\sigma^2$ is also typically not known. I will not wade back into the previous\nsection's debate in arguing whether this should be classified as\na \"hyperparameter\" or not. In any case, let's let\n$\\theta := \\{\\psi, \\phi, \\sigma^2 \\}$ denote the full set of\n(hyper)parameters that must be learned from data.\n\n#### Mean Functions\nThe machine learning community commonly uses the simplest possible form for the\nmean function: $\\mu(x) \\equiv 0$. This zero-mean assumption is less restrictive\nthan it seems, since GPs mainly derive their expressivity from the kernel.\nA slight generalization is to allow a constant, non-zero mean\n$\\mu(x) \\equiv \\beta_0$, where $\\beta_0 \\in \\mathbb{R}$.\nHowever, constant (including zero-mean) GP priors can have some undesirable properties;\ne.g., in the context of extrapolation. Sometimes one wants more flexibility, and\nin these cases it is quite common to consider some sort of linear regression\nmodel\n$$\n\\begin{align}\n\\mu(x) = h(x)^\\top \\beta, \\tag{11}\n\\end{align}\n$$\nwhere $h: \\mathcal{X} \\to \\mathbb{R}^p$ is some feature map and $\\beta \\in \\mathbb{R}^p$\nthe associated coefficient vector. For example, $h(x) = [1, x^\\top]^\\top$\nwould yield a standard linear model, and\n$h(x) = [1, x_1, \\dots, x_d, x_1^2, \\dots, x_d^2]^\\top$ would allow for a quadratic\ntrend.\n\n#### Kernels\nThe positive definite restriction makes defining valid covariance functions\nmuch more difficult than defining mean functions. Thus, one typically falls back\non one of a few popular choices of known parametric kernel families (though\nnote that kernels can be combined in various ways to give a large variety of\noptions). While the goal of this post is not to explore specific kernels, in order to have\na concrete example in mind consider the following parameterization:\n  $$\n\\begin{align}\nk(x, \\tilde{x}) = \\alpha^2 \\sum_{j=1}^{d} \\left(-\\frac{\\lvert x^{j} - \\tilde{x}^j \\rvert}{\\ell^j}\\right)^2.\n\\tag{12}\n\\end{align}\n$$\nNote that I'm using superscripts to index vector entries here.\nThis kernel goes by many names, including **exponentiated quadratic**,\n**squared exponential**, **Gaussian**, **radial basis function**, and\n**automatic relevance determination**. The parameter $\\alpha^2$ is sometimes called\nthe **marginal variance**, or just the **scale parameter**. The parameters\n$\\ell^1, \\dots, \\ell^d$ are often called **lengthscale**, **smoothness**, or\n**range** parameters, since they control the smoothness of the GP realizations\nalong each coordinate direction. Other popular kernels (e.g., MatÃ©rn) have\nanalogous parameters controlling similar features. Note that in this example\nwe have $\\phi = \\{\\alpha^2, \\ell^1, \\dots, \\ell^d \\}$. Also note that people\nchoose to parameterize the Gaussian kernel in many different ways; for example,\nit's not uncommon to see a $1/2$ factor included inside the exponential to make\nthe kernel align with the typical parameterization of the Gaussian probability\ndensity function. Knowing which parameterization you're working with is important\nfor interpreting the hyperparameters, specifying bounds, defining priors, etc.\n\nIt is quite common in the\nspatial statistics (and sometimes the computer experiments) literature to see\nkernels written like $\\alpha^2 k(\\cdot, \\cdot)$; in these cases $k(\\cdot, \\cdot)$\ntypically represents a *correlation* function, which becomes the covariance function\nafter multiplying by the marginal variance $\\alpha^2$. There is an advantage in\ndecomposing the kernel this way when it comes to estimating the hyperparameters,\nwhich we will discuss shortly.\n\n### The GP (Marginal) Likelihood Function\nLet's first recall the GP regression model (5)\n$$\n\\begin{align}\ny(x) &= f(x) + \\epsilon(x) \\newline\nf &\\sim \\mathcal{GP}(\\mu_{\\psi}, k_{\\phi}) \\newline\n\\epsilon &\\overset{iid}{\\sim} \\mathcal{N}(0, \\sigma^2),\n\\end{align}\n$$\nwhere we have now explicitly added the dependence on $\\psi$ and $\\phi$.\nThis model is defined for any $x \\in \\mathcal{X}$. However, when estimating\nhyperparameters, we will naturally be restricting the model to $X$, the finite\nset of locations at which we actually have observations. Restricting to\n$X$ reduces the above model to the standard (finite-dimensional)\nBayesian regression model\n$$\n\\begin{align}\ny(X)|f(X), \\theta &\\sim \\mathcal{N}(f(X), \\sigma^2 I_n) \\tag{13} \\newline\nf(X)|\\theta &\\sim \\mathcal{N}(\\mu_{\\psi}(X), k_{\\phi}(X)).\n\\end{align}\n$$\nWe could consider completing the Bayesian specification by defining a prior\non $\\theta$, but we'll hold off on this for now.\nNotice that the model (13) defines a joint distribution over\n$[y(X), f(X)] | \\theta$, with $y(X)|f(X), \\theta$ representing the\nlikelihood of the observations at the observed input locations $X$. At present\neverything is conditional on a fixed $\\theta$.\nNow, if we marginalize the likelihood $y(X)|f(X), \\theta$ with\nrespect to $f(X)$ then we obtain the distribution $y(X) | \\theta$. This is often\ncalled the **marginal likelihood**, due to the fact that $f$ was marginalized\nout. In particular, the distribution $y(X) | \\theta$ has implicitly marginalized\nthe function values $f(\\tilde{x})$ at all location $\\tilde{x}$ other than $X$.\nThis same logic and terminology applies in the noiseless setting with $\\sigma^2 = 0$,\nin which case the marginal likelihood is given by $f(X) | \\theta$. In the noiseless\nsetting we are marginalizing both over the unobserved function values and\nthe noise $\\epsilon$.\nThanks to all the Gaussian assumptions here, the marginal likelihood\nis available in closed-form. One could approach the derivation using (13) as\nthe starting point, but it's much easier to consider the model written out using\nrandom variables,\n$$\n\\begin{align}\ny(X) &= f(X) + \\epsilon(X).\n\\end{align}\n$$\nSince $f(X)$ and $\\epsilon(X)$ are independent Gaussians, then their sum is also\nGaussian with mean and covariance given by\n$$\n\\begin{align}\n\\mathbb{E}[y(X)|\\theta]\n&= \\mathbb{E}[f(X)|\\theta] + \\mathbb{E}[\\epsilon(X)|\\theta] = \\mu_{\\psi}(X) \\newline\n\\text{Cov}[y(X)|\\theta]\n&= \\text{Cov}[f(X)|\\theta] + \\text{Cov}[\\epsilon(X)|\\theta]\n= k_{\\phi}(X) + \\sigma^2 I_n.\n\\end{align}\n$$\nWe have thus found that\n$$\n\\begin{align}\ny(X)|\\theta \\sim \\mathcal{N}\\left(\\mu_{\\psi}(X), C_{\\phi, \\sigma^2}(X)\\right), \\tag{14}\n\\end{align}\n$$\nrecalling the definition $C_{\\phi, \\sigma^2}(X) := k_{\\phi}(X) + \\sigma^2 I_n$.\nWe will let $\\mathcal{L}(\\theta)$ denote the log density of this Gaussian\ndistribution; i.e. the log **marginal likelihood**:\n$$\n\\begin{align}\n\\mathcal{L}(\\theta)\n&:= -\\frac{1}{2} \\log \\text{det}\\left(2\\pi C_{\\phi, \\sigma^2}(X) \\right) -\n\\frac{1}{2} (y(X) - \\mu_{\\psi}(X))^\\top C_{\\phi, \\sigma^2}(X)^{-1} (y(X) - \\mu_{\\psi}(X)) \\tag{15}\n\\end{align}\n$$\nThe function $\\mathcal{L}(\\theta)$ plays a central role in the typical approach\nto hyperparameter optimization, as we will explore below. Also note that\nthe above derivations also apply to the noiseless setting\n(i.e., $y(X) = f(X)$) by setting $\\sigma^2 = 0$. In this case, the marginal\nlikelihood is simply the GP distribution restricted to the inputs $X$.\n\nI have henceforth been a bit verbose with the notation in (15) to make very explicit\nthe dependence on the inputs $X$ and the hyperparameters. To lighten notation a\nbit, we define $y_n := y(X)$, $\\mu_{\\psi} := \\mu_{\\psi}(X)$, and\n$C_{\\phi, \\sigma^2} := C_{\\phi, \\sigma^2}(X)$, allowing us to rewrite (15) as\n$$\n\\begin{align}\n\\mathcal{L}(\\theta)\n&:= -\\frac{1}{2} \\log \\text{det}\\left(2\\pi C_{\\phi, \\sigma^2} \\right) -\n\\frac{1}{2} (y_n - \\mu_{\\psi})^\\top C_{\\phi, \\sigma^2}^{-1} (y_n - \\mu_{\\psi}). \\tag{16}\n\\end{align}\n$$\nWe have simply suppressed the explicit dependence on $X$ in the notation.\n\n# Hyperparameter Optimization\nWe now begin to turn out attention to methods for learning the values of the\nhyperparameters from data. This section starts with the most popular approach:\noptimizing the marginal likelihood.\n\n## Maximum Marginal Likelihood, or Empirical Bayes\nRecall that (16) gives the expression for the log marginal likelihood $\\mathcal{L}(\\theta)$, which is just the log density of $y(X)|\\theta$ viewed as a function of $\\theta$.\nA natural approach is to set the hyperparameters $\\theta$ to their values\nthat maximize $\\mathcal{L}(\\theta)$:\n$$\n\\begin{align}\n\\hat{\\theta} := \\text{argmax} \\ \\mathcal{L}(\\theta). \\tag{17}\n\\end{align}\n$$\nAt first glance, the Gaussian form of $\\mathcal{L}(\\theta)$ might look quite\nfriendly to closed-form optimization.\nAfter all, maximum likelihood estimates of the mean and covariance of Gaussian\nvectors are indeed available in closed-form. However, upon closer inspection notice\nthat the covariance is not being directly optimized; we are optimizing $\\phi$, and\nthe covariance $C_{\\phi, \\sigma^2}$ is a *nonlinear* function of this\nparameter. Thus, in general some sort of iterative numerical scheme is\nis used for the optimization. Typically, gradient-based approaches are preferred,\nmeaning we must be able to calculate quantities like\n$\\frac{\\partial}{\\partial \\phi} C_{\\phi, \\sigma^2}$.\nThe exact gradient calculations will thus depend on the choice of kernel; specifics\non kernels and optimization schemes are not the focus of this post. We will instead\nfocus on the high level ideas here. The general approach to GP regression\nthat we have outlined so far can be summarized as:\n1. Solve the optimization problem (17) and fix the hyperparameters at their\noptimized values $\\hat{\\theta}$. The hyperparameters will be fixed from\nthis point onward.\n2. Use the GP predictive equations (7) to perform inference at a set of locations\nof interest $\\tilde{X}$.\n\nOne might object to the fact that we are estimating the hyperparameters from\ndata, and then neglecting the uncertainty in $\\hat{\\theta}$ during the\nprediction step. It is true that this uncertainty is being ignored, but it is\nalso very computationally convenient to do so.\nWe will discuss alternatives later\non, but this simple approach is probably the most commonly used\nin practice today. One way to think about this strategy is in an\n**empirical Bayes** context; that is, we can view this approach as an approximation\nto a fully Bayesian hierarchical model, which would involve equipping the\nhyperparameters with their own priors. Instead of marginalizing the hyperparameters,\nwe instead fix them at their most likely values with respect to the\nobserved data. We are using the data to \"fine tune\" the GP prior distribution.\nIn the literature you will see this general hyperparameter optimization strategy\nreferred to as either **empirical Bayes**, **maximum marginal likelihood**, or\neven just **maximum likelihood**.\n\n## Special Case Closed-Form Solutions: Mean Function\nAs mentioned above, in general the maximization of $\\mathcal{L}(\\theta)$ requires\nnumerical methods. However, in certain cases elements of $\\theta$ can be optimized\nin closed-form, meaning that numerical optimization may only be required for\na subset of the hyperparameters. We start by considering closed form optimizers\nfor the parameters defining the mean functions.\n\n### Constant Mean: Plug-In MLE\nWith the choice of constant mean $\\mu_{\\psi}(x) \\equiv \\beta_0$ the log marginal\nlikelihood becomes\n\n$$\n\\begin{align}\n\\mathcal{L}(\\theta)\n&:= -\\frac{1}{2} \\log \\text{det}\\left(2\\pi C_{\\phi, \\sigma^2} \\right) -\n\\frac{1}{2} (y_n - \\beta_0 1_n)^\\top C_{\\phi, \\sigma^2}(X)^{-1} (y_n - \\beta_0 1_n),\n\\end{align}\n$$\n\nwith $1_n \\in \\mathbb{R}^n$ denoting a vector of ones. We now consider optimizing\n$\\mathcal{L}(\\theta)$ as a function of $\\beta_0$ only. The partial derivative\nwith respect to the constant mean equals\n$$\n\\begin{align}\n\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\beta_0}\n&= y_n^\\top C_{\\phi, \\sigma^2}^{-1}1_n - \\beta_0 1_n^\\top C_{\\phi, \\sigma^2}^{-1} 1_n.    \\tag{18}\n\\end{align}\n$$\nSetting (18) equal to zero and solving for $\\beta_0$ gives the optimum\n$$\n\\begin{align}\n\\hat{\\beta}_0(\\phi, \\sigma^2) = \\frac{y_n^\\top C_{\\phi, \\sigma^2}^{-1} 1_n}{1_n^\\top C_{\\phi, \\sigma^2}^{-1} 1_n}. \\tag{19}\n\\end{align}\n$$\nNotice that $\\hat{\\beta}_0(\\phi, \\sigma^2)$ depends on the values of the other hyperparameters\n$\\phi$ and $\\sigma^2$. Therefore, while this does not give us the outright value\nfor the mean, we can plug $\\hat{\\beta}_0(\\phi, \\sigma^2)$ in place of $\\beta_0$ in the marginal\nlikelihood. This yields the **profile likelihood** (aka the **concentrated likelihood**),\nwhich is no longer a function of $\\beta_0$ and hence the dimensionality of the subsequent numerical optimization problem has been reduced.\n\n### Linear Model Coefficients: Plug-In MLE\nLet's try to do the same thing with the mean function $\\mu_{\\psi}(x) = h(x)^\\top \\beta$.\nThe constant mean function is actually just a special case of this more general\nsetting, but its common enough that it warranted its own section.\nIf we denote by $H \\in \\mathbb{R}^{n \\times p}$ the feature matrix with rows equal\nto $h(x_i)^\\top$, $i = 1, \\dots, n$ then the marginal likelihood becomes\n$$\n\\begin{align}\n\\mathcal{L}(\\theta)\n&:= -\\frac{1}{2} \\log \\text{det}\\left(2\\pi C_{\\phi, \\sigma^2} \\right) -\n\\frac{1}{2} (y_n - H\\beta)^\\top C_{\\phi, \\sigma^2}^{-1} (y_n - H\\beta), \\tag{20}\n\\end{align}\n$$\n\nwith gradient\n$$\n\\begin{align}\n\\nabla_{\\beta} \\mathcal{L}(\\theta)\n&= H^\\top C_{\\phi, \\sigma^2}^{-1}y_n - (H^\\top C_{\\phi, \\sigma^2}^{-1} H)\\beta. \\tag{21}\n\\end{align}\n$$\nSetting the gradient equal to zero and solving for $\\beta$ yields the optimality\ncondition\n$$\n\\begin{align}\n\\left(H^\\top C_{\\phi, \\sigma^2}^{-1} H\\right)\\hat{\\beta} &= H^\\top C_{\\phi, \\sigma^2}^{-1}y_n. \\tag{22}\n\\end{align}\n$$\nA unique solution for $\\hat{\\beta}$ thus exists when $H^\\top C_{\\phi, \\sigma^2}^{-1} H$\nis invertible. When does this happen? First note that this matrix is positive\nsemidefinite, since\n$$\n\\begin{align}\n\\beta^\\top \\left(H^\\top C_{\\phi, \\sigma^2}^{-1} H\\right) \\beta\n&= \\beta^\\top (H^\\top [LL^\\top]^{-1} H) \\beta\n= \\lVert L^{-1} H\\beta \\rVert_2^2 \\geq 0,\n\\end{align}\n$$\nwhere we have used the fact that $C_{\\phi, \\sigma^2}$ is positive definite and\nhence admits a decomposition $LL^\\top$. The matrix $H^\\top C_{\\phi, \\sigma^2}^{-1} H$\nis thus positive definite when $L^{-1}H$ has linearly independent columns; i.e., when\nit is full rank. We already know that $L^{-1}$ is full rank. If we assume that\n$H$ is also full rank and $n \\geq p$ then we can conclude that $L^{-1}H$ is\nfull rank; see [this](https://math.stackexchange.com/questions/272049/rank-of-matrix-ab-when-a-and-b-have-full-rank) post for a quick proof. Thus, under these assumptions we conclude\nthat $H^\\top C_{\\phi, \\sigma^2}^{-1} H$ is invertible and so\n$$\n\\begin{align}\n\\hat{\\beta}(\\phi, \\sigma^2) &= \\left(H^\\top C_{\\phi, \\sigma^2}^{-1} H\\right)^{-1} H^\\top C_{\\phi, \\sigma^2}^{-1}y_n.\n\\tag{23}\n\\end{align}\n$$\nNotice that (23) is simply a [generalized least squares](https://en.wikipedia.org/wiki/Generalized_least_squares) estimator. As with the constant mean, we can plug\n$\\hat{\\beta}(\\phi, \\sigma^2)$ into the marginal likelihood to concentrate out\nthe parameter $\\beta$. The resulting concentrated likelihood can then be numerically\noptimized as a function of the remaining hyperparameters.\n\n### Linear Model Coefficients: Closed-Form Marginalization\nThe above section showed that, conditional on fixed kernel hyperparameters,\nthe coefficients of a linear mean function can be optimized in closed form.\nWe now show a similar result: if the mean coefficients are assigned a Gaussian\nprior then, conditional on fixed kernel hyperparameters, the coefficients can\nbe marginalized in closed form. To this end, we consider the same linear mean\nfunction as above, but now equip the coefficients with a Gaussian prior:\n$$\n\\begin{align}\n\\mu_{\\psi}(x) &= h(x)^\\top \\beta, &&\\beta \\sim \\mathcal{N}(b, B).\n\\end{align}\n$$\nRestricted to the model inputs $X$, the model is thus\n$$\n\\begin{align}\ny_n|\\beta &\\sim \\mathcal{N}\\left(H\\beta, C_{\\phi, \\sigma^2} \\right) \\newline\n\\beta &\\sim \\mathcal{N}(b, B).\n\\end{align}\n$$\nOur goal here is derive the marginal distribution of $y_n$. We could resort to\ncomputing the required integral by hand, but an easier approach is to notice\nthat under the above model $[y_n, \\beta]$ is joint Gaussian distributed.\nTherefore, the marginal distribution of $y_n$ must also be Gaussian. It thus\nremains to identify the mean and covariance of this distribution. We obtain\n$$\n\\begin{align}\n\\mathbb{E}[y_n]\n&= \\mathbb{E}\\mathbb{E}[y_n|\\beta] = \\mathbb{E}[H\\beta] = Hb \\newline\n\\text{Cov}[y_n]\n&= \\mathbb{E}[y_n y_n^\\top] - \\mathbb{E}[y_n]\\mathbb{E}[y_n]^\\top \\newline\n&= \\mathbb{E} \\mathbb{E}\\left[y_n y_n^\\top | \\beta\\right] - (Hb)(Hb)^\\top \\newline\n&= \\mathbb{E}\\left[\\text{Cov}[y_n|\\beta] + \\mathbb{E}[y_n|\\beta] \\mathbb{E}[y_n|\\beta]^\\top \\right] - Hbb^\\top H^\\top \\newline\n&= \\mathbb{E}\\left[C_{\\phi, \\sigma^2} + (H\\beta)(H\\beta)^\\top \\right] - Hbb^\\top H^\\top \\newline\n&= C_{\\phi, \\sigma^2} + H\\mathbb{E}\\left[\\beta \\beta^\\top \\right]H^\\top - Hbb^\\top H^\\top \\newline\n&= C_{\\phi, \\sigma^2} + H\\left[B + bb^\\top \\right]H^\\top - Hbb^\\top H^\\top \\newline\n&= C_{\\phi, \\sigma^2} + HBH^\\top,\n\\end{align}\n$$\nwhere we have used the law of total expectation and the various equivalent\ndefinitions for the covariance matrix. To summarize, we have found that the\nabove hierarchical model implies the marginal distribution\n$$\n\\begin{align}\ny_n &\\sim \\mathcal{N}\\left(Hb, C_{\\phi, \\sigma^2} + HBH^\\top \\right).\n\\end{align}\n$$\nSince this holds for any set of inputs, we obtain the analogous result for the\nGP prior:\n$$\n\\begin{align}\ny(x) &= f(x) + \\epsilon(x) \\newline\nf &\\sim \\mathcal{GP}\\left(\\mu^\\prime, k^\\prime \\right) \\newline\n\\epsilon(x) &\\overset{iid}{\\sim} \\mathcal{N}(0, \\sigma^2),\n\\end{align}\n$$\nwhere\n$$\n\\begin{align}\n\\mu^\\prime(x) &= h(x)^\\top b \\newline\nk^\\prime(x_1, x_2) &= k(x_1, x_2) + h(x_1)^\\top B h(x_2).\n\\end{align}\n$$\nAfter marginalizing, we again end up with a mean function that is linear in the\nbasis functions $h(\\cdot)$. The basis function coefficients are now given by\nthe prior mean $b$. The mean $b$ is something that we can prescribe, or we could\nagain entertain an empirical Bayes approach to set its value. Note that we have\ndescended another step in the hierarchical ladder. The kernel that appears from\nthe marginalization is now a sum of two kernels: the original kernel $k$ and\nthe kernel $h(x_1)^\\top B h(x_2)$. The latter can be viewed as a linear kernel\nin the transformed inputs $h(x_1)$, $h(x_2)$ and weighted by the positive\ndefinite matrix $B$. It serves to account for the uncertainty in the coefficients\nof the mean function.\n\n## Special Case Closed-Form Solutions: Marginal Variance\nWe now consider a closed-form plug-in estimate for the marginal variance\n$\\alpha^2$, as mentioned in (12). The takeaway from this section will be that\na closed-form estimate is only available when the covariance matrix appearing\nin the marginal likelihood (16) is of the form\n$$\n\\begin{align}\nC_{\\phi} &= \\alpha^2 C. \\tag{24}\n\\end{align}\n$$\nThis holds for any kernel of the form $\\alpha^2 k(\\cdot, \\cdot)$ provided\nthat $\\sigma^2 = 0$. For example, the exponentiated quadratic kernel in\n(12) satisfies this requirement.\nWith this assumption, the marginal likelihood is given by\n$$\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log\\left(2\\pi \\alpha^2 \\right) - \\frac{1}{2}\\log\\text{det}(C) -\n\\frac{1}{2\\alpha^2} (y_n - \\mu_{\\psi})^\\top C^{-1} (y_n - \\mu_{\\psi}). \\tag{25}\n\\end{align}\n$$\nThe analytical derivations given below go through for a log marginal likelihood\nof this form. However, this doesn't work for the common setting with an observation\nvariance $\\sigma^2 > 0$, since in this case the covariance assumes\nthe form\n$$\n\\begin{align}\nC &= \\left(\\alpha^2 k(X) + \\sigma^2 I_n \\right).\n\\end{align}\n$$\nThis can be addressed via the simple reparameterization\n$$\n\\begin{align}\n\\tilde{\\alpha}^2 C &:= \\tilde{\\alpha}^2\\left(k(X) + \\tilde{\\sigma}^2 I_n \\right).\n\\end{align}\n$$\nThis gives the required form of the covariance, and maintains the same number\nof parameters as before. The one downside is that we lose the straightforward\ninterpretation of the noise variance; the observation noise is now given by\nthe product $\\tilde{\\alpha}^2 \\tilde{\\sigma}^2$ instead of being encoded in\nthe single parameter $\\sigma^2$. This\nreparameterization is utilized in the R package [hetGP](https://cran.r-project.org/package=hetGP).\n\n\n### Plug-In MLE\nLet's consider optimizing the log marginal likelihood with respect to $\\alpha^2$.\nThe partial derivative of (25) with respect to $\\alpha^2$ is given by\n$$\n\\begin{align}\n\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\alpha^2}\n&= -\\frac{n}{2}\\frac{2\\pi}{2\\pi \\alpha^2} - \\frac{(y_n - \\mu_{\\psi})^\\top C^{-1} (y_n - \\mu_{\\psi})}{2\\alpha^4} \\newline\n&= -\\frac{n}{2\\alpha^2} - \\frac{(y_n - \\mu_{\\psi})^\\top C^{-1} (y_n - \\mu_{\\psi})}{2\\alpha^4}.\n\\end{align}\n$$\nSetting this expression equal to zero and solving for $\\alpha^2$ yields\n$$\n\\begin{align}\n\\hat{\\alpha}^2 &= \\frac{(y_n - \\mu_{\\psi})^\\top C^{-1} (y_n - \\mu_{\\psi})}{n}.\n\\end{align}\n$$\nFollowing the same procedure as before, the estimate $\\hat{\\alpha}^2$ can be\nsubbed in for $\\alpha^2$ in $\\mathcal{L}(\\theta)$ to obtain the\nconcentrated log marginal likelihood.\n\n### Closed-Form Marginalization\n\n\n## Bias Corrections\n\n# Bayesian Approaches\n\n# Computational Considerations\n## Log Marginal Likelihood\nWe start by considering the computation of the log marginal likelihood (16),\n$$\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log(2\\pi) -\\frac{1}{2} \\log \\text{det}\\left(C \\right) -\n\\frac{1}{2} (y - \\mu)^\\top C^{-1} (y - \\mu),\n\\end{align}\n$$\nwhere we now suppress all dependence on hyperparameters in the notation for\nsuccinctness.\nSince $C = k(X) + \\sigma^2 I_n$ is positive definite, we may Cholesky decompose it as\n$C = L L^\\top$. Plugging this decomposition into the log marginal likelihood yields\n$$\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log(2\\pi) - \\frac{1}{2} \\log\\text{det}\\left(C\\right) -\n\\frac{1}{2} (y_n - \\mu)^\\top \\left(LL^\\top \\right)^{-1} (y_n - \\mu).\n\\end{align}\n$$\nThe log determinant and the quadratic term can both be conveniently written in terms\nof the Cholesky factor. These terms are given respectively by\n$$\n\\begin{align}\n\\log\\text{det}\\left(LL^\\top\\right)\n&= \\log\\text{det}\\left(L\\right)^2\n= 2 \\log \\prod_{i=1}^{n} L_{ii}\n= 2 \\sum_{i=1}^{n} \\log\\left(L_{ii} \\right),\n\\end{align}\n$$\nand\n$$\n\\begin{align}\n(y_n - \\mu)^\\top \\left(LL^\\top \\right)^{-1} (y_n - \\mu)\n&= (y_n - \\mu)^\\top \\left(L^{-1}\\right)^\\top L^{-1} (y_n - \\mu)\n= \\lVert L^{-1}(y - \\mu)\\rVert_2^2.\n\\end{align}\n$$\nThe linear solve $L^{-1}(y - \\mu)$ can be computed in $\\mathcal{O}(n^2)$ by\nexploiting the fact that the linear system has lower triangular structure.\nPlugging these terms back into the log marginal\nlikelihood gives\n$$\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log(2\\pi) - \\sum_{i=1}^{n} \\log\\left(L_{ii}\\right) -\n\\frac{1}{2} \\lVert L^{-1}(y - \\mu)\\rVert_2^2.\n\\end{align}\n$$\nNote that the Cholesky factor $L$ is a function of $\\phi$ and $\\sigma^2$ and hence\nmust be re-computed whenever the kernel hyperparameters or noise variances\nchange.\n\n## Profile Log Marginal Likelihood with Linear Mean Function\nWe now consider computation of the concentrated marginal log-likelihood under\na mean function of the form (11), $\\mu(x) = h(x)^\\top \\beta$, where the generalized\nleast squares (GLS) estimator $\\hat{\\beta} = \\left(H^\\top C^{-1} H\\right)^{-1} H^\\top C^{-1}y$\n(see (23)) is inserted in place of $\\beta$. We are thus considering the profile log\nmarginal likelihood\n$$\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log(2\\pi) -\\frac{1}{2} \\log \\text{det}\\left(C \\right) -\n\\frac{1}{2} (y - H\\hat{\\beta})^\\top C^{-1} (y - H\\hat{\\beta}).\n\\end{align}\n$$\nWe will derive a numerically stable implementation of this expression in two steps,\nfirst applying a Cholesky decomposition (as in the previous section), and then\nleveraging a QR decomposition as in a typical ordinary least squares (OLS)\ncomputation. We first write $\\hat{\\beta}$ in terms of the Cholesky factor $L$,\nwhere $C = LL^\\top$:\n$$\n\\begin{align}\n\\hat{\\beta}\n&= \\left(H^\\top C^{-1} H\\right)^{-1} H^\\top C^{-1}y \\newline\n&= \\left(H^\\top \\left[LL^\\top\\right]^{-1} H\\right)^{-1} H^\\top \\left[LL^\\top\\right]^{-1}y \\newline\n&= \\left(\\left[L^{-1}H \\right]^\\top \\left[L^{-1}H \\right] \\right)^{-1} \\left[L^{-1}H \\right]^\\top\n\\left[L^{-1}y\\right].\n\\end{align}\n$$\nNotice that the GLS computation boils down to two lower-triangular linear solves:\n$L^{-1}H$ and $L^{-1}y$. However, the above expression still requires one non-triangular\nlinear solve that we will now address via the QR decomposition. The above expression\nfor $\\hat{\\beta}$ can be viewed as a standard OLS estimator with design matrix\n$L^{-1}H$ and response vector $L^{-1}y$. At this point, we could adopt a standard\nOLS technique of taking the QR decomposition of the design matrix $L^{-1}H$.\nThis was my original thought, but I found a nice alternative looking through the\ncode in the R [kergp](https://github.com/cran/kergp/blob/master/R/logLikFuns.R)\npackage (see the function `.logLikFun0` in the file `kergp/R/logLikFuns.R`). The approach\nis to compute the QR decomposition\n$$\n\\begin{align}\n\\begin{bmatrix} L^{-1}H & L^{-1}y \\end{bmatrix} &= QR = Q \\begin{bmatrix} \\tilde{R} & r \\end{bmatrix}.\n\\end{align}\n$$\nThat is, we compute QR on the matrix formed by concatenating $L^{-1}y$ as an additional\ncolumn on the design matrix $L^{-1}H$. We have written the upper triangular matrix\n$R \\in \\mathbb{R}^{(p+1) \\times (p+1)}$ as the concatenation of\n$\\tilde{R} \\in \\mathbb{R}^{(p+1) \\times p}$ and the vector\n$r \\in \\mathbb{R}^{p+1}$ so that $L^{-1}H = Q\\tilde{R}$ and $L^{-1}y = Qr$.\nWe recall the basic properties of the QR decomposition: $R$ is upper triangular\nand invertible, and $Q$ has orthonormal columns with span equal to the column space\nof $\\begin{bmatrix} L^{-1}H & L^{-1}y \\end{bmatrix}$. Taking the QR decomposition\nof this concatenated matrix leads to a very nice expression for the quadratic\nform term of the profile log marginal likelihood. But first let's rewrite $\\hat{\\beta}$\nin terms of these QR factors:\n$$\n\\begin{align}\n\\hat{\\beta}\n&= \\left(\\left[L^{-1}H \\right]^\\top \\left[L^{-1}H \\right] \\right)^{-1} \\left[L^{-1}H \\right]^\\top\n\\left[L^{-1}y\\right] \\newline\n&= \\left(\\left[Q\\tilde{R} \\right]^\\top \\left[Q\\tilde{R} \\right] \\right)^{-1} \\left[Q\\tilde{R} \\right]^\\top \\left[Qr\\right] \\newline\n&= \\left(\\tilde{R}^\\top Q^\\top Q\\tilde{R} \\right)^{-1} \\tilde{R}^\\top Q^\\top Qr \\newline\n&= \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top r,\n\\end{align}\n$$\nwhere we have used the fact that $Q^\\top Q$ is the identity since $Q$ is orthogonal.\nPlugging this into the quadratic form term of the log likelihood gives\n$$\n\\begin{align}\n(y - H\\hat{\\beta})^\\top C^{-1} (y - H\\hat{\\beta})\n&= (y - H\\hat{\\beta})^\\top \\left[LL^\\top \\right]^{-1} (y - H\\hat{\\beta}) \\newline\n&= \\lVert L^{-1}(y - H\\hat{\\beta}) \\rVert_2^2 \\newline\n&= \\lVert L^{-1}y - L^{-1}H\\hat{\\beta} \\rVert_2^2 \\newline\n&= \\lVert Qr - Q\\tilde{R} \\hat{\\beta} \\rVert_2^2 \\newline\n&= \\left\\lVert Qr - Q\\tilde{R} \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top r \\right\\rVert_2^2 \\newline\n&= \\left\\lVert Q\\left[r - \\tilde{R} \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top r \\right] \\right\\rVert_2^2 \\newline\n&= \\left\\lVert r - \\tilde{R} \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top \\right\\rVert_2^2 \\newline\n&= \\left\\lVert \\left[I - \\tilde{R} \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top \\right]r \\right\\rVert_2^2,\n\\end{align}\n$$\nwhere the penultimate line follows from the fact that $Q$ is orthogonal, and hence an\nisometry. At this point, notice that the matrix\n$P := \\tilde{R} \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top$ is the standard OLS projection matrix (i.e., hat matrix)\nconstructed with the design matrix $\\tilde{R}$. Also, take care to notice that\n$\\tilde{R}$ is not invertible (it is not even square). Using standard properties\nof the projection matrix, we know that $P$ has rank $p$, since $\\tilde{R}$ has rank $p$.\nAlso, since $R$ is upper triangular, then the last row of $\\tilde{R}$ contains all zeros.\nLetting, $e_j$ denote the $j^{\\text{th}}$ standard basis vector of $\\mathbb{R}^{p+1}$,\nthis means that\n$$\n\\begin{align}\n\\mathcal{R}(P) \\perp \\text{span}(e_{p+1}),\n\\end{align}\n$$\nwhere $\\mathcal{R}(P)$ denotes the range (i.e., column space) of $P$.\nThe only subspace of $\\mathbb{R}^{p+1}$ with rank $p$ and satisfying this property\nis $\\text{span}(e_1, \\dots, e_p)$. The conclusion is that $P$ projects onto $\\text{span}(e_1, \\dots, e_p)$, and thus the annihilator $I - P$ projects onto\nthe orthogonal complement $\\text{span}(e_{p+1})$. We thus conclude,\n$$\n\\begin{align}\n\\left\\lVert \\left[I - P\\right]r \\right\\rVert_2^2\n&= \\lVert \\langle r, e_{p+1} \\rangle e_{p+1} \\rVert_2^2 \\newline\n&= \\lVert r_{p+1} e_{p+1} \\rVert_2^2 \\newline\n&= r_{p+1}^2,\n\\end{align}\n$$\nwhere $r_{p+1}$ is the last entry of $r$; i.e., the bottom right entry of $R$.\nWe finally arrive at the expression for the concentrated log marginal likelihood\n$$\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log(2\\pi) - \\sum_{i=1}^{n} \\log\\left(L_{ii}\\right) -\n\\frac{1}{2} r_{p+1}^2.\n\\end{align}\n$$\n\n# References\n- Surrogates (Gramacy)\n- Statistics or geostatistics? Sampling error or nugget effect? (Clark)\n- Michael Betencourt's very nice [post](https://betanalpha.github.io/assets/case_studies/gp_part3/part3.html#4_adding_an_informative_prior_for_the_length_scale) on setting\npriors on GP hyperparameters.\n- Large-Scale Spatial Modeling via Local Approximate Gaussian Processes in R.\n(nice discussion of setting ranges for hyperparameters in the appendix; see\nlaGP function `darg`).\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"gp-specifications.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","theme":["cosmo","brand"],"title-block-banner":true,"title":"Introduction to Gaussian Process Priors and Hyperparameter Estimation","subtitle":"A deep dive into hyperparameter specifications for GP mean and covariance functions, including both frequentist and Bayesian methods for hyperparameter estimation.","date":"2024-01-11","categories":["Statistics","Gaussian-Process","kernel-methods"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}