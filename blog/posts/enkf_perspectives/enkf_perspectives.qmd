---
title: "Several Perspectives on the Ensemble Kalman Filter"
subtitle: TODO
description: TODO.
layout: default
date: 2025-10-03
draft: false
categories: [Inverse-Problem, Data-Assimilation, Optimization, Sampling, Computational Statistics]
format:
  html:
    css: ../../styles.css
    number-sections: true
    number-depth: 3
    fig-cap-location: top
    math:
      method: mathjax
---

<div class="hidden-macros">
$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Pr}{\mathbb{P}}
\newcommand{\given}{\mid}
\newcommand{\Def}{:=}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Gaussian}{\mathcal{N}}
\newcommand{\x}{x}
\newcommand{\y}{y}
\newcommand{\h}{h}
\newcommand{\H}{\mathsf{H}}
\newcommand{\yobs}{y^{\dagger}}
\newcommand{\noise}{\epsilon}
\newcommand{\covNoise}{\Sigma}
\newcommand{\m}{m}
\newcommand{\C}{C}
\newcommand{\Cyx}{\C_{\y\x}}
\newcommand{\Cxy}{\C_{\x\y}}
\newcommand{\Cy}{\C_{\y}}
\newcommand{\my}{m_y}
\newcommand{\prior}{\pi}
\newcommand{\priorEmp}{\pi_J}
\newcommand{\post}{\pi^\star}
\newcommand{\dimObs}{n}
\newcommand{\dimPar}{d}
\newcommand{\map}{\mathsf{T}}
\newcommand{\proj}{\mathcal{P}_{\Gaussian}}
$$
</div>

The ensemble Kalman filter (EnKF) is a family of algorithms that perform
approximate inference in probabilistic state space models. The crux of this
inference task is the requirement of solving a sequence of nonlinear,
potentially high-dimensional, inverse problems. To approximate the solutions
to these inverse problems, EnKF algorithms employ Monte Carlo methods
rooted in the linear Gaussian methodology of the Kalman filter.
In this post, we take a deep dive into this methodology, viewing the
EnKF approximations from several different perspectives. Our goal is not to
focus on the algorithmic aspects of state space inference (I cover this in
other posts), but rather to strip away unnecessary details to really
understand the approximations driving the EnKF.

# The Core Inverse Problem

The celebrated "EnKF update equations" describe how to update a model's
forecast distribution (represented via samples) based on the information
in newly observed data. This act of "assimilating" the data point is often
referred to as the *analysis step* of the EnKF. From a generic perspective, the
analysis step requires approximating the solution of an inverse problem.

::: {.callout-note title="Problem Setup"}
Consider the joint probability model over an unobserved variable $x \in \R^\dimPar$
and an observable $\y \in \R^\dimObs$ given by

$$
\begin{align}
&\y = \h(\x) + \noise,
&&\noise \sim \Gaussian(0, \covNoise) \\
&\x \sim \prior,
\end{align}
$$ {#eq-model}

where $\x$ and $\noise$ are a priori independent and $\covNoise \in \R^{n \times n}$
is a known positive definite matrix.

In contrast to standard inverse problem formulations, we assume that the prior
$\prior$ is only accessible via a finite set of samples $\{x_j\}_{j=1}^{J}$.
Given a particular data realization $\yobs$, our goal is to produce samples
$\{\x^\star_j\}_{j=1}^{J}$ that are (approximately) distributed according to
the posterior distribution $\post \Def \mathrm{law}(\x \given \y = \yobs)$.
:::

We are interested in developing posterior
inference methods that are fast and scalable to settings where the parameter
dimension $\dimPar$ may be quite large. Standard inference algorithms like
Markov chain Monte Carlo (MCMC) are not well-suited to these constraints.
In particular, a key challenge here is the lack of access to the prior density.
Instead, we must make do with the empirical approximation
$$
\priorEmp \Def \frac{1}{J} \sum_{j=1}^{J} \delta_{\x_j},
$$ {#eq-empirical-prior}
where $\delta_{\x_j}$ denotes the Dirac delta centered at $\x_j$.

::: {.callout-note title="Notation"}
We utilize the notation $\my \Def \E(\y)$,
$\Cy \Def \Cov(\y)$, $\Cxy \Def \Cov(\x,\y)$, and $\Cyx \Def \Cxy^\top$
for the first two moments of $\x$ and $\y$ under the probability distribution
implied by @eq-model.
:::

# Linear Gaussian Inverse Problems

When the observation operator $\h$ is linear and the prior is Gaussian, the
inverse problem in @eq-model can be solved analytically. The results from
this linear Gaussian setting form the foundation for methods that address
the more challenging general setting. See this
[post](https://arob5.github.io/blog/posts/linear-Gaussian/linear_Gaussian_inverse_problems.html)
for a detailed exploration of linear Gaussian inverse problems.
For our present purposes, the following result will suffice.

::: {.callout-note title="Linear Gaussian Conditional"}
The linear Gaussian inverse problem

$$
\begin{align}
&\y = \H\x + \noise,
&&\noise \sim \Gaussian(0, \covNoise) \\
&\x \sim \Gaussian(m,C)
\end{align}
$$ {#eq-linear-Gaussian}

admits a Gaussian conditional $\x \given [\y = \yobs] \sim \Gaussian(m^\star, C^\star)$.
The conditional moments are given by
$$
\begin{align}
m^\star &= m + K(\yobs - \my) \\
C^\star &= C - K \Cyx,
\end{align}
$$ {#eq-conditional-moments}
where
$$
K = \Cxy \Cy^{-1} = C \H^\top (\H C \H^\top + \covNoise)^{-1}.
$$ {#eq-Kalman-gain}

The matrix $K \in \R^{\dimPar \times \dimObs}$ is known as the **Kalman gain**.
:::

# Linear Ensemble Transforms
We now return to the general problem in @eq-model, which is made challenging
by the nonlinear observation operator and empirical nature of the prior.
There are many approaches to deal with the latter issue.
For example, a kernel density estimate can be employed to smooth $\priorEmp$,
yielding a continuous prior approximation.
Alternatively, importance sampling techniques work directly with
$\{x_j\}_{j=1}^{J}$, re-weighting the particles to achieve a sample-based
approximation of the posterior $\post$. However, both of these methods
are known to degenerate as the parameter dimension $\dimPar$ increases.

This post focuses on methods that produce approximate samples from $\post$
by applying a map $\map: \R^\dimPar \to \R^\dimPar$ to each prior sample; that is,
$$
x_j^\star \Def \map(x_j), \qquad j = 1, 2, \dots, J.
$$
The function $\map$ is sometimes called a *transport map*, and may be deterministic
or stochastic. In particular, we focus on linear transport maps, yielding a
family of algorithms known as *linear ensemble transforms*. Ensemble Kalman
transforms construct linear transport maps $\map$ that depend on the empirical
distribution $\priorEmp$ only through its first two moments, the sample mean
and covariance:
$$
\begin{align}
\hat{m} &\Def \frac{1}{J} \sum_{j=1}^{J} x_j \\
\hat{C} &\Def \frac{1}{J-1} \sum_{j=1}^{J} (x_j - \hat{m})(x_j - \hat{m})^\top.
\end{align}
$$ {#eq-moments}

# Gaussian Prior Approximation
We first explore a perspective on the EnKF rooted in the idea of estimating
a parametric prior approximation using $\{x_j\}_{j=1}^{J}$. While different
distributional families can be used, Ensemble Kalman methods employ a Gaussian
approximation. Given a square-integrable distribution $\nu$, let $\proj \nu$
denote the Gaussian "projection" of $\nu$; that is, the Gaussian distribution
that matches the mean and covariance of $\nu$. Given this definition, the
Gaussian prior approximation employed by the EnKF can be phrased as follows.

::: {.callout-note title="Gaussian Prior Approximation"}
Given the setup in @eq-model, the EnKF uses the samples $\{x_j\}_{j=1}^{J}$
to fit a Gaussian prior approximation
$$
\hat{\prior} \Def \proj \priorEmp = \Gaussian(\hat{m}, \hat{C}),
$$ {#eq-Gaussian-prior}
where $\hat{m},\hat{C}$ are the empirical mean and covariance, as defined in @eq-moments.
:::

The parametric approximation has a regularizing effect in high-dimensions, and
the choice of a Gaussian distribution allows for convenient analytical expressions.
This is particularly true when the observation operator $\h$ is linear, in which
case the problem reduces to the linear Gaussian setting.
