<html>
<head>
    <title>Linear Gaussian Inverse Problems</title>
    <meta charset='UTF-8'>
    <meta content='width=device-width, initial-scale=1' name='viewport'/>

    <meta name='description' content='Andrew Roberts is a PhD student at Boston University.'>
    <meta name='keywords' content='statistics'>
    <meta name='author' content='Andrew Roberts'>

    <link href='/css/blog.css' rel='stylesheet'/>
    <link href='/css/trac.css' rel='stylesheet'/>
    <link href='/css/markdown.css' rel='stylesheet'/>

    <script type='text/x-mathjax-config'>
MathJax.Hub.Config({
  jax: ['input/TeX', 'output/HTML-CSS'],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    extensions: ['color.js']
  },
  messageStyle: 'none',
  'HTML-CSS': { preferredFont: 'TeX', availableFonts: ['STIX','TeX'] }
});
</script>

<script src='//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML' type='text/javascript'></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
</head>
<body>
<div class='content'>
    <div class='nav'>
    <ul class='wrap'>
        <li><a href='/'>Home</a></li>
        <li><a href='/blog'>Blog</a></li>
        <li><a href='/feed.xml'>RSS</a></li>
    </ul>
</div>

    <div class='front-matter'>
        <div class='wrap'>
            <h1>Linear Gaussian Inverse Problems</h1>
            <h4>Derivations and discussion of linear Gaussian inverse problems.</h4>
            <div class='bylines'>
                <div class='byline'>
                    <h3>Published</h3>
                    <p>03 July 2024</p>
                </div>
            </div>
            <div class='clear'></div>
        </div>
    </div>
    <div class='wrap article'>
        <p>This post focuses on Bayesian inverse problems with the following features:</p>
<ul>
  <li>Linear forward model.</li>
  <li>Additive Gaussian observation noise.</li>
  <li>Gaussian prior distribution.</li>
  <li>Prior independence of the observation noise and prior.<br />
We refer to such inverse problems as <em>linear Gaussian</em>. The typical Bayesian
linear regression model with a Gaussian prior on the coefficients constitutes
a common example of a linear Gaussian inverse problem. The assumptions of
linearity and Gaussianity play quite nicely together, resulting in a closed-form
Gaussian posterior distribution. Moreover, many extensions to nonlinear and/or
non-Gaussian settings rely on methods rooted in our understanding of the linear
Gaussian regime.</li>
</ul>

<h1 id="setup">Setup</h1>
<p>We consider the following linear Gaussian regression model
\begin{align}
y &amp;= Gu + \epsilon \tag{1} \newline
\epsilon &amp;\sim \mathcal{N}(0, \Sigma) \newline
u &amp;\sim \mathcal{N}(m, C), &amp;&amp; u \perp \epsilon
\end{align}
consisting of the <em>observation</em> (or <em>data</em>) $y \in \mathbb{R}^n$, <em>parameter</em> $u \in \mathbb{R}^d$,
<em>noise</em> $\epsilon \in \mathbb{R}^n$, and linear <em>forward model</em> represented by the
matrix $G \in \mathbb{R}^{n \times d}$. The <em>observation covariance</em>
$\Sigma \in \mathbb{R}^{n \times n}$ and <em>prior covariance</em> $u \in \mathbb{R}^{d \times d}$
are both fixed positive definite matrices. The vector $m \in \mathbb{R}^d$ is the
<em>prior mean</em>. We write $u \perp \epsilon$ to indicate the key assumption that
$u$ and $\epsilon$ are a priori statistically independent. The model (1) can
equivalently be written as
\begin{align}
y|u &amp;\sim \mathcal{N}(Gu, \Sigma) \tag{2} \newline
u &amp;\sim \mathcal{N}(m, C),
\end{align}
which gives the explicit expression for the Gaussian likelihood $p(y|u)$.
The solution of the Bayesian inverse problem is the posterior distribution
$p(u|y)$. We provide two approaches to calculating this distribution below, which
yield different (but equivalent) expressions.</p>

<h1 id="computing-the-posterior">Computing the Posterior.</h1>
<h2 id="method-1-completing-the-square">Method 1: Completing the Square</h2>
<p>We first tackle the problem directly, using Bayes’ theorem and
the matrix analog of completing the square from elementary algebra. Applying
Bayes’ theorem to (2) yields
\begin{align}
p(u|y)
&amp;\propto p(y|u)p(u) \newline
&amp;\propto \exp\left(-\frac{1}{2}\left[(y-Gu)^\top \Sigma^{-1} (y-Gu) + (u-m)^\top C^{-1} (u-m) \right] \right) \newline
&amp;\propto \exp\left(-\frac{1}{2}\left[u^\top(G^\top \Sigma^{-1}G + C^{-1})u -
2u^\top(G^\top \Sigma^{-1}y + C^{-1}m)\right] \right). \tag{3}
\end{align}
All we have done above is to combine the Gaussian likelihood and prior, dropping
any multiplicative constants that don’t depend on $u$, and grouping like terms
in $u$. Note that since (3) is an exponential of a quadratic in $u$, then we
immediately know that the posterior must be Gaussian. It therefore remains to find
the mean $\overline{m}$ and covariance $\overline{C}$. Knowing that (3) is
proportional to a Gaussian density, let’s set the term in square brackets equal to<br />
\begin{align}
(u - \overline{m})^\top \overline{C}^{-1} (u - \overline{m})
= u^\top \overline{C}^{-1}u - 2u^\top \overline{C}^{-1} \overline{m} +
\overline{m}^\top \overline{C}^{-1} \overline{m}
\end{align}
and equate like terms to solve for the unknowns $\overline{m}$ and $\overline{C}$.
Doing so, we find that
\begin{align}
\overline{C}^{-1} &amp;= G^\top \Sigma^{-1} G + C^{-1} \newline
\overline{C}^{-1}\overline{m} &amp;= G^\top \Sigma^{-1}y + C^{-1}m.
\end{align}
The $\overline{m}^\top \overline{C}^{-1} \overline{m}$ is not a problem, as it
will simply be absorbed in the proportionality sign.
Rearranging the above expressions gives the desired mean and covariance equations,
which are summarized in the following result.</p>

<blockquote>
  <p><strong>Proposition.</strong>
  The posterior distribution under the linear Gaussian model (1) is Gaussian
  $u|y \sim \mathcal{N}(\overline{m}, \overline{C})$, with
  \begin{align}
  \overline{m} &amp;= \overline{C} \left[G^\top \Sigma^{-1}y + C^{-1}m \right] \tag{4} \newline
  \overline{C} &amp;= \left[G^\top \Sigma^{-1} G + C^{-1} \right]^{-1}.
  \end{align}
  </p>
</blockquote>

<h2 id="method-2-joint-gaussian-conditioning">Method 2: Joint Gaussian Conditioning</h2>
<p>We now present a second method for computing $p(u|y)$. This approach relies on
the observation that the vector $(u, y)^\top \in \mathbb{R}^{d+n}$ has a
joint Gaussian distribution. This follows from the prior independence of $u$
and $\epsilon$, and is formally proved in the appendix. Writing out this joint
Gaussian explicitly gives
\begin{align}
\begin{bmatrix} u \newline y \end{bmatrix}
&amp;\sim \mathcal{N}\left(
\begin{bmatrix} m \newline Gm \end{bmatrix},
\begin{bmatrix} C &amp; CG^\top \newline GC &amp; GCG^\top + \Sigma \end{bmatrix}
\right). \tag{5}
\end{align}
The mean and covariance of $u$ is immediate from (1),
and the remaining quantities are computed as:
\begin{align}
\mathbb{E}[y] &amp;= \mathbb{E}[Gu + \epsilon]
= G\mathbb{E}[u] + \mathbb{E}[\epsilon] = Gm \tag{6} \newline
\text{Cov}[y] &amp;= \text{Cov}[Gu + \epsilon]
= \text{Cov}[Gu] + \text{Cov}[\epsilon] = GCG^\top + \Sigma \tag{7} \newline
\text{Cov}[y, u] &amp;= \text{Cov}[Gu + \epsilon, u]
= \text{Cov}[Gu, u] + \text{Cov}[\epsilon, u]
= GC. \tag{8}
\end{align}
In (6) we use the linearity of expectation and the fact that the noise is zero-mean.
The covariance splits into the sum in (7) due to the independence of $u$ and
$\epsilon$. This independence assumption is similarly leveraged in (8).</p>

<p>The conditional distributions of joint Gaussians are well-known to also be
Gaussian, and can be computed in closed-form. Applying these Gaussian conditioning
identities to (5) provides expressions for the posterior distribution $u|y$,
which is summarized in the following result.</p>
<blockquote>
  <p><strong>Proposition.</strong>
  The posterior distribution under the linear Gaussian model (1) is Gaussian
  $u|y \sim \mathcal{N}(\overline{m}, \overline{C})$, with
  \begin{align}
  \overline{m} &amp;= m + CG^\top [GCG^\top + \Sigma]^{-1}(y - Gm) \tag{9} \newline
  \overline{C} &amp;= C - CG^\top [GCG^\top + \Sigma]^{-1} GC.
  \end{align}
  </p>
</blockquote>

<h2 id="equivalence-of-the-two-approaches">Equivalence of the Two Approaches</h2>
<p>TODO</p>

<h1 id="investigating-the-posterior-equations">Investigating the Posterior Equations</h1>

<h2 id="the-posterior-covariance">The Posterior Covariance</h2>
<p>A first important observation is that the the posterior covariance
$\overline{C}$ is independent of the data $y$. In this sense, the specific
data realization observed does not affect the uncertainty in the estimation
of $u$. The expression coming from the first derivation (4) tells us that the
posterior <em>precision</em> (inverse covariance) $\overline{C}^{-1}$ is the sum of
the prior precision $C^{-1}$ and $G^\top \Sigma^{-1}G$, which
is the observation precision $\Sigma^{-1}$ modified by the forward model. Since
the posterior covariance is the inverse of $G^\top \Sigma^{-1}G + C^{-1}$,
we should verify that this matrix is indeed invertible. First, note that
$\Sigma^{-1}$ and $C^{-1}$ are both positive definite, since the
inverse of positive definite matrices are also positive definite. Thus,
the factorization $\Sigma^{-1} = SS^\top$ exists, which implies
\begin{align}
x^\top [G^\top \Sigma^{-1}G]x
&amp;= x^\top [G^\top SS^\top G]x
= \lVert S^\top Gx \rVert^2_2 \geq 0. \tag{10}
\end{align}
That is, $G^\top \Sigma^{-1}G$ is positive semidefinite. Since the sum of a
positive semidefinite and positive definite matrix is positive definite, then
$G^\top \Sigma^{-1}G + C^{-1}$ is positive definite, and thus invertible.</p>

<p>The covariance expression in (9) provides an alternative perspective. First,
the expression tells us that conditioning on the data $y$ always decreases
variance. This can be seen by noting that the matrix
$CG^\top [GCG^\top + \Sigma]^{-1} GC$ (which is subtracted from the prior
covariance) is positive semidefinite, and thus in particular has nonnegative values
on its diagonal. To show this, we use the fact that we have just proven that
$[GCG^\top + \Sigma]^{-1}$ is positive definite, and thus admits a
decomposition of the form $SS^\top$. Thus,
\begin{align}
x^\top \left(CG^\top [GCG^\top + \Sigma]^{-1} GC\right) x
= x^\top \left(CG^\top SS^\top GC\right) x
= \lVert S^\top GCx \rVert_2^2 \geq 0, \tag{11}
\end{align}
so $CG^\top [GCG^\top + \Sigma]^{-1} GC$ is indeed positive semidefinite. Note
that the covariance expression in (9) can also be written as
\begin{align}
\text{Cov}[u|y]
&amp;= \text{Cov}[u] - \text{Cov}[u,y] \text{Cov}[y]^{-1} \text{Cov}[y, u]. \tag{12}
\end{align}</p>

<ul>
  <li>TODO: show that $GCG^\top$ is the Hessian of the log-likelihood.</li>
  <li>Positive definiteness/invertibility.</li>
</ul>

<h1 id="posterior-predictive-distribution">Posterior Predictive Distribution</h1>
<p>Suppose that we are now interested in a new forward model
$\tilde{G} \in \mathbb{R}^{m \times d}$ and are interested in estimating the
unobserved quantity $\tilde{y} \in \mathbb{R}^m$ under the model
\begin{align}
\tilde{y} = \tilde{G}u + \tilde{\epsilon}, \tag{13}
\end{align}
with the same prior distribution as in (1). For the noise distribution, we will
assume
\begin{align}
\begin{bmatrix} \tilde{\epsilon} \newline \epsilon \end{bmatrix} \sim
\mathcal{N}\left(
\begin{bmatrix} 0 \newline 0 \end{bmatrix},
\begin{bmatrix} \tilde{\Sigma} &amp; \Sigma^\prime \newline
                [\Sigma^\prime]^\top &amp; \Sigma \end{bmatrix}
\right), \tag{14}
\end{align}
again with the assumption that $\tilde{\epsilon}$ and $u$ are a priori independent.
The classic example of
this setup is prediction at a new set of inputs within a regression setup. In
this scenario, $G$ would represent the <em>design matrix</em> containing the
$n$ inputs at which the responses $y$ were observed, while $\tilde{G}$
represents a new set of $m$ inputs whose associated responses we would like
to predict. The basic regression setting with homoscedastic variance would
result in $\Sigma = \sigma^2 I_n$, $\tilde{\Sigma} = \sigma^2 I_m$, and
$\Sigma^\prime = 0$.</p>

<p>In the generic inverse problem formulation, we see that this is
a question of propagating the posterior uncertainty in $u$ through a new
forward model $\tilde{G}$. Concretely, we are interested in characterizing the
<em>posterior predictive</em> distribution $p(\tilde{y} | y)$.
Just as with the posterior, we present two different ways to compute this
quantity.</p>

<h2 id="method-1-marginalization">Method 1: Marginalization</h2>
<p>The first method consists of viewing $p(\tilde{y} | y)$ as the marginal
distribution of $p(\tilde{y}, u| y)$. We have,
\begin{align}
p(\tilde{y} | y)
&amp;= \int p(\tilde{y}, u| y) du \newline
&amp;= \int p(\tilde{y}|u,y) p(u|y) du \newline
&amp;= \int p(\tilde{y}|u) p(u|y) du \newline
&amp;= \int \mathcal{N}\left(\tilde{y}|\tilde{G}u, \Sigma \right) \mathcal{N}\left(u|\overline{m}, \overline{C} \right) du
\end{align}</p>

<p>This integral is not too difficult to compute, but there is an easier approach.
Observe that $(u, \tilde{y}, y)$ is joint Gaussian distributed (this is verified
in the appendix). Therefore, $(u, \tilde{y})|y$ is also joint Gaussian. This
implies that $\tilde{y}|y$ is also Gaussian, since Gaussians have Gaussian
marginals. It thus remains to find the mean and covariance of this distribution.
To this end, we have
\begin{align}
\mathbb{E}[\tilde{y} | y] &amp;= \mathbb{E}[\tilde{G}u + \tilde{\epsilon} | y]
\end{align}</p>

<h2 id="method-2-joint-gaussian-conditioning-1">Method 2: Joint Gaussian Conditioning</h2>

<h1 id="example-linear-regression">Example: Linear Regression</h1>
<p>Perhaps the most common example of a linear Gaussian model takes the form of a
Bayesian linear regression model. In this setting, we suppose that we have access
to observed input-output pairs $(x_i, y_i)_{i=1}^{n}$ and assume that the $y_i$
arise as a linear function of the $x_i$, which are then perturbed by noise.
While different formulations are possible, a popular specification assumes
\begin{align}
y_i|x_i, \beta &amp;\sim \mathcal{N}(x_i^\top \beta, \sigma^2),
\end{align}
meaning that the magnitude of the observation noise is iid across observations.
If we stack the inputs row-wise into a matrix $X \in \mathbb{R}^{n \times d}$
and the outputs into a vector $y \in \mathbb{R}^n$, then this model can be written
as
\begin{align}
y &amp;= X\beta + \epsilon \newline
\epsilon &amp;\sim \mathcal{N}(0, \sigma^2 I_n) \newline
\beta &amp;\sim \mathcal{N}(m, \sigma^2 C),
\end{align}
where we have also assumed a Gaussian prior on $\beta$. Connecting to our generic
inverse problem setup, we see that the forward model
$G = X$ is given by the data matrix, while the parameter $u = \beta$ is the
coefficient vector. The parameterization of the prior covariance as $\sigma^2 C$ is
common in this setting, as it will lead to some convenient cancellations in the
posterior formulas. Indeed, applying (4) gives the posterior
$\beta|y \sim \mathcal{N}(\overline{m}, \overline{C})$, with covariance
\begin{align}
\overline{C} &amp;= \left[\frac{1}{\sigma^2} X^\top X + \frac{1}{\sigma^2} C^{-1} \right]^{-1}
= \sigma^2 \left[X^\top X + C^{-1} \right]^{-1}.
\end{align}
The posterior mean is thus
\begin{align}
\overline{m}
&amp;= \overline{C} \left[\frac{1}{\sigma^2} X^\top y + \frac{1}{\sigma^2} C^{-1}m \right]
= \left[X^\top X + C^{-1} \right]^{-1} \left[X^\top y + C^{-1}m \right],
\end{align}
since the $\sigma^2$ term from the covariance cancels with its reciprocal.</p>

<h1 id="marginal-prior">Marginal Prior</h1>
<p>TODO</p>

<h1 id="numerically-implementing-the-posterior-equations">Numerically Implementing the Posterior Equations</h1>

<h1 id="appendix">Appendix</h1>

<h2 id="joint-gaussian-distribution">Joint Gaussian Distribution</h2>
<p>Throughout this post we rely on the claim that various quantities are jointly
Gaussian distributed. We verify these claims here. In the proofs, we use the
fact that a random vector <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span> is Gaussian if and only if it is equal
in distribution to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>+</mo><mi>B</mi><mi>z</mi></mrow><annotation encoding="application/x-tex">a + Bz</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span></span>, for <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>∼</mo><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>I</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">z \sim \mathcal{N}(0, I)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.14736em;">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="mclose">)</span></span></span></span> and some constant
vector <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span></span></span></span> and matrix <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span></span>. Any random variables labelled “z” in this section
should be interpreted as standard Gaussians, with subscripts potentially
indicating the random variables that they generate; e.g., <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>ϵ</mi></msub></mrow><annotation encoding="application/x-tex">z_{\epsilon}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">ϵ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>.</p>

<h3 id="joint-distribution-uyu-yuy">Joint distribution: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>u</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(u, y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">u</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span>.</h3>
<p>We first verify that the vector <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>u</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(u, y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">u</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span> has a joint Gaussian distribution
under model (1). Taking the square roots of the covariance matrices allows us
to write the correlated Gaussian variables as transformations of iid Gaussian noise.
We have,
\begin{align}
\begin{bmatrix} u \newline y \end{bmatrix}
&amp;\overset{d}{=}
\begin{bmatrix} Gu + \epsilon \newline u \end{bmatrix} \newline
&amp;\overset{d}{=}
\begin{bmatrix} G\left(m + C^{1/2}z_u\right) + \Sigma^{1/2}z_{\epsilon} \newline m + C^{1/2}z_u \end{bmatrix} \newline
&amp;\overset{d}{=}
\begin{bmatrix} Gu \newline m \end{bmatrix} +
\begin{bmatrix} GC^{1/2}z_u + \Sigma^{1/2}z_{\epsilon} \newline
                 C^{1/2}z_u \end{bmatrix} \newline
&amp;\overset{d}{=}
\begin{bmatrix} Gu \newline m \end{bmatrix} +
\begin{bmatrix} GC^{1/2} &amp; \Sigma^{1/2} \newline C^{1/2} &amp; 0 \end{bmatrix}
\begin{bmatrix} z_u \newline z_{\epsilon} \end{bmatrix}.  <br />
\end{align}<br />
Under the assumption <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>⊥</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">u \perp \epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">u</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">⊥</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">ϵ</span></span></span></span> it follows that
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>u</mi></msub></mrow><annotation encoding="application/x-tex">z_u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">u</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>ϵ</mi></msub></mrow><annotation encoding="application/x-tex">z_{\epsilon}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">ϵ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> are independent,
so <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>z</mi><mi>u</mi></msub><mo separator="true">,</mo><msub><mi>z</mi><mi>ϵ</mi></msub><msup><mo stretchy="false">)</mo><mi mathvariant="normal">⊤</mi></msup><mo>∼</mo><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><msub><mi>I</mi><mrow><mi>d</mi><mo>+</mo><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(z_u, z_{\epsilon})^\top \sim \mathcal{N}(0, I_{d+n})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.099108em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">u</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">ϵ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.14736em;">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mbin mtight">+</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>, thus verifying the
claim.</p>

<h3 id="joint-distribution-yytildey-yyy">Joint distribution: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>~</mo></mover><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\tilde{y}, y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6678599999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3.35em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">~</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span>.</h3>
<p>We similarly show that <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>~</mo></mover><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\tilde{y}, y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6678599999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3.35em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">~</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span> is joint Gaussian, under model
(13). This fact is used in the derivation of the posterior predictive
distribution. We recall that we are allowing the noise terms
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">ϵ</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>ϵ</mi><mo>~</mo></mover></mrow><annotation encoding="application/x-tex">\tilde{\epsilon}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6678599999999999em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6678599999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">ϵ</span></span></span><span style="top:-3.35em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">~</span></span></span></span></span></span></span></span></span></span> to be correlated; let’s partition the square
root of their joint covariance by
\begin{align}
\text{Cov}[(\tilde{\epsilon}, \epsilon)]^{1/2}
&amp;= \begin{bmatrix} B_{11} &amp; B_{12} \newline B_{21} &amp; B_{22} \end{bmatrix}.
\end{align}
We then have
\begin{align}
\begin{bmatrix} \tilde{y} \newline y \end{bmatrix}
&amp;\overset{d}{=}
\begin{bmatrix} \tilde{G}u \newline Gu\end{bmatrix} +
\begin{bmatrix} \tilde{\epsilon} \newline \epsilon \end{bmatrix} \newline
&amp;\overset{d}{=}
\begin{bmatrix} \tilde{G}\left(m + C^{1/2}z_u \right)\newline G\left(m + C^{1/2}z_u \right) \end{bmatrix} +
\begin{bmatrix} B_{11} &amp; B_{12} \newline B_{21} &amp; B_{22} \end{bmatrix}
\begin{bmatrix} z_1 \newline z_2 \end{bmatrix} \newline
&amp;\overset{d}{=}
\begin{bmatrix} \tilde{G}m \newline Gm \end{bmatrix} +
\begin{bmatrix} \tilde{G}C^{1/2} &amp; B_{11} &amp; B_{12} \newline
                 GC^{1/2} &amp; B_{21} &amp; B_{22} \end{bmatrix}
\begin{bmatrix} z_u \newline z_1 \newline z_2 \end{bmatrix}.
\end{align}
Again, the independence of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>u</mi></msub></mrow><annotation encoding="application/x-tex">z_u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">u</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> with respect to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">z_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">z_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is assured
by the assumptions <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>⊥</mo><mover accent="true"><mi>ϵ</mi><mo>~</mo></mover></mrow><annotation encoding="application/x-tex">u \perp \tilde{\epsilon}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">u</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">⊥</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.6678599999999999em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6678599999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">ϵ</span></span></span><span style="top:-3.35em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">~</span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>⊥</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">u \perp \epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">u</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">⊥</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">ϵ</span></span></span></span>. Thus,
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>z</mi><mi>u</mi></msub><mo separator="true">,</mo><msub><mi>z</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>z</mi><mn>2</mn></msub><msup><mo stretchy="false">)</mo><mi mathvariant="normal">⊤</mi></msup><mo>∼</mo><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><msub><mi>I</mi><mrow><mi>d</mi><mo>+</mo><mi>m</mi><mo>+</mo><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(z_u, z_1, z_2)^\top \sim \mathcal{N}(0, I_{d+m+n})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.099108em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">u</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.14736em;">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mbin mtight">+</span><span class="mord mathdefault mtight">m</span><span class="mbin mtight">+</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>. The
matrices <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mn>12</mn></msub></mrow><annotation encoding="application/x-tex">B_{12}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mn>21</mn></msub></mrow><annotation encoding="application/x-tex">B_{21}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> serve to “mix up” the two independent noise
sources <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">z_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">z_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> in order to produce the correlations between <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">ϵ</span></span></span></span>
and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>ϵ</mi><mo>~</mo></mover></mrow><annotation encoding="application/x-tex">\tilde{\epsilon}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6678599999999999em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6678599999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">ϵ</span></span></span><span style="top:-3.35em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">~</span></span></span></span></span></span></span></span></span></span>. If <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">ϵ</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>ϵ</mi><mo>~</mo></mover></mrow><annotation encoding="application/x-tex">\tilde{\epsilon}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6678599999999999em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6678599999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">ϵ</span></span></span><span style="top:-3.35em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">~</span></span></span></span></span></span></span></span></span></span> are assumed
uncorrelated then <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mn>12</mn></msub><mo>=</mo><msub><mi>B</mi><mn>21</mn></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">B_{12} = B_{21} = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span>.</p>


    </div>
    <div id='bibliography'>
        <div class='wrap'>
            <ol class="bibliography"></ol>
        </div>
    </div>
</div>
</body>
</html>
