[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Andrew G. Roberts",
    "section": "",
    "text": "Metropolis-Hastings Kernels for General State Spaces\n\n\nAn introduction to Tierney’s influential formulation of Metropolis-Hastings algorithms.\n\n\n\nMCMC\n\n\nSampling\n\n\nComputational Statistics\n\n\n\n\n\n\n\n\n\nJul 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAnnealed Importance Sampling\n\n\n\n\n\n\nMCMC\n\n\nSampling\n\n\nComputational Statistics\n\n\n\n\n\n\n\n\n\nJul 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDoubly Intractable MCMC\n\n\nDoubly intractable MCMC, auxiliary variable methods, and the exchange algorithm.\n\n\n\nMCMC\n\n\nSampling\n\n\nComputational Statistics\n\n\n\nExact MCMC with an intractable likelihood.\n\n\n\n\n\nJul 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTransforming Simplex-values Parameters\n\n\n\n\n\n\nStatistics\n\n\n\nWalking through Stan’s parameter transformation for parameters that sum to one.\n\n\n\n\n\nMar 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Reversed Cholesky Decomposition\n\n\n\n\n\n\nLinear-Algebra\n\n\n\nCholesky-like decomposition with upper-triangular matrices.\n\n\n\n\n\nJan 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nRegularized Least Squares with Singular Prior\n\n\n\n\n\n\nStatistics\n\n\nData-Assimilation\n\n\nOptimization\n\n\nInverse-Problem\n\n\n\nSolving the regularized least squares optimization problem when the prior covariance matrix is not positive definite.\n\n\n\n\n\nDec 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNonlinear Least Squares\n\n\n\n\n\n\nStatistics\n\n\nOptimization\n\n\nInverse-Problem\n\n\n\nGauss-Newton, Levenberg-Marquardt\n\n\n\n\n\nNov 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEnsemble Kalman Methods for Solving Inverse Problems: An Introduction\n\n\nEKI Part 1\n\n\n\nInverse-Problem\n\n\nData-Assimilation\n\n\nOptimization\n\n\nSampling\n\n\nComputational Statistics\n\n\nEKI\n\n\n\nThe first post in a series on using Ensemble Kalman methods to approximately solve Bayesian inverse problems.\n\n\n\n\n\nOct 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPseudo-Marginal MCMC\n\n\nPseudo-Marginal MCMC\n\n\n\nMCMC\n\n\nSampling\n\n\nComputational Statistics\n\n\n\nMCMC with an unbiased likelihood approximation.\n\n\n\n\n\nSep 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBasis Expansions for Black-Box Function Emulation\n\n\n\n\n\n\nGaussian-Process\n\n\n\nA discussion of the popular output dimensionality strategy for emulating multi-output functions.\n\n\n\n\n\nJun 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nApproximating Nonlinear Functions of Gaussians\n\n\nAnd the Extended Kalman Filter\n\n\n\nData-Assimilation\n\n\n\nI discuss the generic problem of approximating the distribution resulting from a non-linear transformation of a Gaussian random variable, and then show how this leads to extensions of the Kalman filter which yield approximate filtering algorithms in the non-linear setting.\n\n\n\n\n\nFeb 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Gaussian Process Priors and Hyperparameter Estimation\n\n\n\n\n\n\nStatistics\n\n\nGaussian-Process\n\n\nkernel-methods\n\n\n\nA deep dive into hyperparameter specifications for GP mean and covariance functions, including both frequentist and Bayesian methods for hyperparameter estimation.\n\n\n\n\n\nJan 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDeriving the Metropolis-Hastings Update from the Transition Kernel\n\n\n\n\n\n\nMCMC\n\n\nProbability\n\n\nComputational Statistics\n\n\n\n\n\n\n\n\n\nDec 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPrincipal Components Analysis\n\n\n\n\n\n\nStatistics\n\n\nLinear-Algebra\n\n\n\nA deep dive into PCA.\n\n\n\n\n\nDec 15, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/pca.html",
    "href": "blog/posts/pca.html",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "Suppose that we have data \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_N \\in \\mathbb{R}^D\\), stacked into the rows of a matrix \\(X \\in \\mathbb{R}^{N \\times D}\\). Our task is to find a subspace of smaller dimension \\(R &lt; D\\) such that the projection of the data points onto the subspace retains as much information as possible. By restricting our attention to orthonormal bases for the low-dimensional subspace, we reduce the problem to finding a set of orthonormal basis vectors \\[\n\\begin{align}\n&\\mathbf{b}_1, \\dots, \\mathbf{b}_R \\in \\mathbb{R}^D,\n&&\\langle \\mathbf{b}_r, \\mathbf{b}_s \\rangle = \\delta_{r,s}.\n\\end{align}\n\\] Define \\(B \\in \\mathbb{R}^{D \\times R}\\) to be the matrix with \\(r^{\\text{th}}\\) column equal to \\(\\mathbf{b}_r\\). The subspace generated by the basis \\(B\\) is given by \\[\n\\text{span}(B) := \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R).\n\\] Throughout this post I will abuse notation by referring to the matrix \\(B\\) when actually talking about the set of vectors \\(\\{\\mathbf{b}_1, \\dots, \\mathbf{b}_R\\}\\). Since there is no a priori reason to assume that the data is centered, we should also allow for the subspace to be shifted by some intercept \\(\\mathbf{w}_0 \\in \\mathbb{R}^D\\), resulting in the affine space \\[\n\\mathbf{w}_0 + \\text{span}(B) = \\left\\{\\mathbf{w}_0 +\n\\sum_{r=1}^{R} w_r \\mathbf{b}_r : w_1, \\dots, w_R \\in \\mathbb{R} \\right\\}.\n\\] Loosely speaking, the task is to find the basis \\(B\\), intercept \\(\\mathbf{w}_0\\), and pointwise weights \\(\\mathbf{w}_1, \\dots, \\mathbf{w}_N \\in \\mathbb{R}^R\\) such that \\[\n\\begin{align}\n\\mathbf{x}_n &\\approx \\mathbf{w}_0 + \\sum_{r=1}^{R} (\\mathbf{w}_n)_r \\mathbf{b}_r &&\\forall n=1,\\dots,N \\\\\n&= \\mathbf{w}_0 + B\\mathbf{w}_n.\n\\end{align}\n\\] To formalize this notion, PCA measures the error in the above approximation using Euclidean distance, averaged over the \\(N\\) data points. To further simplify notation, we stack the \\(\\mathbf{w}_n\\) in the columns of a matrix \\(W \\in \\mathbb{R}^{R \\times N}\\). With all of this notation established, we can state that PCA solves the optimization problem \\[\n\\text{argmin}_{B, W, \\mathbf{w}_0} \\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - (\\mathbf{w}_0 + B\\mathbf{w}_n) \\rVert_2^2, \\tag{1}\n\\] where the basis \\(B\\) is constrained to be orthonormal. As we will see, this optimization naturally breaks down into two distinct problems which can be solved sequentially: 1. Given the basis \\(B\\) and intercept \\(\\mathbf{w}_0\\), find the optimal basis coefficients \\(\\mathbf{w}_n\\) corresponding to each data point \\(\\mathbf{x}_n\\). 2. Find the optimal basis and intercept.\nPart of the popularity of PCA stems from the fact that both problems can be solved in closed-form. Let us consider both problems in turn.\n\n\n\nLet us first consider \\(\\mathbf{w}_0\\) and \\(B\\) to be fixed, meaning that we are fixing an affine subspace of dimension \\(R\\). We seek to find the optimal way to represent the data \\(X\\) in this lower-dimensional space. As we will show, the Euclidean objective used by PCA implies that this problem reduces to straightforward orthogonal projection. For now, let \\(\\mathbf{x}^c_n := \\mathbf{x}_n - \\mathbf{w}_0\\) denote the centered data points (we will deal with the intercept shortly). We are thus considering the problem \\[\n\\text{argmin}_{W} \\sum_{n=1}^{N} \\lVert \\mathbf{x}^c_n - B\\mathbf{w}_n \\rVert_2^2 \\tag{2}\n\\] Observe that \\(\\mathbf{w}_n\\) only appears in the \\(n^{\\text{th}}\\) term of the sum, meaning that we can consider each summand independently, \\[\n\\text{argmin}_{\\mathbf{w}_n} \\lVert \\mathbf{x}^c_n - B\\mathbf{w}_n \\rVert_2^2.\n\\] In words, we seek the linear combination of the basis vectors \\(B\\) that results in minimal Euclidean distance from \\(\\mathbf{x}^c_n\\); this is a standard orthogonal projection problem from linear algebra. Since the basis vectors are orthonormal, the optimal projection coefficients are given by \\[\n\\begin{align}\n&(\\mathbf{w}_n)_r = \\langle \\mathbf{x}_n^c, \\mathbf{b}_r \\rangle,\n&&\\mathbf{w}_n = B^\\top \\mathbf{x}_n^c\n\\end{align}\n\\] which can be written succinctly for all data points by stacking the \\(\\mathbf{w}_n^\\top\\) as rows in a matrix \\(W\\); i.e., \\[\nW := X^c B,\n\\] with \\(X^c\\) denoting the centered data matrix with rows set to the \\((\\mathbf{x}^c_n)^\\top\\).\n\n\n\nIn the previous section, we saw that for a fixed basis and intercept, optimizing the basis weights reduced to an orthogonal projection problem. In this section we show that with the weights fixed at their optimal values, optimizing the basis reduces to solving a sequence of eigenvalue problems. To be clear, we are now considering the problem \\[\n\\text{argmin}_{B} \\sum_{n=1}^{N} \\lVert \\mathbf{x}^c_n - B\\mathbf{w}^*_n \\rVert_2^2, \\tag{3}\n\\] where the \\(\\mathbf{w}^*_n\\) are now fixed at the optimal values satisfying (2); i.e., \\(\\mathbf{w}^*_n = B^\\top \\mathbf{x}^c_n\\). However, in the derivations below we will just write \\(\\mathbf{w}_n = \\mathbf{w}^*_n\\) to keep the notation lighter. Note that we are still treating \\(\\mathbf{w}_0\\) as fixed for the time being. We will make another notational simplification in this section by writing \\(\\mathbf{x}_n = \\mathbf{x}_n^c\\). Just keep in mind that throughout this section, \\(\\mathbf{x}_n\\) should be interpreted as \\(\\mathbf{x}_n - \\mathbf{w}_0\\).\nThis problem is also referred to as minimizing the reconstruction error, since \\(\\lVert \\mathbf{x}_n - \\mathbf{\\hat{x}}_n \\rVert_2 := \\lVert \\mathbf{x}_n - B\\mathbf{w}_n \\rVert_2\\) is the error between the original data point \\(\\mathbf{x}_n\\) and the \\(D\\)-dimensional vector \\(\\mathbf{\\hat{x}}_n\\) which can be thought of as an approximation to \\(\\mathbf{x}_n\\) that has been reconstructed from its lower-dimensional representation \\(\\mathbf{w}_n\\). The key here is to re-write this objective function so that this optimization problem takes the form of an eigenvalue problem, which is something that we already know how to solve (see the appendix, A1).\nTo start, we extend the orthonormal set \\(\\mathbf{b}_1, \\dots, \\mathbf{b}_R\\) to an orthonormal basis \\(\\mathbf{b}_1, \\dots, \\mathbf{b}_D\\) for \\(\\mathbb{R}^D\\). Now we can write the original data point \\(\\mathbf{x}_n\\) and its approximation \\(\\mathbf{\\hat{x}}_n\\) with respect to this basis as \\[\n\\begin{align}\n&\\mathbf{x}_n = \\sum_{r=1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r,\n&&\\mathbf{\\hat{x}}_n = \\sum_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r\n\\end{align}\n\\]\nand hence the residual \\(\\mathbf{x}_n - \\mathbf{\\hat{x}}_n\\) is given by \\[\n\\begin{align}\n\\mathbf{x}_n - \\mathbf{\\hat{x}}_n &= \\sum_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r.\n\\end{align}\n\\]\nThus, the objective function in (3) can be written as \\[\n\\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{\\hat{x}}_n \\rVert_2^2\n= \\sum_{n=1}^{N} \\bigg\\lVert \\sum_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r \\bigg\\rVert_2^2\n= \\sum_{n=1}^{N} \\sum_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2 \\lVert \\mathbf{b}_r \\rVert_2^2\n= \\sum_{n=1}^{N} \\sum_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2,\n\\]\nwhere the second and third equalities use the facts that the \\(\\mathbf{b}_r\\) are orthogonal and of unit norm, respectively.\nWe could continue working with this formulation, but at this point it is convenient to re-write the minimization problem we have been working with as an equivalent maximization problem. Note that the above residual calculation is of the form \\(\\mathbf{\\hat{e}}_n = \\mathbf{x}_n - \\mathbf{\\hat{x}}_n\\) (and summed over \\(n\\)). Since \\(\\mathbf{x}_n\\) is fixed, then minimizing the residual (i.e., the reconstruction error) is equivalent to maximizing \\(\\mathbf{\\hat{x}}_n\\). More rigorously, we have\n\\[\n\\begin{align}\n\\text{argmin}_B \\sum_{n=1}^{N} \\sum_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\n&= \\text{argmin}_B \\sum_{n=1}^{N}\n\\left(\\sum_{r=1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2 - \\sum\\_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\\right) \\\\\n&= \\text{argmin}_B \\sum_{n=1}^{N}\n\\left(\\lVert \\mathbf{x}_n \\rVert_2^2 - \\sum_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\\right) \\\\\n&= \\text{argmax}_B \\sum_{n=1}^{N} \\sum_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2. \\tag{4}\n\\end{align}\n\\]\nWe can now re-write the squared inner product to obtain \\[\n\\begin{align}\n\\sum_{n=1}^{N} \\sum_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\n&= \\sum_{n=1}^{N} \\sum_{r=1}^{R} \\mathbf{b}_r^\\top \\mathbf{x}_n \\mathbf{x}_n^\\top \\mathbf{b}_r\n= \\sum_{r=1}^{R} \\mathbf{b}_r^\\top \\left(\\sum_{n=1}^{N}\\mathbf{x}_n \\mathbf{x}_n^\\top\\right) \\mathbf{b}_r\n= \\sum_{r=1}^{R} \\mathbf{b}_r^\\top (X^\\top X) \\mathbf{b}_r^\\top,\n\\end{align}\n\\]\nwhere the final step uses this fact. We have managed to re-write (3) as \\[\n\\text{argmax}_{B} \\sum_{r=1}^{D} \\mathbf{b}_r^\\top (X^\\top X) \\mathbf{b}_r^\\top, \\tag{5}\n\\] where we recall that this is also subject to the constraint that \\(B\\) is orthogonal.\nBefore proceeding, we note that \\(X^\\top X\\) is a positive semi-definite matrix, whose eigenvalues we denote \\(\\lambda_1, \\dots, \\lambda_D\\), sorted in decreasing order. Note that the eigenvalues are all non-negative due to the positive definiteness. Let \\(\\mathbf{e}_1, \\dots, \\mathbf{e}_D\\) denote the respective eigenvectors, normalized to have unit norm. These vectors are guaranteed to be orthogonal by the Spectral Theorem.\nWe now notice in (5) that the objective function has been decomposed into \\(R\\) different terms, each of which only depends on a single \\(\\mathbf{b}_r\\). However, these do not constitute \\(R\\) independent optimization problems, as they are all coupled through the orthogonality constraint. We will thus consider solving them in a recursive fashion, beginning with the first term, \\[\n\\text{argmax}_{\\lVert \\mathbf{b}_1 \\rVert_2=1} \\mathbf{b}_1^\\top (X^\\top X) \\mathbf{b}_1^\\top\n= \\text{argmax}_{\\lVert \\mathbf{b}_1 \\rVert_2=1} \\lVert X \\mathbf{b}_1 \\rVert_2^2.\n\\] This is an eigenvalue problem! It is precisely of the form (A4) (see appendix) and so we apply that result to conclude that the optimal argument is \\(\\mathbf{b}_1 = \\mathbf{e}_1\\) with associated optimal value \\(\\lambda_1\\) (note the objective here is the squared norm, in contrast to the statement in the appendix). Taking this as the base case, we now proceed inductively. Assume that at the \\(r^{\\text{th}}\\) problem in the sequence, the solution is given by \\((\\mathbf{b}_1, \\dots, \\mathbf{b}_r) = (\\mathbf{e}_1, \\dots, \\mathbf{e}_r)\\). We must show the solution to the \\((r+1)^{\\text{st}}\\) problem is \\(\\mathbf{e}_{r+1}\\). Under the inductive hypothesis, this problem is constrained so that \\(\\mathbf{b}_{r+1}\\) is orthogonal to each of \\(\\mathbf{e}_1, \\dots, \\mathbf{e}_r\\); i.e., we require \\(\\mathbf{b}_{r+1} \\perp \\text{span}(\\mathbf{e}_1, \\dots, \\mathbf{e}_r)\\). If we denote \\(\\mathcal{E}_{r} := \\text{span}(\\mathbf{e}_1, \\dots, \\mathbf{e}_r)\\) and \\(\\mathcal{E}^{\\perp}_{r}\\) the orthogonal complement of \\(\\mathcal{E}_{r}\\), then a succinct way to write the orthogonality constraint is that \\(\\mathbf{b}_{r+1} \\in \\mathcal{E}^{\\perp}_r\\). The problem can thus be written as \\[\n\\begin{align}\n\\text{argmax}_{\\mathbf{b}_{r+1} \\in \\mathcal{E}^{\\perp}_{r}, \\lVert \\mathbf{b}_{r+1} \\rVert_2=1} \\lVert X \\mathbf{b}_{r+1} \\rVert_2^2, \\tag{6}\n\\end{align}\n\\] which is another eigenvalue problem, precisely of the form (A3). Using this result from the appendix, we conclude that this is solved by \\(\\mathbf{b}_{r+1} = \\mathbf{e}_{r+1}\\), with the maximal objective value \\(\\lambda_{r+1}\\).\nThat was a lot, so before moving on let’s briefly summarize. First of all, recall that I have been abusing notation by writing \\(\\mathbf{x}_n\\) where I should be writing \\(\\mathbf{x}_n^c = \\mathbf{x}_n - \\mathbf{w}_0\\). In summarizing the result here I will make this correction. Here we have considered the problem of finding the optimal orthonormal basis \\(B\\), for any fixed \\(\\mathbf{w}_0 \\in \\mathbb{R}^D\\), but with the \\(\\mathbf{w}_n\\) set to their optimal values satisfying (2); i.e., \\(\\mathbf{w}_n = B^\\top \\mathbf{x}^c_n\\). Given this, we showed that the reconstruction error (5) is minimized by setting \\(B\\) equal to the matrix with columns given by the dominant \\(R\\) (normalized) eigenvectors of \\((X^c)^\\top X^c\\). We arrived at this solution by showing that the error minimization problem (5) could be viewed as a sequence of \\(R\\) eigenvalue problems.\n\n\n\nThe last ingredient we are missing to solve (1) is the optimal value of \\(\\mathbf{w}_0\\), which has henceforth been viewed as fixed in the above derivations. At first glance, this problem might seem like somewhat of an afterthought, but there are some subtleties that are worth exploring here.\nThe problem we are now considering is \\[\n\\text{argmin}_{\\mathbf{w}_0} \\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{w}_0 - B\\mathbf{w}^*_n \\rVert_2^2, \\tag{7}\n\\] with \\(\\mathbf{w}^*_n\\) denoting the optimal weights \\(\\mathbf{w}^*_n = B^\\top \\mathbf{x}_n\\) derived above (these derivations will go through with any orthonormal basis \\(B\\)). Plugging in this expression for \\(\\mathbf{w}^*_n\\) gives \\[\n\\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{w}_0 - B\\mathbf{w}^*_n \\rVert_2^2\n= \\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{w}_0 - BB^\\top \\mathbf{x}_n \\rVert_2^2\n= \\sum_{n=1}^{N} \\lVert (I - BB^\\top)(\\mathbf{x}_n - \\mathbf{w}_0) \\rVert_2^2.\n\\] Computing the gradient of this expression with respect to \\(\\mathbf{w}_0\\) and setting it equal to zero yields the optimality condition \\[\n\\sum_{n=1}^{N} (I - BB^\\top)(\\mathbf{x}_n - \\mathbf{w}_0) = 0,\n\\] where we have used the fact that \\((I - BB^\\top)^2 = (I - BB^\\top)\\) (since \\(I - BB^\\top\\) is a projection matrix; see appendix). By linearity we then have \\[\n\\sum_{n=1}^{N} (I - BB^\\top)(\\mathbf{x}_n - \\mathbf{w}_0)\n= (I - BB^\\top) \\sum_{n=1}^{N} (\\mathbf{x}_n - \\mathbf{w}_0)\n= (I - BB^\\top)(N \\bar{\\mathbf{x}} - N\\mathbf{w}_0), \\tag{7}\n\\] where we have defined \\[\n\\bar{\\mathbf{x}} := \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}_n,\n\\] the empirical mean of the data. Since \\(\\mathbf{w}_0\\) is optimal when (8) is equal to zero, this leads to the condition \\[\n(I - BB^\\top)(\\bar{\\mathbf{x}} - \\mathbf{w}_0) = 0,\n\\] or equivalently, \\[\n\\bar{\\mathbf{x}} - \\mathbf{w}_0 \\in \\text{Null}(I - BB^\\top).\n\\] Noting that the null space is non-trivial, since \\(\\text{Null}(I - BB^\\top) = \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R)\\) (again, see the appendix section on projection matrices), we then conclude that there are infinitely many optimal solutions! Using the basis \\(\\mathbf{b}_1, \\dots, \\mathbf{b}_R\\) for the null space, we can characterize the set of optimal \\(\\mathbf{w}_0\\) as those satisfying \\[\n\\bar{\\mathbf{x}} - \\mathbf{w}_0 \\in \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R),\n\\] or, more explicitly, all vectors within the affine space \\[\n\\mathbf{w}_0 \\in \\bar{\\mathbf{x}} + \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R). \\tag{9}\n\\] While we have an infinity of optimal solutions we could choose from, the obvious choice \\(\\mathbf{w}_0^* := \\bar{\\mathbf{x}}\\) stands out. Indeed, this is the choice that is essentially always made in practice, so much so that many PCA tutorials will begin by assuming that the data points have all been centered by subtracting off their empirical mean. I find it more insightful to include the intercept as a variable in the PCA optimization problem, and then show that the choice to set it equal to \\(\\bar{\\mathbf{x}}\\) is actually justified.\nMoreover, it is quite interesting that the mean is not actually the unique optimal choice here. Why is this? The characterization (9) says that we can add any vector lying in the span of the orthonormal basis to \\(\\bar{\\mathbf{x}}\\) and still maintain optimality. So the key requirement is that, after shifting the data by subtracting off \\(\\mathbf{w}_0\\), the resulting shifted points must “lie along” the lower-dimensional subspace \\(\\text{span}(B)\\). Since, \\(\\text{span}(B)\\) defines a hyperplane, the data must lie somewhere along this plane; from the perspective of the optimization problem, it doesn’t matter whether it lies around the origin or somewhere very far away, so long as it is clustered around this plane. A picture is worth a thousand words here, and I will try to add one once I have time.\nFinally, note that the specific choice of \\(\\bar{\\mathbf{x}}\\) has various other practical benefits. It leads to projections that are clustered around the origin, thus keeping numbers relatively small. It also leads to a nice statistical interpretation of the eigenvalue problems discussed in the previous subsection; e.g. the basis vector \\(\\mathbf{b}_1\\) can be viewed as the direction along which the empirical variance of the projected data is maximized. This maximum variance perspective is discussed in more detail below."
  },
  {
    "objectID": "blog/posts/pca.html#setup-and-notation",
    "href": "blog/posts/pca.html#setup-and-notation",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "Suppose that we have data \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_N \\in \\mathbb{R}^D\\), stacked into the rows of a matrix \\(X \\in \\mathbb{R}^{N \\times D}\\). Our task is to find a subspace of smaller dimension \\(R &lt; D\\) such that the projection of the data points onto the subspace retains as much information as possible. By restricting our attention to orthonormal bases for the low-dimensional subspace, we reduce the problem to finding a set of orthonormal basis vectors \\[\n\\begin{align}\n&\\mathbf{b}_1, \\dots, \\mathbf{b}_R \\in \\mathbb{R}^D,\n&&\\langle \\mathbf{b}_r, \\mathbf{b}_s \\rangle = \\delta_{r,s}.\n\\end{align}\n\\] Define \\(B \\in \\mathbb{R}^{D \\times R}\\) to be the matrix with \\(r^{\\text{th}}\\) column equal to \\(\\mathbf{b}_r\\). The subspace generated by the basis \\(B\\) is given by \\[\n\\text{span}(B) := \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R).\n\\] Throughout this post I will abuse notation by referring to the matrix \\(B\\) when actually talking about the set of vectors \\(\\{\\mathbf{b}_1, \\dots, \\mathbf{b}_R\\}\\). Since there is no a priori reason to assume that the data is centered, we should also allow for the subspace to be shifted by some intercept \\(\\mathbf{w}_0 \\in \\mathbb{R}^D\\), resulting in the affine space \\[\n\\mathbf{w}_0 + \\text{span}(B) = \\left\\{\\mathbf{w}_0 +\n\\sum_{r=1}^{R} w_r \\mathbf{b}_r : w_1, \\dots, w_R \\in \\mathbb{R} \\right\\}.\n\\] Loosely speaking, the task is to find the basis \\(B\\), intercept \\(\\mathbf{w}_0\\), and pointwise weights \\(\\mathbf{w}_1, \\dots, \\mathbf{w}_N \\in \\mathbb{R}^R\\) such that \\[\n\\begin{align}\n\\mathbf{x}_n &\\approx \\mathbf{w}_0 + \\sum_{r=1}^{R} (\\mathbf{w}_n)_r \\mathbf{b}_r &&\\forall n=1,\\dots,N \\\\\n&= \\mathbf{w}_0 + B\\mathbf{w}_n.\n\\end{align}\n\\] To formalize this notion, PCA measures the error in the above approximation using Euclidean distance, averaged over the \\(N\\) data points. To further simplify notation, we stack the \\(\\mathbf{w}_n\\) in the columns of a matrix \\(W \\in \\mathbb{R}^{R \\times N}\\). With all of this notation established, we can state that PCA solves the optimization problem \\[\n\\text{argmin}_{B, W, \\mathbf{w}_0} \\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - (\\mathbf{w}_0 + B\\mathbf{w}_n) \\rVert_2^2, \\tag{1}\n\\] where the basis \\(B\\) is constrained to be orthonormal. As we will see, this optimization naturally breaks down into two distinct problems which can be solved sequentially: 1. Given the basis \\(B\\) and intercept \\(\\mathbf{w}_0\\), find the optimal basis coefficients \\(\\mathbf{w}_n\\) corresponding to each data point \\(\\mathbf{x}_n\\). 2. Find the optimal basis and intercept.\nPart of the popularity of PCA stems from the fact that both problems can be solved in closed-form. Let us consider both problems in turn."
  },
  {
    "objectID": "blog/posts/pca.html#optimizing-the-basis-coefficients",
    "href": "blog/posts/pca.html#optimizing-the-basis-coefficients",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "Let us first consider \\(\\mathbf{w}_0\\) and \\(B\\) to be fixed, meaning that we are fixing an affine subspace of dimension \\(R\\). We seek to find the optimal way to represent the data \\(X\\) in this lower-dimensional space. As we will show, the Euclidean objective used by PCA implies that this problem reduces to straightforward orthogonal projection. For now, let \\(\\mathbf{x}^c_n := \\mathbf{x}_n - \\mathbf{w}_0\\) denote the centered data points (we will deal with the intercept shortly). We are thus considering the problem \\[\n\\text{argmin}_{W} \\sum_{n=1}^{N} \\lVert \\mathbf{x}^c_n - B\\mathbf{w}_n \\rVert_2^2 \\tag{2}\n\\] Observe that \\(\\mathbf{w}_n\\) only appears in the \\(n^{\\text{th}}\\) term of the sum, meaning that we can consider each summand independently, \\[\n\\text{argmin}_{\\mathbf{w}_n} \\lVert \\mathbf{x}^c_n - B\\mathbf{w}_n \\rVert_2^2.\n\\] In words, we seek the linear combination of the basis vectors \\(B\\) that results in minimal Euclidean distance from \\(\\mathbf{x}^c_n\\); this is a standard orthogonal projection problem from linear algebra. Since the basis vectors are orthonormal, the optimal projection coefficients are given by \\[\n\\begin{align}\n&(\\mathbf{w}_n)_r = \\langle \\mathbf{x}_n^c, \\mathbf{b}_r \\rangle,\n&&\\mathbf{w}_n = B^\\top \\mathbf{x}_n^c\n\\end{align}\n\\] which can be written succinctly for all data points by stacking the \\(\\mathbf{w}_n^\\top\\) as rows in a matrix \\(W\\); i.e., \\[\nW := X^c B,\n\\] with \\(X^c\\) denoting the centered data matrix with rows set to the \\((\\mathbf{x}^c_n)^\\top\\)."
  },
  {
    "objectID": "blog/posts/pca.html#optimizing-the-basis",
    "href": "blog/posts/pca.html#optimizing-the-basis",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "In the previous section, we saw that for a fixed basis and intercept, optimizing the basis weights reduced to an orthogonal projection problem. In this section we show that with the weights fixed at their optimal values, optimizing the basis reduces to solving a sequence of eigenvalue problems. To be clear, we are now considering the problem \\[\n\\text{argmin}_{B} \\sum_{n=1}^{N} \\lVert \\mathbf{x}^c_n - B\\mathbf{w}^*_n \\rVert_2^2, \\tag{3}\n\\] where the \\(\\mathbf{w}^*_n\\) are now fixed at the optimal values satisfying (2); i.e., \\(\\mathbf{w}^*_n = B^\\top \\mathbf{x}^c_n\\). However, in the derivations below we will just write \\(\\mathbf{w}_n = \\mathbf{w}^*_n\\) to keep the notation lighter. Note that we are still treating \\(\\mathbf{w}_0\\) as fixed for the time being. We will make another notational simplification in this section by writing \\(\\mathbf{x}_n = \\mathbf{x}_n^c\\). Just keep in mind that throughout this section, \\(\\mathbf{x}_n\\) should be interpreted as \\(\\mathbf{x}_n - \\mathbf{w}_0\\).\nThis problem is also referred to as minimizing the reconstruction error, since \\(\\lVert \\mathbf{x}_n - \\mathbf{\\hat{x}}_n \\rVert_2 := \\lVert \\mathbf{x}_n - B\\mathbf{w}_n \\rVert_2\\) is the error between the original data point \\(\\mathbf{x}_n\\) and the \\(D\\)-dimensional vector \\(\\mathbf{\\hat{x}}_n\\) which can be thought of as an approximation to \\(\\mathbf{x}_n\\) that has been reconstructed from its lower-dimensional representation \\(\\mathbf{w}_n\\). The key here is to re-write this objective function so that this optimization problem takes the form of an eigenvalue problem, which is something that we already know how to solve (see the appendix, A1).\nTo start, we extend the orthonormal set \\(\\mathbf{b}_1, \\dots, \\mathbf{b}_R\\) to an orthonormal basis \\(\\mathbf{b}_1, \\dots, \\mathbf{b}_D\\) for \\(\\mathbb{R}^D\\). Now we can write the original data point \\(\\mathbf{x}_n\\) and its approximation \\(\\mathbf{\\hat{x}}_n\\) with respect to this basis as \\[\n\\begin{align}\n&\\mathbf{x}_n = \\sum_{r=1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r,\n&&\\mathbf{\\hat{x}}_n = \\sum_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r\n\\end{align}\n\\]\nand hence the residual \\(\\mathbf{x}_n - \\mathbf{\\hat{x}}_n\\) is given by \\[\n\\begin{align}\n\\mathbf{x}_n - \\mathbf{\\hat{x}}_n &= \\sum_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r.\n\\end{align}\n\\]\nThus, the objective function in (3) can be written as \\[\n\\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{\\hat{x}}_n \\rVert_2^2\n= \\sum_{n=1}^{N} \\bigg\\lVert \\sum_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r \\bigg\\rVert_2^2\n= \\sum_{n=1}^{N} \\sum_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2 \\lVert \\mathbf{b}_r \\rVert_2^2\n= \\sum_{n=1}^{N} \\sum_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2,\n\\]\nwhere the second and third equalities use the facts that the \\(\\mathbf{b}_r\\) are orthogonal and of unit norm, respectively.\nWe could continue working with this formulation, but at this point it is convenient to re-write the minimization problem we have been working with as an equivalent maximization problem. Note that the above residual calculation is of the form \\(\\mathbf{\\hat{e}}_n = \\mathbf{x}_n - \\mathbf{\\hat{x}}_n\\) (and summed over \\(n\\)). Since \\(\\mathbf{x}_n\\) is fixed, then minimizing the residual (i.e., the reconstruction error) is equivalent to maximizing \\(\\mathbf{\\hat{x}}_n\\). More rigorously, we have\n\\[\n\\begin{align}\n\\text{argmin}_B \\sum_{n=1}^{N} \\sum_{r=R+1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\n&= \\text{argmin}_B \\sum_{n=1}^{N}\n\\left(\\sum_{r=1}^{D} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2 - \\sum\\_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\\right) \\\\\n&= \\text{argmin}_B \\sum_{n=1}^{N}\n\\left(\\lVert \\mathbf{x}_n \\rVert_2^2 - \\sum_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\\right) \\\\\n&= \\text{argmax}_B \\sum_{n=1}^{N} \\sum_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2. \\tag{4}\n\\end{align}\n\\]\nWe can now re-write the squared inner product to obtain \\[\n\\begin{align}\n\\sum_{n=1}^{N} \\sum_{r=1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle^2\n&= \\sum_{n=1}^{N} \\sum_{r=1}^{R} \\mathbf{b}_r^\\top \\mathbf{x}_n \\mathbf{x}_n^\\top \\mathbf{b}_r\n= \\sum_{r=1}^{R} \\mathbf{b}_r^\\top \\left(\\sum_{n=1}^{N}\\mathbf{x}_n \\mathbf{x}_n^\\top\\right) \\mathbf{b}_r\n= \\sum_{r=1}^{R} \\mathbf{b}_r^\\top (X^\\top X) \\mathbf{b}_r^\\top,\n\\end{align}\n\\]\nwhere the final step uses this fact. We have managed to re-write (3) as \\[\n\\text{argmax}_{B} \\sum_{r=1}^{D} \\mathbf{b}_r^\\top (X^\\top X) \\mathbf{b}_r^\\top, \\tag{5}\n\\] where we recall that this is also subject to the constraint that \\(B\\) is orthogonal.\nBefore proceeding, we note that \\(X^\\top X\\) is a positive semi-definite matrix, whose eigenvalues we denote \\(\\lambda_1, \\dots, \\lambda_D\\), sorted in decreasing order. Note that the eigenvalues are all non-negative due to the positive definiteness. Let \\(\\mathbf{e}_1, \\dots, \\mathbf{e}_D\\) denote the respective eigenvectors, normalized to have unit norm. These vectors are guaranteed to be orthogonal by the Spectral Theorem.\nWe now notice in (5) that the objective function has been decomposed into \\(R\\) different terms, each of which only depends on a single \\(\\mathbf{b}_r\\). However, these do not constitute \\(R\\) independent optimization problems, as they are all coupled through the orthogonality constraint. We will thus consider solving them in a recursive fashion, beginning with the first term, \\[\n\\text{argmax}_{\\lVert \\mathbf{b}_1 \\rVert_2=1} \\mathbf{b}_1^\\top (X^\\top X) \\mathbf{b}_1^\\top\n= \\text{argmax}_{\\lVert \\mathbf{b}_1 \\rVert_2=1} \\lVert X \\mathbf{b}_1 \\rVert_2^2.\n\\] This is an eigenvalue problem! It is precisely of the form (A4) (see appendix) and so we apply that result to conclude that the optimal argument is \\(\\mathbf{b}_1 = \\mathbf{e}_1\\) with associated optimal value \\(\\lambda_1\\) (note the objective here is the squared norm, in contrast to the statement in the appendix). Taking this as the base case, we now proceed inductively. Assume that at the \\(r^{\\text{th}}\\) problem in the sequence, the solution is given by \\((\\mathbf{b}_1, \\dots, \\mathbf{b}_r) = (\\mathbf{e}_1, \\dots, \\mathbf{e}_r)\\). We must show the solution to the \\((r+1)^{\\text{st}}\\) problem is \\(\\mathbf{e}_{r+1}\\). Under the inductive hypothesis, this problem is constrained so that \\(\\mathbf{b}_{r+1}\\) is orthogonal to each of \\(\\mathbf{e}_1, \\dots, \\mathbf{e}_r\\); i.e., we require \\(\\mathbf{b}_{r+1} \\perp \\text{span}(\\mathbf{e}_1, \\dots, \\mathbf{e}_r)\\). If we denote \\(\\mathcal{E}_{r} := \\text{span}(\\mathbf{e}_1, \\dots, \\mathbf{e}_r)\\) and \\(\\mathcal{E}^{\\perp}_{r}\\) the orthogonal complement of \\(\\mathcal{E}_{r}\\), then a succinct way to write the orthogonality constraint is that \\(\\mathbf{b}_{r+1} \\in \\mathcal{E}^{\\perp}_r\\). The problem can thus be written as \\[\n\\begin{align}\n\\text{argmax}_{\\mathbf{b}_{r+1} \\in \\mathcal{E}^{\\perp}_{r}, \\lVert \\mathbf{b}_{r+1} \\rVert_2=1} \\lVert X \\mathbf{b}_{r+1} \\rVert_2^2, \\tag{6}\n\\end{align}\n\\] which is another eigenvalue problem, precisely of the form (A3). Using this result from the appendix, we conclude that this is solved by \\(\\mathbf{b}_{r+1} = \\mathbf{e}_{r+1}\\), with the maximal objective value \\(\\lambda_{r+1}\\).\nThat was a lot, so before moving on let’s briefly summarize. First of all, recall that I have been abusing notation by writing \\(\\mathbf{x}_n\\) where I should be writing \\(\\mathbf{x}_n^c = \\mathbf{x}_n - \\mathbf{w}_0\\). In summarizing the result here I will make this correction. Here we have considered the problem of finding the optimal orthonormal basis \\(B\\), for any fixed \\(\\mathbf{w}_0 \\in \\mathbb{R}^D\\), but with the \\(\\mathbf{w}_n\\) set to their optimal values satisfying (2); i.e., \\(\\mathbf{w}_n = B^\\top \\mathbf{x}^c_n\\). Given this, we showed that the reconstruction error (5) is minimized by setting \\(B\\) equal to the matrix with columns given by the dominant \\(R\\) (normalized) eigenvectors of \\((X^c)^\\top X^c\\). We arrived at this solution by showing that the error minimization problem (5) could be viewed as a sequence of \\(R\\) eigenvalue problems."
  },
  {
    "objectID": "blog/posts/pca.html#optimizing-the-intercept",
    "href": "blog/posts/pca.html#optimizing-the-intercept",
    "title": "Principal Components Analysis",
    "section": "",
    "text": "The last ingredient we are missing to solve (1) is the optimal value of \\(\\mathbf{w}_0\\), which has henceforth been viewed as fixed in the above derivations. At first glance, this problem might seem like somewhat of an afterthought, but there are some subtleties that are worth exploring here.\nThe problem we are now considering is \\[\n\\text{argmin}_{\\mathbf{w}_0} \\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{w}_0 - B\\mathbf{w}^*_n \\rVert_2^2, \\tag{7}\n\\] with \\(\\mathbf{w}^*_n\\) denoting the optimal weights \\(\\mathbf{w}^*_n = B^\\top \\mathbf{x}_n\\) derived above (these derivations will go through with any orthonormal basis \\(B\\)). Plugging in this expression for \\(\\mathbf{w}^*_n\\) gives \\[\n\\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{w}_0 - B\\mathbf{w}^*_n \\rVert_2^2\n= \\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - \\mathbf{w}_0 - BB^\\top \\mathbf{x}_n \\rVert_2^2\n= \\sum_{n=1}^{N} \\lVert (I - BB^\\top)(\\mathbf{x}_n - \\mathbf{w}_0) \\rVert_2^2.\n\\] Computing the gradient of this expression with respect to \\(\\mathbf{w}_0\\) and setting it equal to zero yields the optimality condition \\[\n\\sum_{n=1}^{N} (I - BB^\\top)(\\mathbf{x}_n - \\mathbf{w}_0) = 0,\n\\] where we have used the fact that \\((I - BB^\\top)^2 = (I - BB^\\top)\\) (since \\(I - BB^\\top\\) is a projection matrix; see appendix). By linearity we then have \\[\n\\sum_{n=1}^{N} (I - BB^\\top)(\\mathbf{x}_n - \\mathbf{w}_0)\n= (I - BB^\\top) \\sum_{n=1}^{N} (\\mathbf{x}_n - \\mathbf{w}_0)\n= (I - BB^\\top)(N \\bar{\\mathbf{x}} - N\\mathbf{w}_0), \\tag{7}\n\\] where we have defined \\[\n\\bar{\\mathbf{x}} := \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}_n,\n\\] the empirical mean of the data. Since \\(\\mathbf{w}_0\\) is optimal when (8) is equal to zero, this leads to the condition \\[\n(I - BB^\\top)(\\bar{\\mathbf{x}} - \\mathbf{w}_0) = 0,\n\\] or equivalently, \\[\n\\bar{\\mathbf{x}} - \\mathbf{w}_0 \\in \\text{Null}(I - BB^\\top).\n\\] Noting that the null space is non-trivial, since \\(\\text{Null}(I - BB^\\top) = \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R)\\) (again, see the appendix section on projection matrices), we then conclude that there are infinitely many optimal solutions! Using the basis \\(\\mathbf{b}_1, \\dots, \\mathbf{b}_R\\) for the null space, we can characterize the set of optimal \\(\\mathbf{w}_0\\) as those satisfying \\[\n\\bar{\\mathbf{x}} - \\mathbf{w}_0 \\in \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R),\n\\] or, more explicitly, all vectors within the affine space \\[\n\\mathbf{w}_0 \\in \\bar{\\mathbf{x}} + \\text{span}(\\mathbf{b}_1, \\dots, \\mathbf{b}_R). \\tag{9}\n\\] While we have an infinity of optimal solutions we could choose from, the obvious choice \\(\\mathbf{w}_0^* := \\bar{\\mathbf{x}}\\) stands out. Indeed, this is the choice that is essentially always made in practice, so much so that many PCA tutorials will begin by assuming that the data points have all been centered by subtracting off their empirical mean. I find it more insightful to include the intercept as a variable in the PCA optimization problem, and then show that the choice to set it equal to \\(\\bar{\\mathbf{x}}\\) is actually justified.\nMoreover, it is quite interesting that the mean is not actually the unique optimal choice here. Why is this? The characterization (9) says that we can add any vector lying in the span of the orthonormal basis to \\(\\bar{\\mathbf{x}}\\) and still maintain optimality. So the key requirement is that, after shifting the data by subtracting off \\(\\mathbf{w}_0\\), the resulting shifted points must “lie along” the lower-dimensional subspace \\(\\text{span}(B)\\). Since, \\(\\text{span}(B)\\) defines a hyperplane, the data must lie somewhere along this plane; from the perspective of the optimization problem, it doesn’t matter whether it lies around the origin or somewhere very far away, so long as it is clustered around this plane. A picture is worth a thousand words here, and I will try to add one once I have time.\nFinally, note that the specific choice of \\(\\bar{\\mathbf{x}}\\) has various other practical benefits. It leads to projections that are clustered around the origin, thus keeping numbers relatively small. It also leads to a nice statistical interpretation of the eigenvalue problems discussed in the previous subsection; e.g. the basis vector \\(\\mathbf{b}_1\\) can be viewed as the direction along which the empirical variance of the projected data is maximized. This maximum variance perspective is discussed in more detail below."
  },
  {
    "objectID": "blog/posts/pca.html#minimum-error-or-maximum-variance",
    "href": "blog/posts/pca.html#minimum-error-or-maximum-variance",
    "title": "Principal Components Analysis",
    "section": "Minimum Error or Maximum Variance?",
    "text": "Minimum Error or Maximum Variance?\nWhile the derivations in the preceding section are somewhat lengthy, recall that this was all in the pursuit of solving the optimization problem (1). In words, we derived the best \\(R\\)-dimensional affine subspace to represent the data \\(X\\), where “best” is defined as minimizing the average Euclidean error between the data points and their projections onto the subspace. We showed that this error minimization problem could be re-written as a sequence of \\(R\\) maximization problems of the form (6). We now show that these maximization problems have a very nice statistical interpretation.\n\nSample covariance of the \\(\\mathbf{x}_n\\)\nWe first recall that the empirical covariance matrix of the data points \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\) is defined to be \\[\n\\hat{C} := \\frac{1}{N-1} \\sum_{n=1}^{N} (\\mathbf{x}_n - \\bar{\\mathbf{x}}) (\\mathbf{x}_n - \\bar{\\mathbf{x}})^\\top,\n\\] which can be re-written as \\[\n\\hat{C} = \\frac{1}{N-1} \\sum_{n=1}^{N} \\mathbf{x}^c_n (\\mathbf{x}^c_n)^\\top = \\frac{1}{N-1} X^c (X^c)^\\top, \\tag{10}\n\\] where the superscript c indicates that the observations have been centered by subtracting off their empirical mean.\nRecall that solving the maximization problems (6) revealed that the optimal basis vectors are given by the dominant eigenvectors of the matrix \\(X^c (X^c)^\\top\\), which is the (unscaled) covariance (10)! The scaling factor does not affect the optimal basis vectors, it simply scales the objective function. Specifically, \\[\n\\begin{align}\n\\text{argmax}_{\\mathbf{b}_{r+1} \\in \\mathcal{E}^{\\perp}_{r}, \\lVert \\mathbf{b}_{r+1} \\rVert_2=1}\n\\left(\\mathbf{b}_{r+1}^\\top X^c (X^c)^\\top \\mathbf{b}_{r+1}\\right)\n= \\text{argmax}_{\\mathbf{b}_{r+1} \\in \\mathcal{E}^{\\perp}_{r}, \\lVert \\mathbf{b}_{r+1} \\rVert_2=1}\n\\left(\\mathbf{b}_{r+1}^\\top \\hat{C} \\mathbf{b}_{r+1}\\right). \\tag{11}\n\\end{align}\n\\] We haven’t changed anything from (6) here, other than noting that a re-scaling of the objective function allows us involve \\(\\hat{C}\\) in the expression.\n\n\nSample covariance of the \\(\\mathbf{w}_n\\)\nGiven that the sample covariance matrix of the data \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_N \\in \\mathbb{R}^D\\) is given by \\(\\hat{C}\\), it is natural to also consider the empirical covariance of \\(\\mathbf{w}_1, \\dots, \\mathbf{w}_N \\in \\mathbb{R}^R\\). We begin by computing the sample mean \\[\n\\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{w}_n\n= \\frac{1}{N} \\sum_{n=1}^{N} B\\mathbf{x}^c_n\n= B \\left[\\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}^c_n \\right] = 0,\n\\] using the fact that the empirical mean of the centered data points is \\(0\\). Recalling that the row vectors \\(\\mathbf{w}_n^\\top\\) are stored in the rows of the matrix \\(W\\), it follows that the empirical covariance matrix of \\(\\mathbf{w}_1, \\dots, \\mathbf{w}_N\\) is given by \\[\n\\hat{C}_w := \\frac{1}{N-1} W^\\top W, \\tag{12}\n\\] which follows from the calculation (10) with \\(W\\) in place of \\(X^c\\). Since \\(W = XB\\), we have \\[\n\\hat{C}_w = \\frac{1}{N-1} (XB)^\\top (XB) = B^\\top \\hat{C} B,\n\\] which allows us to write the covariance of the \\(\\mathbf{w}_n\\) as a function of the covariance of the \\(\\mathbf{x}_n\\). We can say even more since we know that \\(B\\) is given by \\(V_R\\), the truncated set of eigenvectors obtained from the eigendecomposition \\(X^\\top X = V \\Lambda V^\\top\\). We thus have \\[\n\\hat{C}_w\n= \\frac{1}{N-1} B^\\top (X^\\top X) B\n= \\frac{1}{N-1} V_R^\\top (X^\\top X) V_R\n= \\frac{1}{N-1} V_R^\\top V_R \\Lambda_R\n= \\frac{1}{N-1} \\Lambda_R,  \\tag{13}\n\\] where \\((X^\\top X) V_R = V_R \\Lambda_R\\) follows from the fact that the columns of \\(V_R\\) store the first \\(R\\) eigenvectors of \\(X^\\top X\\). The conclusion here is that \\(\\hat{C}_w\\) is diagonal, with variances equal to the eigenvalues of \\(\\hat{C}\\). In other words, PCA computes a change-of-basis that diagonalizes the empirical covariance of the data.\n\n\nPCA as Variance Maximization\nWe now return to the goal of providing a statistical interpretation of the objective function \\(\\mathbf{b}_{r}^\\top \\hat{C} \\mathbf{b}_r\\) in (11). Given the derivation of \\(\\hat{C}_w\\) in (13), we see the empirical variance of \\((\\mathbf{w}_1)_r, \\dots, (\\mathbf{w}_N)_r\\) (i.e., the values in the \\(r^{\\text{th}}\\) column of \\(W\\)) is equal to \\[\n[\\hat{C}_w]_{rr} = \\mathbf{b}_r^\\top \\hat{C} \\mathbf{b}_r,\n\\] which is precisely the objective being maximized. To interpret this quantity more clearly, we consider the projection of \\(\\mathbf{x}_n^c\\) onto the span of \\(\\mathbf{b}_r\\), \\[\n\\text{proj}_{\\mathbf{b}_r} \\mathbf{x}^c_n\n:= \\langle \\mathbf{x}^c_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r\n= (\\mathbf{w}_n)_r \\mathbf{b}_r,\n\\] which implies that \\((\\mathbf{w}_n)_r\\) is the magnitude of the projection. Therefore, we conclude that \\(\\mathbf{b}_r^\\top \\hat{C} \\mathbf{b}_r\\) is the sample variance of the magnitude of the projections onto the subspace \\(\\text{span}(\\mathbf{b}_r)\\); loosely speaking, the variance of the projection along the \\(r^{\\text{th}}\\) basis vector. Combining all of these equivalent expressions yields the chain of equalities, \\[\n\\text{Tr}(\\hat{C}_w)\n= \\frac{1}{N-1} \\sum_{r=1}^{R} \\lambda_r\n= \\sum_{n=1}^{N} \\sum_{r=1}^{R} W_{nr}^2.\n= \\sum_{r=1}^{R} \\mathbf{b}_r^\\top \\hat{C} \\mathbf{b}_r \\tag{15}\n\\] The trace \\(\\text{Tr}(\\hat{C}_w)\\) thus represents the total variance of the projection, summed over all of the basis vectors. The total variance is equivalently given by the sum of the eigenvalues of \\(\\hat{C}\\) or by the squared Frobenius norm \\(\\lVert W \\rVert_F^2\\).\nThe final term in (15) provides an alternative interpretation of the objective function in (5); namely, that PCA seeks the basis \\(B\\) which results in the maximal projected total variance. The resulting sequence of constrained problems, as in (11), are interpreted similarly. In particular, \\[\n\\text{argmax}_{\\mathbf{b}_{r+1} \\in \\mathcal{E}^{\\perp}_{r}, \\lVert \\mathbf{b}_{r+1} \\rVert_2=1}\n\\left(\\mathbf{b}_{r+1}^\\top \\hat{C} \\mathbf{b}_{r+1}\\right)\n\\] can be viewed as seeking the direction along which the variance of the projections is maximized, subject to the constraint that the direction be orthogonal to the previous \\(r\\) directions. The optimal solution is the direction corresponding to the \\((r+1)^{\\text{st}}\\) eigenvector of the empirical covariance \\(\\hat{C}\\), and the resulting maximal variance in this direction is given by the associated eigenvalue."
  },
  {
    "objectID": "blog/posts/pca.html#the-singular-value-decomposition",
    "href": "blog/posts/pca.html#the-singular-value-decomposition",
    "title": "Principal Components Analysis",
    "section": "The Singular Value Decomposition",
    "text": "The Singular Value Decomposition"
  },
  {
    "objectID": "blog/posts/pca.html#a-matrix-approximation-problem-the-eckart-young-theorem",
    "href": "blog/posts/pca.html#a-matrix-approximation-problem-the-eckart-young-theorem",
    "title": "Principal Components Analysis",
    "section": "A Matrix Approximation Problem: The Eckart-Young Theorem",
    "text": "A Matrix Approximation Problem: The Eckart-Young Theorem\nIgnoring the intercept (or assuming that the \\(\\mathbf{x}_n\\) have already been centered), we can re-write the reconstruction error as \\[\n\\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - B\\mathbf{w}_n \\rVert_2^2\n= \\lVert X - WB^\\top \\rVert_F^2,\n\\] where \\(\\lVert \\cdot \\rVert_F\\) denotes the Frobenius norm. The PCA optimization problem can then be written as the matrix approximation problem \\[\n\\text{argmin}_{B, W} \\lVert X - WB^\\top \\rVert_F^2, \\tag{10}\n\\] where \\(B\\) is constrained to be an orthogonal matrix. We can equivalently write this as \\[\n\\begin{align}\n&\\text{argmin}_{\\hat{X} \\in \\mathcal{M}} \\lVert X - \\hat{X} \\rVert_F^2,\n&&\\mathcal{M} = \\{\\hat{X} \\in \\mathbb{R}^{N \\times D} : \\hat{X}=WB^\\top, B \\in \\mathbb{R}^{D \\times R} \\text{ is orthogonal}\\}, \\tag{11}\n\\end{align}\n\\] which makes it even more clear that PCA can generically be viewed as the problem of approximating the data matrix \\(X\\) with another matrix \\(\\hat{X}\\) that is constrained to lie in a subset \\(\\mathcal{M}\\) of all \\(N \\times D\\) matrices. We can phrase this even more succinctly by noticing that \\(\\mathcal{M}\\) is precisely the set of all \\(N \\times D\\) matrices with rank at most \\(R\\). Indeed, if \\(\\hat{X} \\in \\mathcal{M}\\) then \\(\\text{rank}(\\hat{X}) = \\text{rank}(W B^\\top) \\leq R\\) since \\(W\\) has \\(R\\) columns and thus the rank of this matrix product cannot exceed \\(R\\). Conversely, if we let \\(\\hat{X}\\) be an arbirary \\(N \\times D\\) matrix of rank \\(r \\leq R\\) then \\(\\hat{X}\\) can be expanded using the compact SVD as \\[\n\\hat{X} = U\\tilde{\\Sigma}V^\\top = (U\\tilde{\\Sigma})V^\\top,\n\\] where \\(U \\in \\mathbb{R}^{N \\times r}\\), \\(V \\in \\mathbb{R}^{D \\times r}\\) are orthogonal and \\(\\tilde{\\Sigma} \\in \\mathbb{R}^{r \\times r}\\) is diagonal. By setting \\(W := U\\tilde{\\Sigma}\\) and \\(B := V\\) we have almost written things in the form required by (11), but (11) restricts \\(B\\) to be orthogonal with exactly \\(R\\) columns. Thus, we can simply extend \\(V\\) to have \\(R\\) orthonormal columns by appending columns \\(B = [V|\\mathbf{v}_{r+1}, \\dots, \\mathbf{v}_{R}]\\) (this is justified by Gram-Schmidt) and then set \\(W = [U\\tilde{\\Sigma}|\\boldsymbol{0}]\\). Thus, \\(\\hat{X} = (U\\tilde{\\Sigma})V^\\top = WB^\\top\\) with \\(B\\) now of the required form.\nWe have thus verified that (11) can equivalently be written as the low-rank matrix approximation problem \\[\n\\begin{align}\n&\\text{argmin}_{\\hat{X} \\in \\mathcal{M}} \\lVert X - \\hat{X} \\rVert_F^2,\n&&\\mathcal{M} = \\{\\hat{X} \\in \\mathbb{R}^{N \\times D} : \\text{rank}(\\hat{X}) \\leq R \\}. \\tag{12}\n\\end{align}\n\\]\nThis is precisely the problem considered by the celebrated Eckart-Young theorem. The theorem concludes that the optimal solution to (12) is given by the truncated SVD \\(X = U_R \\Sigma_R V_R^\\top\\), which is precisely the PCA solution computed using SVD as discussed in the previous section."
  },
  {
    "objectID": "blog/posts/pca.html#regression-with-an-optimal-basis",
    "href": "blog/posts/pca.html#regression-with-an-optimal-basis",
    "title": "Principal Components Analysis",
    "section": "Regression with an optimal basis",
    "text": "Regression with an optimal basis"
  },
  {
    "objectID": "blog/posts/pca.html#statisticalprobabalistic-perspectives",
    "href": "blog/posts/pca.html#statisticalprobabalistic-perspectives",
    "title": "Principal Components Analysis",
    "section": "Statistical/Probabalistic Perspectives",
    "text": "Statistical/Probabalistic Perspectives\nThere are various ways we might attach a statistical or probabilistic perspective to PCA - we briefly discuss a few of them here. Throughout this section we will take \\[\n\\text{argmin}_{W,B} \\sum_{n=1}^{N} \\lVert \\mathbf{x}_n - B\\mathbf{w}_n \\rVert_2^2  \\tag{13}\n\\] as the jumping off point for a probabilistic generalization; that is, we are implicitly assuming the data is centered so that we can ignore the intercept. Note that, as always, \\(B\\) is constrained to be orthogonal.\n\nA Special Case of the Karhunen-Loeve Expansion\nWe start by noting that the objective function in (13) looks like a sample average of the quantities \\(\\lVert \\mathbf{x}_n - B\\mathbf{w}_n \\rVert_2^2\\). It is therefore natural to consider some underlying true expectation that this sample average is approximating. To this end, let us view \\(\\mathbf{x} = \\mathbf{x}(\\omega)\\) and \\(\\mathbf{w} = \\mathbf{w}(\\omega)\\) as random vectors, defined over some probability space \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\). We can then consider \\[\n\\text{argmin}_{\\mathbf{w},B} \\mathbb{E} \\lVert \\mathbf{x} - B\\mathbf{w} \\rVert_2^2\n= \\text{argmin}_{\\mathbf{w},B} \\int_{\\Omega} \\left\\lVert \\mathbf{x}(\\omega) - \\sum_{r=1}^{R} \\mathbf{w}_r(\\omega) \\mathbf{b}_r \\right\\rVert_2^2 \\mathbb{P}(d\\omega), \\tag{14}\n\\] which can be viewed as the population analog of the sample approximation in (13). We note that the second expression above shows that the low-rank approximation of the random vector \\(\\mathbf{x}\\) takes the form of a linear combination of (non-random) basis vectors, with random coefficients.\nWith \\(B\\) fixed, the optimization in \\(\\mathbf{w}\\) is just as easy as before. Indeed, for any fixed \\(\\omega \\in \\Omega\\), \\[\n\\text{argmin}_{\\mathbf{w}(\\omega)} \\lVert \\mathbf{x}(\\omega) - B\\mathbf{w}(\\omega) \\rVert_2^2 = B^\\top \\mathbf{x}(\\omega),\n\\] using the same exact orthogonal projection reasoning as before. We have thus optimized for \\(\\mathbf{w}(\\omega)\\) on an \\(\\omega\\)-by-\\(\\omega\\) basis. The same result thus holds in expectation: \\[\n\\text{argmin}_{\\mathbf{w}} \\mathbb{E} \\lVert \\mathbf{x} - B\\mathbf{w} \\rVert_2^2 = B^\\top \\mathbf{x}.\n\\] It is important to note that we have found the optimal random vector \\(\\mathbf{w}\\), which was shown to be a linear transformation of the random vector \\(\\mathbf{x}\\).\nOptimizing for \\(B\\) with the optimal \\(\\mathbf{w}\\) fixed also follows similarly from previous derivations. Using some of the results already derived above (see the section on optimizing the basis), we have\n\\[\n\\begin{align}\n\\text{argmin}_{B} \\mathbb{E}\\lVert \\mathbf{x} - B\\mathbf{w} \\rVert_2^2\n&= \\text{argmin}_{B} \\mathbb{E} \\sum_{r=R+1}^{D} \\langle \\mathbf{x}, \\mathbf{b}_r \\rangle^2 \\newline\n&= \\text{argmin}_{B} \\mathbb{E} \\left[\\lVert \\mathbf{x} \\rVert_2^2 - \\sum_{r=1}^{R} \\langle \\mathbf{x}, \\mathbf{b}_r \\rangle^2 \\right] \\newline\n&= \\text{argmax}_{B} \\mathbb{E} \\sum_{r=1}^{R} \\langle \\mathbf{x}, \\mathbf{b}_r \\rangle^2 \\newline\n&= \\text{argmax}_{B} \\mathbb{E} \\sum_{r=1}^{R} \\mathbf{b}_r^\\top \\mathbf{x}\\mathbf{x}^\\top \\mathbf{b}_r \\newline\n&= \\text{argmax}_{B} \\sum_{r=1}^{R} \\mathbf{b}_r^\\top \\mathbb{E}\\left[\\mathbf{x}\\mathbf{x}^\\top\\right] \\mathbf{b}_r \\newline\n&= \\text{argmax}_{B} \\sum_{r=1}^{R} \\mathbf{b}_r^\\top C \\mathbf{b}_r,\n\\end{align}\n\\]\nwhere \\(C := \\text{Cov}[\\mathbf{x}]\\). Here we have used the centering assumption \\(\\mathbb{E}[\\mathbf{x}] = 0\\), as well as the implicit assumption that the random vector \\(\\mathbf{x}\\) has a well-defined covariance matrix. At this point, the derivations go through exactly as before, with \\(C\\) replacing the empirical covariance \\(\\hat{C}\\); that is, the optimal basis \\(B\\) is obtained by extracting the first \\(R\\) columns of \\(V\\), where \\(C = V\\Lambda V^\\top\\). Unsurprisingly, the results that held for the sample covariance hold analogously in this setting. For example, since \\(\\mathbf{x}\\) has zero mean then \\[\n\\mathbb{E}[\\mathbf{w}_r] = \\mathbb{E}[\\mathbf{v}_r^\\top \\mathbf{x}] = 0.\n\\] Moreover, \\[\n\\text{Cov}[\\mathbf{w}_r, \\mathbf{w}_s]\n= \\text{Cov}[\\mathbf{v}_r^\\top \\mathbf{x}, \\mathbf{v}_s^\\top \\mathbf{x}]\n= \\mathbf{v}_r^\\top C \\mathbf{v}_s\n= \\mathbf{v}_r^\\top V \\Lambda V^\\top \\mathbf{v}_s\n= \\lambda_r 1[r = s],\n\\] which says that the weights \\(\\mathbf{w}_r\\) are uncorrelated, with variance equal to their respective eigenvalues. It is common, therefore, to normalize these random variables to have unit variance \\[\n\\mathbf{\\tilde{w}}_r := \\frac{1}{\\sqrt{\\lambda_r}} \\mathbf{w}_r,\n\\] which means the optimal rank \\(R\\) approximation to the random vector \\(\\mathbf{x}\\) may be expressed as \\[\n\\mathbf{\\hat{x}}(\\omega) := \\sum_{r=1}^{R} \\sqrt{\\lambda_r} \\mathbf{\\tilde{w}}_r(\\omega) \\mathbf{v}_r,\n\\] where we have included the \\(\\omega\\) argument to emphasize which quantities are random here. The following result summarizes our findings, extending to the case with non-zero mean.\n\n\nProposition. Let \\(\\mathbf{x}\\) be a square-integrable \\(D\\)-dimensional random vector defined on \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\). Among all such random vectors constrained to take values in an \\(R\\)-dimensional subspace of \\(\\mathbb{R}^D\\), the random vector \\[\n  \\mathbf{\\hat{x}}(\\omega) = \\mathbb{E}[\\mathbf{x}] + \\sum_{r=1}^{R} \\sqrt{\\lambda_r} \\mathbf{\\tilde{w}}_r(\\omega) \\mathbf{v}_r, \\tag{15}\n  \\] provides an optimal approximation to \\(\\mathbf{x}\\), in the expected Euclidean distance sense (14). Moreover, the random weights \\(\\mathbf{\\tilde{w}}_r\\) are uncorrelated, have zero mean and unit variance.\n\n\nNote that random vectors can conceptually be thought of as stochastic processes with finite-dimensional index sets. A similar decomposition to (15) can be constructed for more general stochastic processes with potentially uncountable index sets, with the modification that the sum in (15) will require a countably infinite number of terms. This result is generally known as the Karhunen-Loeve expansion. Thus, from this perspective PCA can be thought of as the special finite-dimensional case of the Karhunen-Loeve expansion.\n\nGaussian Random Vectors\nThe above derivations did not make any assumptions regarding the distribution of \\(\\mathbf{x}\\), just that its covariance exists. Consequently, the main result (15) does not give the distribution of the weights \\(\\mathbf{\\tilde{w}}_r\\), only that they are uncorrelated, have zero mean, and unit variance. If we add the distributional assumption \\(\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{m}, C)\\) then we are able to say more. Indeed, recalling that the (unscaled) weights are given by projections \\(\\mathbf{w}_r = \\langle \\mathbf{x} - \\mathbf{m}, \\mathbf{v}_r \\rangle\\), then we find that \\[\n\\mathbf{w} = V^\\top (\\mathbf{x} - \\mathbf{m}) \\sim \\mathcal{N}(0, V^\\top C V).\n\\] The key point here are that the \\(\\mathbf{w}_r\\) are jointly Gaussian, and hence their uncorrelatedness implies that they are in fact independent. The weights inherit Gaussianity from \\(\\mathbf{x}\\). Thus, under this stronger assumption we are able to characterize the distribution of the weights exactly.\n\n\n\nMaximum Likelihood Estimation with Gaussian Noise\n\n\nProbabilistic PCA\nhttps://stats.stackexchange.com/questions/190308/why-does-probabilistic-pca-use-gaussian-prior-over-latent-variables"
  },
  {
    "objectID": "blog/posts/pca.html#eigenvalue-problems",
    "href": "blog/posts/pca.html#eigenvalue-problems",
    "title": "Principal Components Analysis",
    "section": "Eigenvalue Problems",
    "text": "Eigenvalue Problems\nIn this section, I briefly discuss the spectral norm and eigenvalue problems in finite-dimensional vector spaces, which I utilize above when optimizing the basis \\(B\\) in the PCA derivation. Consider a matrix \\(A \\in \\mathbb{R}^{N \\times D}\\), which represents a linear transformation from \\(\\mathbb{R}^{D}\\) to \\(\\mathbb{R}^N\\). We define the spectral norm of \\(A\\) as the largest factor by which the map \\(A\\) can “stretch” a vector \\(\\mathbf{u} \\in \\mathbb{R}^{D}\\), \\[\n\\lVert A \\rVert_2 := \\max_{\\lVert \\mathbf{u} \\rVert_2 \\neq \\boldsymbol{0}}\n\\frac{\\lVert A\\mathbf{u} \\rVert_2}{\\lVert \\mathbf{u} \\rVert_2}.\n\\] Using the linearity of \\(A\\), one can show that we need only consider vectors of unit length; that is, \\[\n\\lVert A \\rVert_2 = \\max_{\\lVert \\mathbf{u}_2 \\rVert=1} \\lVert A\\mathbf{u} \\rVert_2. \\tag{A1}\n\\] Optimization problems of this type are called eigenvalue problems for reasons that will shortly become clear. For the purposes of the PCA derivation, we will require consideration of a slightly more general eigenvalue problem. To define this problem, first note that the matrix \\(A^\\top A\\) is symmetric, positive semi-definite since \\[\n\\begin{align}\n&(A^\\top A)^\\top = A^\\top (A^\\top)^\\top = A^\\top A,\n&\\mathbf{u}^\\top (A^\\top A)\\mathbf{u} = \\lVert A \\mathbf{u} \\rVert_2^2 \\geq 0.\n\\end{align}\n\\] Thus, by the spectral theorem \\(A^\\top A\\) has \\(D\\) orthogonal eigenvectors, which we will denote by \\(\\mathbf{e}_1, \\dots, \\mathbf{e}_D\\) and assume that they have been normalized to have unit norm. By positive definiteness the respective eigenvalues \\(\\lambda_1, \\dots, \\lambda_D\\) (sorted in decreasing order) are all non-negative. For \\(d = 1, \\dots, D\\), let \\(\\mathcal{E}_d := \\text{span}(\\mathbf{e}_1, \\dots, \\mathbf{e}_d)\\), with \\(\\mathcal{E}^{\\perp}_d := \\text{span}(\\mathbf{e}_{d+1}, \\dots, \\mathbf{e}_D)\\) its orthogonal complement. We now consider the eigenvalue problem\n\\[\n\\begin{align}\n\\max_{\\mathbf{u} \\in \\mathcal{E}^{\\perp}_d, \\lVert \\mathbf{u} \\rVert_2=1} \\lVert A\\mathbf{u} \\rVert_2. \\tag{A2}\n\\end{align}\n\\]\nWe have generalized (A1) by adding an orthogonality constraint. Taking as a convention \\(\\mathcal{E}_0 := \\{\\mathbf{0}\\}\\) we then have \\(\\mathcal{E}^{\\perp}_0 = \\mathbb{R}^D\\), which means that setting \\(d = 0\\) in (A2) recovers the original problem (A1) as a special case. We will prove the following result.\n\n\nProposition. Let \\(A \\in \\mathbb{R}^{N \\times D}\\) be a matrix. Let \\(\\mathbf{e}_1, \\dots, \\mathbf{e}_D\\) denote the orthonormal eigenbasis of \\(A^\\top A\\), with respective eigenvalues \\(\\lambda_1, \\dots, \\lambda_D\\) sorted in descending order by magnitude. For \\(d = 1, \\dots, D\\) define \\(\\mathcal{E}_d := \\text{span}(\\mathbf{e}_1, \\dots, \\mathbf{e}_d)\\), with \\(\\mathcal{E}^{\\perp}_d := \\text{span}(\\mathbf{e}_{d+1}, \\dots, \\mathbf{e}_D)\\) its orthogonal complement. Then \\[\n  \\mathbf{e}_{d+1} = \\text{argmax}_{\\mathbf{u} \\in \\mathcal{E}^{\\perp}_d, \\mathbf{u}=1} \\lVert A\\mathbf{u} \\rVert_2. \\tag{A3}\n  \\] with the maximal value equal to \\(\\sqrt{\\lambda_{d+1}}\\). In particular, we have \\[\n  \\mathbf{e}_1 = \\text{argmax}_{\\lVert \\mathbf{u} \\rVert_2=1} \\lVert A\\mathbf{u} \\rVert_2, \\tag{A4}\n  \\] with maximal value \\(\\sqrt{\\lambda_1}\\).\n\n\nProof. Let \\(\\mathbf{u} \\in \\mathcal{E}^{\\perp}_d\\) be an arbitrary vector of unit length. This vector may be represented with respect to the eigenbasis as \\[\n\\mathbf{u} = \\sum_{r=d+1}^{D} u_r \\mathbf{e}_r, \\qquad u_r := \\langle \\mathbf{u}, \\mathbf{e}_r \\rangle\n\\] We will use this representation to show that 1. \\(\\lVert A\\mathbf{u} \\rVert_2\\) is upper bounded by \\(\\sqrt{\\lambda_{d+1}}\\). 2. The upper bound is achieved by some \\(\\mathbf{u} \\in \\mathcal{E}^{\\perp}_d\\),\nwhich together imply the claimed result. We will actually work with the squared norm instead, which allows us to leverage the inner product. We have \\[\n\\begin{align}\n\\lVert A\\mathbf{u} \\rVert^2_2\n= \\langle A\\mathbf{u}, A\\mathbf{u} \\rangle\n= \\langle A^\\top A\\mathbf{u}, \\mathbf{u} \\rangle\n&= \\left\\langle A^\\top A \\sum_{r=d+1}^{D} u_r \\mathbf{e}_r,\n\\sum_{r=d+1}^{D} u_r \\mathbf{e}_r \\right\\rangle \\newline\n&= \\left\\langle \\sum_{r=d+1}^{D} u_r (A^\\top A \\mathbf{e}_r),\n\\sum_{r=d+1}^{D} u_r \\mathbf{e}_r \\right\\rangle \\newline\n&= \\left\\langle \\sum_{r=d+1}^{D} u_r \\lambda_r \\mathbf{e}_r,\n\\sum_{r=d+1}^{D} u_r \\mathbf{e}_r \\right\\rangle,\n\\end{align}\n\\] having used the fact the \\(\\mathbf{e}_r\\) are eigenvectors of \\(A^\\top A\\). Now we can take advantage of the fact that the \\(\\mathbf{e}_r\\) are orthonormal to obtain \\[\n\\begin{align}\n\\left\\langle \\sum_{r=d+1}^{D} u_r \\lambda_r \\mathbf{e}_r,\n\\sum_{r=d+1}^{D} u_r \\mathbf{e}_r \\right\\rangle\n= \\sum_{r=d+1}^{D} u_r^2 \\lambda_r \\lVert \\mathbf{e}_r \\rVert^2_2\n= \\sum_{r=d+1}^{D} u_r^2 \\lambda_r\n\\leq \\sum_{r=d+1}^{D} u_r^2 \\lambda_{d+1}\n= \\lambda_{d+1} \\lVert \\mathbf{u} \\rVert_2^2\n= \\lambda_{d+1}\n\\end{align}\n\\] where the inequality follows from the fact that the eigenvalues are sorted in descending order. This verifies the upper bound \\(\\lVert A\\mathbf{u} \\rVert_2 \\leq \\sqrt{\\lambda_{d+1}}\\). To show that the bound is achieved, we consider setting \\(\\mathbf{u} = \\mathbf{e}_{d+1}\\). Then,\n\\[\n\\begin{align}\n\\lVert A\\mathbf{e}_{d+1} \\rVert^2_2\n= \\langle A\\mathbf{e}_{d+1}, A\\mathbf{e}_{d+1} \\rangle\n= \\langle A^\\top A\\mathbf{e}_{d+1}, \\mathbf{e}_{d+1} \\rangle\n= \\langle \\lambda_{d+1} \\mathbf{e}_{d+1}, \\mathbf{e}_{d+1} \\rangle\n= \\lambda_{d+1} \\lVert \\mathbf{e}_{d+1} \\rVert^2_2\n= \\lambda_{d+1},\n\\end{align}\n\\]\nso we have indeed verified that the equality \\(\\lVert A\\mathbf{u} \\rVert_2 = \\sqrt{\\lambda_{d+1}}\\) is achieved for some unit-norm vector \\(\\mathbf{u} \\in \\mathcal{E}^{\\perp}_{d}\\). The claim is thus proved. \\(\\qquad \\blacksquare\\)"
  },
  {
    "objectID": "blog/posts/pca.html#projections",
    "href": "blog/posts/pca.html#projections",
    "title": "Principal Components Analysis",
    "section": "Projections",
    "text": "Projections"
  },
  {
    "objectID": "blog/posts/pca.html#truncated-svd-and-eigendecomposition",
    "href": "blog/posts/pca.html#truncated-svd-and-eigendecomposition",
    "title": "Principal Components Analysis",
    "section": "Truncated SVD and Eigendecomposition",
    "text": "Truncated SVD and Eigendecomposition"
  },
  {
    "objectID": "blog/posts/nonlinear-maps-of-Gaussians.html",
    "href": "blog/posts/nonlinear-maps-of-Gaussians.html",
    "title": "Approximating Nonlinear Functions of Gaussians",
    "section": "",
    "text": "In a previous post, I discussed the Kalman filter (KF), which provides the closed-form mean and covariance recursions characterizing the Gaussian filtering distributions for linear Gaussian hidden Markov models. In this post, we retain the Gaussian noise assumption, but generalize to nonlinear dynamics and observation operators. Our primary focus is the additive noise model \\[\n\\begin{align}\nv_{k+1} &= g(v_k) + \\eta_{k+1} && \\eta_{k+1} \\sim \\mathcal{N}(0, Q) \\tag{1} \\newline\ny_{k+1} &= h(v_{k+1}) + \\epsilon_{k+1}, && \\epsilon_{k+1} \\sim \\mathcal{N}(0, R) \\newline\nv_0 &\\sim \\mathcal{N}(m_0, C_0), &&\\{\\epsilon_k\\} \\perp \\{\\eta_k\\} \\perp v_0\n\\end{align}\n\\] but we will also touch on the case where the noise is also subject to nonlinear mapping; i.e.,\n\\[\n\\begin{align}\nv_{k+1} &= g(v_k, \\eta_{k+1}) && \\eta_{k+1} \\sim \\mathcal{N}(0, Q) \\tag{2} \\newline\ny_{k+1} &= h(v_{k+1}, \\epsilon_{k+1}) && \\epsilon_{k+1} \\sim \\mathcal{N}(0, R) \\newline\nv_0 &\\sim \\mathcal{N}(m_0, C_0), &&\\{\\epsilon_k\\} \\perp \\{\\eta_k\\} \\perp v_0\n\\end{align}\n\\]\nNote even this more general formulation is still a special case of the generic Bayesian filtering problem, since we are restricting to the setting with Gaussian noise and a Gaussian initial condition.\nLet \\(Y_k := \\{y_1, \\dots, y_k\\}\\) denote the set of observations up through time step \\(k\\). We seek to characterize the filtering distributions \\(v_k|Y_k\\), and update them in an online fashion as more data arrives. I will denote the density of \\(v_k|Y_k\\) by \\(\\pi_k(v_k) = p(v_k|Y_k)\\). In the linear Gaussian setting these distributions are Gaussian, and can be computed analytically. The introduction of nonlinearity renders the filtering distributions non-Gaussian and not analytically tractable in general, motivating the need for approximations. Certain methods, such as particle filters, are designed to handle the situation where the departure from Gaussianity is severe. The methods discussed in this post, however, are applicable when Gaussian approximations are still reasonable. Given this, the algorithms discussed here all proceed by approximating the current filtering distribution by a Gaussian \\(v_k|Y_k \\sim \\mathcal{N}(m_k, C_k)\\). The algorithms differ in the approximations they employ to deal with the nonlinear functions \\(h\\) and \\(g\\) in order to arrive at a Gaussian approximation of the subsequent filtering distribution \\(v_{k+1}|Y_{k+1} \\sim \\mathcal{N}(m_{k+1}, C_{k+1})\\).\nThere are many methods that fit into this general Gaussian approximation framework. In this post we focus on methods rooted in linearization of the nonlinear functions. Alternative approaches employ quadrature-based (e.g., the unscented Kalman filter) or Monte Carlo (e.g., the ensemble Kalman filter) approximations. Although the motivation stems from the filtering problem, we will focus mostly on the underlying fundamental problem here: approximating the distribution of a nonlinear function of a Gaussian random variable. The next section illustrates how this problem arises in the filtering context."
  },
  {
    "objectID": "blog/posts/nonlinear-maps-of-Gaussians.html#motivating-the-generic-problem",
    "href": "blog/posts/nonlinear-maps-of-Gaussians.html#motivating-the-generic-problem",
    "title": "Approximating Nonlinear Functions of Gaussians",
    "section": "Motivating the Generic Problem",
    "text": "Motivating the Generic Problem\nWe recall from the post on Bayesian filtering that the map \\(\\pi_k \\mapsto \\pi_{k+1}\\) naturally decomposes into two steps: the forecast and analysis steps. We assume here the Gaussian approximation \\(v_k := v_k|Y_k \\sim \\mathcal{N}(m_k, C_k)\\) has been invoked and consider approximating the map \\(\\pi_k \\mapsto \\pi_{k+1}\\).\n\nForecast\nThe forecast distribution \\(\\hat{\\pi}_{k+1}(v_{k+1}) := p(v_{k+1}|Y_k)\\) is the distribution implied by feeding \\(\\pi_k\\) through the stochastic dynamics model. In the additive noise case (1), we observe that this comes down to approximating the distribution of the random variable \\[\ng(v_k) + \\eta_{k+1} , \\qquad v_k \\sim \\mathcal{N}(m_k, C_k), \\ \\eta_{k+1} \\sim \\mathcal{N}(0, Q). \\tag{3}\n\\] However, since we will be invoking a Gaussian approximation \\(g(v_k) \\sim \\mathcal{N}(\\hat{m}_{k+1}, \\hat{C}_{k+1})\\), in addition to the assumption that \\(v_k\\) and \\(\\eta_{k+1}\\) are independent, then we can just focus our attention on the \\(g(v_k)\\) term. Due to independence, once we obtain the approximation \\(g(v_k) \\sim \\mathcal{N}(\\hat{m}_{k+1}, \\hat{C}_{k+1})\\), then \\(g(v_k) + \\eta_{k+1} \\sim \\mathcal{N}(\\hat{m}_{k+1}, \\hat{C}_{k+1} + Q)\\) is immediate.\nIn the non-additive noise case, the problem similarly comes down to approximating the distribution \\[\ng(v_k, \\eta_{k+1}), \\qquad v_k \\sim \\mathcal{N}(m_k, C_k), \\ \\eta_{k+1} \\sim \\mathcal{N}(0, Q). \\tag{4}\n\\] Again, due to the independence assumptions we have that \\((v_k, \\eta_{k+1})\\) are jointly distributed \\[\n\\begin{align}\n\\begin{bmatrix} v_k \\newline \\eta_{k+1} \\end{bmatrix}\n\\sim \\mathcal{N}\\left(\\begin{bmatrix} m_k \\newline 0 \\end{bmatrix},\n  \\begin{bmatrix} C_k & 0 \\newline 0 & Q \\end{bmatrix} \\right).\n\\end{align}\n\\]\nThus, this situation also reduces to approximating the distribution of a nonlinear map of a Gaussian; in this case, the map \\(g(\\tilde{v}_k)\\), where \\(\\tilde{v}_k := (v_k, \\eta_{k+1})^\\top\\) is the Gaussian input.\n\n\nAnalysis\nLet’s now suppose that we have the forecast approximation \\(\\hat{v}_{k+1} := v_{k+1}|Y_k \\sim \\mathcal{N}(\\hat{m}_{k+1}, \\hat{C}_{k+1})\\) in hand. The map \\(\\hat{\\pi}_{k+1} \\mapsto \\pi_{k+1}\\) from forecast to filtering distribution is defined by the action of conditioning on the data \\(y_{k+1}\\). In the additive noise case (1), this entails the following application of Bayes’ theorem, \\[\n\\begin{align}\n\\pi_{k+1}(v_{k+1})\n&\\propto \\mathcal{N}(y_{k+1}|h(v_{k+1}), R)\\mathcal{N}(v_{k+1}|\\hat{m}_{k+1}, \\hat{C}_{k+1}).\n\\end{align}\n\\] Although everything is Gaussian here, the nonlinear function \\(h\\) breaks the Gaussianity of \\(\\pi_{k+1}\\) in general. One idea to deal with this might be to run MCMC and invoke the approximation \\(v_{k+1}|Y_{k+1} \\sim \\mathcal{N}(m_{k+1}, C_{k+1})\\) with \\(m_{k+1}\\) and \\(C_{k+1}\\) set to their empirical estimates computed from the MCMC samples. This has the distinct disadvantage of requiring an MCMC run at every time step.\nIn order to discover alternative approximation methods, it is useful to recall the joint Gaussian view of the analysis step, which I discuss in this post. The idea here was that, in the linear Gaussian setting, \\((v_{k+1}, y_{k+1})|Y_k\\) has a joint Gaussian distribution. The filtering distribution \\(v_{k+1}|Y_{k+1} = v_{k+1}|y_{k+1}, Y_k\\) is then obtained as a conditional distribution of the joint Gaussian, which is available in closed-form. In the present setting of the additive noise model (1) this joint distribution is given by\n\\[\n\\begin{align}\n\\begin{bmatrix} v_{k+1} \\newline y_{k+1} \\end{bmatrix} \\bigg| Y_k\n&= \\begin{bmatrix} \\hat{v}_{k+1} \\newline h(\\hat{v}_{k+1}) + \\epsilon_{k+1} \\end{bmatrix},\n&& \\hat{v}_{k+1} \\sim \\mathcal{N}(\\hat{m}_{k+1}, \\hat{C}_{k+1})\n\\end{align}\n\\]\nwhich is again generally non-Gaussian due to \\(h\\). This perspective points to the idea of approximating this joint distribution as a Gaussian, so that an approximation of the filtering distribution then falls out as a conditional. Notice that we have found ourselves in a very similar situation to the analysis step, in that we again want to approximate the nonlinear mapping of a Gaussian with a Gaussian. The problem is thus to furnish a Gaussian approximation of \\[\n\\begin{align}\n\\tilde{h}(\\hat{v}_{k+1}, \\epsilon_{k+1}) &= \\begin{bmatrix} \\hat{v}_{k+1} \\newline h(\\hat{v}_{k+1}) + \\epsilon_{k+1} \\end{bmatrix},\n&& \\hat{v}_{k+1} \\sim \\mathcal{N}(\\hat{m}_{k+1}, \\hat{C}_{k+1}), \\ \\epsilon_{k+1} \\sim \\mathcal{N}(0, R). \\tag{5}\n\\end{align}\n\\] In the non-additive error case (2), \\(h(\\hat{v}_{k+1}, \\epsilon_{k+1})\\) replaces \\(h(\\hat{v}_{k+1}) + \\epsilon_{k+1}\\) in the above expression. Note that the independence assumptions imply that \\((\\hat{v}_{k+1}, \\epsilon_{k+1})\\) is joint Gaussian so \\(\\tilde{h}(\\hat{v}_{k+1}, \\epsilon_{k+1})\\) is indeed a nonlinear map of a Gaussian."
  },
  {
    "objectID": "blog/posts/nonlinear-maps-of-Gaussians.html#the-generic-problem-setting",
    "href": "blog/posts/nonlinear-maps-of-Gaussians.html#the-generic-problem-setting",
    "title": "Approximating Nonlinear Functions of Gaussians",
    "section": "The Generic Problem Setting",
    "text": "The Generic Problem Setting\nNow that we have identified the fundamental issues in the context of nonlinear filtering, we state the problem in generic terms. The notation used in this section should be viewed anew, not to be confused with the state space notation used above. The task is to provide a Gaussian approximation to a random variable \\(u = f(v)\\), where \\(v\\) is Gaussian-distributed and \\(f\\) is a nonlinear function; more precisely, \\[\n\\begin{align}\nu &= f(v), && v \\sim \\mathcal{N}(m, C), \\quad f: \\mathbb{R}^n \\to \\mathbb{R}^m. \\tag{6}\n\\end{align}\n\\] In the filtering context, the forecast step represented an instantiation of this problem where \\(f = g\\) and hence a special case where the dimensions of the domain and codomain of \\(f\\) are equal. In the analysis step, \\(f\\) is given by the map \\(v \\mapsto (v, h(v))^\\top\\) (ignoring the \\(\\eta/\\epsilon\\) for now) and thus represents the case where \\(m &gt; n\\). Although both of these cases are subsumed by (4), it is also helpful to consider them separately, as the second case has special structure which can present a more challenging problem. We thus define \\[\n\\begin{align}\n\\tilde{u} &= \\tilde{f}(v) := \\begin{bmatrix} v \\newline f(v) \\end{bmatrix}, \\tag{7}\n\\end{align}\n\\] which captures this special case. With the generic problem stated, we now proceed to discuss specific methods which utilize different notions of linearization to produce Gaussian approximations of the distribution of \\(u\\) and \\(\\tilde{u}\\)."
  },
  {
    "objectID": "blog/posts/nonlinear-maps-of-Gaussians.html#taylor-series-approximations",
    "href": "blog/posts/nonlinear-maps-of-Gaussians.html#taylor-series-approximations",
    "title": "Approximating Nonlinear Functions of Gaussians",
    "section": "Taylor Series Approximations",
    "text": "Taylor Series Approximations\nThe first approach we consider leverages a Taylor series approximation of the nonlinear function \\(f\\). When applied to the filtering problem, the resulting algorithm is known as the extended Kalman filter. We note that higher order Taylor approximations are also possible in certain settings, but we restrict to first order approximations here.\n\nThe Generic Method\nWe consider approximating the nonlinear function \\(f\\) with a local linear approximation, given by the Taylor series expansion around the current mean \\(m\\), \\[\n\\begin{align}\nf(v) \\approx f(m) + Df(m)[v - m].\n\\end{align}\n\\] Note that I am applying the Jacobian notation so that \\(Df(m) \\in \\mathbb{R}^{m \\times n}\\). Under this approximation we use the fact that \\(v \\sim \\mathcal{N}(m, C)\\) to obtain \\[\n\\begin{align}\nu = f(v) & \\approx f(m) + Df(m)[v - m] \\tag{8} \\newline\n&\\sim \\mathcal{N}(f(m), [Df(m)]C [Df(m)]^\\top).\n\\end{align}\n\\]\nThe situation for \\(\\tilde{f}\\) is quite similar: \\[\n\\begin{align}\n\\tilde{f}(v) &\\approx \\tilde{f}(m) + D\\tilde{f}(m)[v - m] \\tag{9} \\newline\n&= \\begin{bmatrix} m \\newline f(m) \\end{bmatrix} + \\begin{bmatrix} I \\newline Df(m) \\end{bmatrix}[v-m] \\newline\n&\\sim\n\\mathcal{N}\\left(\\begin{bmatrix} m \\newline f(m) \\end{bmatrix},\n\\begin{bmatrix} I \\newline Df(m) \\end{bmatrix} C \\begin{bmatrix} I \\newline Df(m) \\end{bmatrix}^\\top \\right) \\newline\n&= \\mathcal{N}\\left(\\begin{bmatrix} m \\newline f(m) \\end{bmatrix},\n\\begin{bmatrix} C & C[Df(m)]^\\top \\newline [Df(m)]C & [Df(m)]C[Df(m)]^\\top \\end{bmatrix} \\right)\n\\end{align}\n\\] where the last equality is in distribution. It is important to stress that these are local approximations; the linearization is constructed using only the local derivative information at the point \\(m\\). Thus, we would expect the quality of the approximation to decay for points farther from \\(m\\), and this decay to be more severe for \\(f\\) which are highly nonlinear. Thus, intuitively we would expect the approximation (8) to be reasonable when the distribution of \\(v\\) is tightly clustered around its mean. Distributions that are more diffuse will naturally lead to poorer approximations given that more of the probability mass exists in regions where the local linear approximation is not adequate. The situation in (9) presents an even greater concern; the quality of this approximation relies on the joint distribution of \\((v, f(v))\\) staying close to its mean. Not only does this require the current distribution of \\(v\\) to be concentrated about \\(m\\), but also the image \\(f(v)\\) to be clustered about \\(f(m)\\). Thus, even if \\(v\\) is tightly bound to its mean, highly nonlinear maps \\(f\\) have the potential to yield a large spread of points in the codomain and thus reduce the quality of the approximation.\n\n\nApplication: The Extended Kalman Filter\nWe now apply these generic equations to the filtering settings (1) and (2), again breaking the problem into the forecast and analysis steps. The resulting approximate filtering algorithm is called the extended Kalman filter (EKF).\n\nForecast\nAssume the filtering distribution at time \\(k\\) is given by \\(v_k \\sim \\mathcal{N}(m_k, C_k)\\). Starting with the additive noise model (1), we see that we must approximate the distribution \\(g(v_k)\\). Applying (8) and then adding the independent Gaussian \\(\\eta_{k+1}\\) yields the approximate forecast distribution \\[\n\\begin{align}\n\\hat{v}_{k+1} := v_{v+1}|Y_k \\sim \\mathcal{N}(g(m_k), [Dg(m_k)]C_k [Dg(m_k)]^\\top + Q). \\tag{10}\n\\end{align}\n\\] This is quite similar to the forecast distribution for the Kalman filter, which is \\(\\mathcal{N}(Gm_k, GC_k G^\\top + Q)\\) corresponding to the linear forward model \\(g(v) = Gv\\). We see that the EKF forecast covariance is equivalent to that obtained from the Kalman filter applied with the linear forward model \\(G := Dg(m_k)\\).\nThe case of the non-additive noise (4) is similar, but now we must approximate \\(g(v_k, \\eta_{k+1})\\). Recall that \\((v_k, \\eta_{k+1})\\) is joint Gaussian distributed, with mean \\((m_k, 0)\\). Applying (8) thus yields \\[\n\\begin{align}\nv_{v+1}|Y_k &\\sim\n\\mathcal{N}\\left(g(m_k,0), [Dg(m_k,0)]\\begin{bmatrix} C_k & 0 \\newline 0 & Q \\end{bmatrix} [Dg(m_k,0)]^\\top\\right) \\newline\n&= \\mathcal{N}\\left(g(m_k,0), [D_vg(m_k,0)]C_k [D_vg(m_k,0)]^\\top + [D_{\\eta}g(m_k,0)]Q [D_{\\eta}g(m_k,0)]^\\top\\right), \\tag{11}\n\\end{align}\n\\] where the equality is in distribution and the subscripts \\(D_v\\), \\(D_{\\eta}\\) indicate the respective partial derivatives. Note the similarity between (10) and (11). The general form is the same, but the non-additive case requires derivatives with respect to the noise \\(\\eta\\) in order to approximate the effect of pushing \\(\\eta\\) through the nonlinear forward model. Following our intuition on when we expect the Taylor series linearization to be reasonable, we now observe that the approximation may deteriorate when either the current state \\(v_k\\) or the stochastic noise \\(\\eta_{k+1}\\) is highly variable, in which case significant probability mass may be present in regions far from the point \\((m_k, 0)\\) about which the Taylor series is expanded.\n\n\nAnalysis\nStarting with the additive noise model, we recall that the analysis step requires approximation of (5). To this end, we apply (9) with \\(\\tilde{f}(\\hat{v}_{k+1}) = (\\hat{v}_{k+1}, h(\\hat{v}_{k+1}))^\\top\\) where \\(\\hat{v}_{k+1} \\sim \\mathcal{N}(\\hat{m}_{k+1}, \\hat{C}_{k+1})\\). We actually require approximation of \\((\\hat{v}_{k+1}, y_{k+1})^\\top = (\\hat{v}_{k+1}, h(\\hat{v}_{k+1}) + \\epsilon_{k+1})^\\top\\) but due to independence we can simply add \\((0, \\epsilon_{k+1})^\\top\\) post-hoc. The combination of (9) with the addition of the noise term gives \\[\n\\begin{align}\n\\begin{bmatrix} \\hat{v}_{k+1} \\newline h(\\hat{m}_{k+1}) + \\epsilon_{k+1} \\end{bmatrix}\n\\sim \\mathcal{N}\\left(\\begin{bmatrix} \\hat{m}_{k+1} \\newline h(\\hat{m}_{k+1}) \\end{bmatrix},\n\\begin{bmatrix} \\hat{C}_{k+1} & \\hat{C}_{k+1}[Dh(\\hat{m}_{k+1})]^\\top \\newline\n[Dh(\\hat{m}_{k+1})]\\hat{C}_{k+1} & [Dh(\\hat{m}_{k+1})]\\hat{C}_{k+1} [Dh(\\hat{m}_{k+1})]^\\top + R \\end{bmatrix} \\right) \\tag{12}\n\\end{align}\n\\]"
  },
  {
    "objectID": "blog/posts/gp-specifications.html",
    "href": "blog/posts/gp-specifications.html",
    "title": "Introduction to Gaussian Process Priors and Hyperparameter Estimation",
    "section": "",
    "text": "Gaussian processes (GP) are widely utilized across various fields, each with their own preferences, terminology, and conventions. Some notable domains that make significant use of GPs include - Spatial statistics (kriging) - Design and analysis of computer experiments (emulator/surrogate modeling) - Bayesian optimization - Machine learning\nEven if you’re a GP expert in one of these domains, these differences can make navigating the GP literature in other domains a bit tricky. The goal of this post is to summarize common approaches for specifying GP distributions, and emphasize conventions and assumptions that tend to differ across fields. By “specifying GP distributions”, what I am really talking about here is parameterizing the mean and covariance functions that define the GP. While GPs are non-parametric models in a certain sense, specifying and learning the hyperparameters making up the mean and covariance functions is a crucial step to successful GP applications. I will discuss popular parameterizations for these functions, and different algorithms for learning these parameter values from data. In the spirit of drawing connections across different domains, I will try my best to borrow terminology from different fields, and will draw attention to synonymous terms by using boldface."
  },
  {
    "objectID": "blog/posts/gp-specifications.html#background",
    "href": "blog/posts/gp-specifications.html#background",
    "title": "Introduction to Gaussian Process Priors and Hyperparameter Estimation",
    "section": "Background",
    "text": "Background\n\nGaussian Processes\nGaussian processes (GPs) define a probability distribution over a space of functions in such a way that they can be viewed as a generalization of Gaussian random vectors. Just as Gaussian vectors are defined by their mean vector and covariance matrix, GPs are defined by a mean and covariance function. We will interchangeably refer to the latter as either the covariance function or kernel.\nWe will consider GPs defined over a space of functions of the form \\(f: \\mathcal{X} \\to \\mathbb{R}\\), where \\(\\mathcal{X} \\subseteq \\mathbb{R}^d\\). We will refer to elements \\(x \\in \\mathcal{X}\\) as inputs or locations and the images \\(f(x) \\in \\mathbb{R}\\) as outputs or responses. If the use of the word “locations” seems odd, note that in spatial statistical settings, the inputs \\(x\\) are often geographic coordinates. We will denote the mean and covariance function defining the GP by \\(\\mu: \\mathcal{X} \\to \\mathbb{R}\\) and \\(k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\), respectively. The mean function is essentially unrestricted, but the covariance function \\(k(\\cdot, \\cdot)\\) must be a valid positive definite kernel. If \\(f(\\cdot)\\) is a GP with mean function \\(\\mu(\\cdot)\\) and kernel \\(k(\\cdot, \\cdot)\\) we will denote this by \\[\n\\begin{align}\nf \\sim \\mathcal{GP}(\\mu, k). \\tag{1}\n\\end{align}\n\\]\nThe defining property of GPs is that their finite-dimensional distributions are Gaussian; that is, for an arbitrary finite set of \\(n\\) inputs \\(X := \\{x_1, \\dots, x_N\\} \\subset \\mathcal{X}\\), the vector \\(f(X) \\in \\mathbb{R}^n\\) is distributed as \\[\n\\begin{align}\nf(X) \\sim \\mathcal{N}(\\mu(X), k(X, X)). \\tag{2}\n\\end{align}\n\\] We are vectorizing notation here so that \\([f(X)]_i := f(x_i)\\), \\([\\mu(X)]_i := \\mu(x_i)\\), and \\([k(X, X)]_{i,j} := k(x_i, x_j)\\). When the two input sets to the kernel are equal, we lighten notation by writing \\(k(X) := k(X, X)\\). Now suppose we have two sets of inputs \\(X\\) and \\(\\tilde{X}\\), containing \\(n\\) and \\(m\\) inputs, respectively. The defining property (2) then implies\n\\[\n\\begin{align}\n\\begin{bmatrix} f(\\tilde{X}) \\newline f(X) \\end{bmatrix}\n&\\sim \\mathcal{N}\\left(\n  \\begin{bmatrix} \\mu(\\tilde{X}) \\newline \\mu(X) \\end{bmatrix},\n  \\begin{bmatrix}\n  k(\\tilde{X}) & k(\\tilde{X}, X) \\newline\n  k(X, \\tilde{X}) & k(X)\n  \\end{bmatrix}\n\\right). \\tag{3}\n\\end{align}\n\\]\nThe Gaussian joint distribution (3) implies that the conditional distributions are also Gaussian. In particular, the distribution of \\(f(\\tilde{X})|f(X)\\) can be obtained by applying the well-known Gaussian conditioning identities:\n\\[\n\\begin{align}\nf(\\tilde{X})|f(X) &\\sim \\mathcal{N}(\\hat{\\mu}(\\tilde{X}), \\hat{k}(\\tilde{X})), \\tag{4} \\newline\n\\hat{\\mu}(\\tilde{X}) &:= \\mu(\\tilde{X}) + k(\\tilde{X}, X)k(X)^{-1} [f(X) - \\mu(X)] \\newline\n\\hat{k}(\\tilde{X}) &:= k(\\tilde{X}) - k(\\tilde{X}, X)k(X)^{-1} k(X, \\tilde{X}).\n\\end{align}\n\\] The fact that the result (4) holds for arbitrary finite sets of inputs \\(\\tilde{X}\\) implies that the conditional \\(f | f(X)\\) is also a GP, with mean and covariance functions \\(\\hat{\\mu}(\\cdot)\\) and \\(\\hat{k}(\\cdot, \\cdot)\\) defined by (4). On a terminology note, the \\(n \\times n\\) matrix \\(k(X)\\) is often called the kernel matrix. This is the matrix containing the kernel evaluations at the set of \\(n\\) observed locations.\n\n\nRegression with GPs\nOne common application of GPs is their use as a flexible nonlinear regression model. Let’s consider the basic regression setup with observed data pairs \\((x_1, y_1), \\dots, (x_n, y_n)\\). We assume that the \\(y_i\\) are noisy observations of some underlying latent function output \\(f(x_i)\\). The GP regression model arises by placing a GP prior distribution on the latent function \\(f\\). We thus consider the regression model \\[\n\\begin{align}\ny(x) &= f(x) + \\epsilon(x) \\tag{5} \\newline\nf &\\sim \\mathcal{GP}(\\mu, k) \\newline\n\\epsilon(x) &\\overset{iid}{\\sim} \\mathcal{N}(0, \\sigma^2),\n\\end{align}\n\\] where we have assumed a simple additive Gaussian noise model. This assumption is quite common in the GP regression setting due to the fact that it results in closed-form conditional distributions, similar to (4). We will assume the error model (5) throughout this post, but note that there are many other possibilities if one is willing to abandon closed-form posterior inference.\nThe solution of the regression problem is given by the distribution of \\(f(\\cdot)|y(X)\\) or \\(y(\\cdot)|y(X)\\), where \\(y(X)\\) is the \\(n\\)-dimensional vector of observed responses. The first distribution is the posterior on the latent function \\(f\\), while the second incorporates the observation noise as well. Both distributions can be derived in the same way, so we focus on the second. Letting \\(\\tilde{X}\\) denote a set of \\(m\\) inputs at which we would like to predict the response, consider the joint distribution \\[\n\\begin{align}\n\\begin{bmatrix} y(\\tilde{X}) \\newline y(X) \\end{bmatrix}\n&\\sim \\mathcal{N}\\left(\n  \\begin{bmatrix} \\mu(\\tilde{X}) \\newline \\mu(X) \\end{bmatrix},\n  \\begin{bmatrix}\n  k(\\tilde{X}) + \\sigma^2 I_m & k(\\tilde{X}, X) \\newline\n  k(X, \\tilde{X}) & k(X) + \\sigma^2 I_n\n  \\end{bmatrix}\n\\right). \\tag{6}\n\\end{align}\n\\] This is quite similar to (3), but now takes into account the noise term \\(\\epsilon\\). This does not affect the mean vector since \\(\\epsilon\\) is mean-zero; nor does it affect the off-diagonal elements of the covariance matrix since \\(\\epsilon\\) and \\(f\\) were assumed independent. Applying the Gaussian conditioning identities (4) yields the posterior distribution \\[\n\\begin{align}\ny(\\tilde{X})|y(X) &\\sim \\mathcal{N}(\\hat{\\mu}(\\tilde{X}), \\hat{k}(\\tilde{X})), \\tag{7} \\newline\n\\hat{\\mu}(\\tilde{X}) &:= \\mu(\\tilde{X}) + k(\\tilde{X}, X)[k(X) + \\sigma^2 I_n]^{-1} [f(X) - \\mu(X)] \\newline\n\\hat{k}(\\tilde{X}) &:= \\sigma^2 I_m + k(\\tilde{X}) - k(\\tilde{X}, X)[k(X) + \\sigma^2 I_n]^{-1} k(X, \\tilde{X}).\n\\end{align}\n\\] We will refer to (7) as the GP posterior, predictive, or generically conditional, distribution. We observe that these equations are identical to (4), modulo the appearance of \\(\\sigma^2\\) in the predictive mean and covariance equations. The distribution \\(f(\\tilde{X})|y(X)\\) is identical to (7), except that the \\(\\sigma^2 I_m\\) is removed in the predictive covariance. Again, this reflects the subtle distinction between doing inference on the latent function \\(f\\) versus on the observation process \\(y\\).\n\n\nNoise, Nuggets, and Jitters\nObserve that this whole regression procedure is only slightly different from the noiseless GP setting explored in the previous section (thanks to the Gaussian likelihood assumption). Indeed, the conditional distribution of \\(f(\\tilde{X})|y(X)\\) is derived from \\(f(\\tilde{X})|f(X)\\) by simply replacing \\(k(X)\\) with \\(k(X) + \\sigma^2 I_n\\) (obtaining the distribution \\(y(\\tilde{X})|y(X)\\) requires the one additional step of adding \\(\\sigma^2 I_m\\) to the predictive covariance). In other words, we have simply applied standard GP conditioning using the modified kernel matrix \\[\n\\begin{align}\nC(X) := k(X) + \\sigma^2 I_n. \\tag{8}\n\\end{align}\n\\] We thus might reasonably wonder if the model (5) admits an alternative equivalent representation by defining a GP directly on the observation process \\(y\\). Defining such a model would require defining a kernel \\(c: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\) that is consistent with (8). This route is fraught with difficulties and subtleties, which I will do my best to describe clearly here. At first glance, it seems like the right choice is \\[\n\\begin{align}\nc(x, x^\\prime) := k(x, x^\\prime) + \\sigma^2 \\delta(x, x^\\prime), \\tag{9}\n\\end{align}\n\\] where \\(\\delta(x, x^\\prime) := 1[x = x^\\prime]\\) is sometimes called the stationary white noise kernel. Why isn’t this quite right? Notice in (9) that \\(\\sigma^2\\) is added whenever the inputs \\(x = x^\\prime\\) are equal. However, suppose we observe multiple independent realizations of the process at the same inputs \\(X\\). In the regression model (9) the errors \\(\\epsilon(x)\\) are independent across these realizations, even at the same locations. However, this will not hold true in the model under (9), since \\(\\delta(x, x^\\prime)\\) only sees the values of the inputs, and has no sense of distinction across realizations. We might try to fix this by writing something like \\[\n\\begin{align}\nc(x_i, x_j) := k(x_i, x_j) + \\sigma^2 \\delta_{ij}, \\tag{10}\n\\end{align}\n\\] where the Delta function now depends on the labels \\(i, j\\) instead of the values of the inputs. In the spatial statistics literature, it is not uncommon to see a covariance function defined like (10), but this is basically a notational hack. A kernel is a function of two inputs from \\(\\mathcal{X}\\) - we can’t have it also depending on some side information like the labels \\(i, j\\). At the end of the day, (9) and (10) are attempts to incorporate some concept of white noise inside the kernel itself, rather than via a hierarchical model like (5). I would just stick with the hierarchical model, which is easily rigorously defined and much more intuitive.\nNonetheless, one should not be surprised if expressions like (10) pop up, especially in the spatial statistics literature. Spatial statisticians refer to the noise term \\(\\epsilon(x)\\) as the nugget, and \\(\\sigma^2\\) the nugget variance (sometimes these terms are conflated). In this context, instead of representing observation noise, \\(\\sigma^2\\) is often thought of as representing some unresolved small-scale randomness in the spatial field itself. If you imagine sampling a field to determine the concentration of some mineral across space, then you would hope that repeated measurements (taken around the same time) would yield the same values. Naturally, they may not, and the introduction of the nugget is one way to account for this.\nWhile this discussion may seem needlessly abstract, we recall that the effect of incorporating the noise term (however you want to interpret it) is to simply replace the kernel matrix \\(k(X)\\) with the new matrix \\(c(X) = k(X) + \\sigma^2 I_n\\). Confusingly, there is one more reason (having nothing to do with observation error or nuggets) that people use a matrix of the form \\(c(X)\\) in place of \\(k(X)\\): numerical stability. Indeed, even though \\(k(X)\\) is theoretically positive definite, in practice its numerical instantiation may fail to have this property. A simple approach to deal with this is to add a small, fixed constant \\(\\sigma^2\\) to the diagonal of the kernel matrix. In this context, \\(\\sigma^2\\) is often called the jitter. While computationally its effect is the same as the nugget, note that its introduction is motivated very differently. The jitter is not stemming from some sort of random white noise; it is purely a computational hack to improve the conditioning of the kernel matrix. Check out this thread for some entertaining debates on the use of the nugget and jitter concepts.\n\n\nParameterized Means and Kernels\nEverything we have discussed this far assumes fixed mean and covariance functions. In practice, suitable choices for these quantities are not typically known. Thus, the usual approach is to specify some parametric families \\(\\mu = \\mu_{\\psi}\\) and \\(k = k_{\\phi}\\) and learn their parameters from data. The parameters \\(\\psi\\) and \\(\\phi\\) are often referred to as hyperparameters, since they are not the primary parameters of interest in the GP regression model. Recalling from (5) that the GP acts as a prior distribution on the latent function, we see that \\(\\psi\\) and \\(\\phi\\) control the specification of this prior distribution. In addition to \\(\\psi\\) and \\(\\phi\\), the parameter \\(\\sigma^2\\) is also typically not known. I will not wade back into the previous section’s debate in arguing whether this should be classified as a “hyperparameter” or not. In any case, let’s let \\(\\theta := \\{\\psi, \\phi, \\sigma^2 \\}\\) denote the full set of (hyper)parameters that must be learned from data.\n\nMean Functions\nThe machine learning community commonly uses the simplest possible form for the mean function: \\(\\mu(x) \\equiv 0\\). This zero-mean assumption is less restrictive than it seems, since GPs mainly derive their expressivity from the kernel. A slight generalization is to allow a constant, non-zero mean \\(\\mu(x) \\equiv \\beta_0\\), where \\(\\beta_0 \\in \\mathbb{R}\\). However, constant (including zero-mean) GP priors can have some undesirable properties; e.g., in the context of extrapolation. Sometimes one wants more flexibility, and in these cases it is quite common to consider some sort of linear regression model \\[\n\\begin{align}\n\\mu(x) = h(x)^\\top \\beta, \\tag{11}\n\\end{align}\n\\] where \\(h: \\mathcal{X} \\to \\mathbb{R}^p\\) is some feature map and \\(\\beta \\in \\mathbb{R}^p\\) the associated coefficient vector. For example, \\(h(x) = [1, x^\\top]^\\top\\) would yield a standard linear model, and \\(h(x) = [1, x_1, \\dots, x_d, x_1^2, \\dots, x_d^2]^\\top\\) would allow for a quadratic trend.\n\n\nKernels\nThe positive definite restriction makes defining valid covariance functions much more difficult than defining mean functions. Thus, one typically falls back on one of a few popular choices of known parametric kernel families (though note that kernels can be combined in various ways to give a large variety of options). While the goal of this post is not to explore specific kernels, in order to have a concrete example in mind consider the following parameterization: \\[\n\\begin{align}\nk(x, \\tilde{x}) = \\alpha^2 \\sum_{j=1}^{d} \\left(-\\frac{\\lvert x^{j} - \\tilde{x}^j \\rvert}{\\ell^j}\\right)^2.\n\\tag{12}\n\\end{align}\n\\] Note that I’m using superscripts to index vector entries here. This kernel goes by many names, including exponentiated quadratic, squared exponential, Gaussian, radial basis function, and automatic relevance determination. The parameter \\(\\alpha^2\\) is sometimes called the marginal variance, or just the scale parameter. The parameters \\(\\ell^1, \\dots, \\ell^d\\) are often called lengthscale, smoothness, or range parameters, since they control the smoothness of the GP realizations along each coordinate direction. Other popular kernels (e.g., Matérn) have analogous parameters controlling similar features. Note that in this example we have \\(\\phi = \\{\\alpha^2, \\ell^1, \\dots, \\ell^d \\}\\). Also note that people choose to parameterize the Gaussian kernel in many different ways; for example, it’s not uncommon to see a \\(1/2\\) factor included inside the exponential to make the kernel align with the typical parameterization of the Gaussian probability density function. Knowing which parameterization you’re working with is important for interpreting the hyperparameters, specifying bounds, defining priors, etc.\nIt is quite common in the spatial statistics (and sometimes the computer experiments) literature to see kernels written like \\(\\alpha^2 k(\\cdot, \\cdot)\\); in these cases \\(k(\\cdot, \\cdot)\\) typically represents a correlation function, which becomes the covariance function after multiplying by the marginal variance \\(\\alpha^2\\). There is an advantage in decomposing the kernel this way when it comes to estimating the hyperparameters, which we will discuss shortly.\n\n\n\nThe GP (Marginal) Likelihood Function\nLet’s first recall the GP regression model (5) \\[\n\\begin{align}\ny(x) &= f(x) + \\epsilon(x) \\newline\nf &\\sim \\mathcal{GP}(\\mu_{\\psi}, k_{\\phi}) \\newline\n\\epsilon &\\overset{iid}{\\sim} \\mathcal{N}(0, \\sigma^2),\n\\end{align}\n\\] where we have now explicitly added the dependence on \\(\\psi\\) and \\(\\phi\\). This model is defined for any \\(x \\in \\mathcal{X}\\). However, when estimating hyperparameters, we will naturally be restricting the model to \\(X\\), the finite set of locations at which we actually have observations. Restricting to \\(X\\) reduces the above model to the standard (finite-dimensional) Bayesian regression model \\[\n\\begin{align}\ny(X)|f(X), \\theta &\\sim \\mathcal{N}(f(X), \\sigma^2 I_n) \\tag{13} \\newline\nf(X)|\\theta &\\sim \\mathcal{N}(\\mu_{\\psi}(X), k_{\\phi}(X)).\n\\end{align}\n\\] We could consider completing the Bayesian specification by defining a prior on \\(\\theta\\), but we’ll hold off on this for now. Notice that the model (13) defines a joint distribution over \\([y(X), f(X)] | \\theta\\), with \\(y(X)|f(X), \\theta\\) representing the likelihood of the observations at the observed input locations \\(X\\). At present everything is conditional on a fixed \\(\\theta\\). Now, if we marginalize the likelihood \\(y(X)|f(X), \\theta\\) with respect to \\(f(X)\\) then we obtain the distribution \\(y(X) | \\theta\\). This is often called the marginal likelihood, due to the fact that \\(f\\) was marginalized out. In particular, the distribution \\(y(X) | \\theta\\) has implicitly marginalized the function values \\(f(\\tilde{x})\\) at all location \\(\\tilde{x}\\) other than \\(X\\). This same logic and terminology applies in the noiseless setting with \\(\\sigma^2 = 0\\), in which case the marginal likelihood is given by \\(f(X) | \\theta\\). In the noiseless setting we are marginalizing both over the unobserved function values and the noise \\(\\epsilon\\). Thanks to all the Gaussian assumptions here, the marginal likelihood is available in closed-form. One could approach the derivation using (13) as the starting point, but it’s much easier to consider the model written out using random variables, \\[\n\\begin{align}\ny(X) &= f(X) + \\epsilon(X).\n\\end{align}\n\\] Since \\(f(X)\\) and \\(\\epsilon(X)\\) are independent Gaussians, then their sum is also Gaussian with mean and covariance given by \\[\n\\begin{align}\n\\mathbb{E}[y(X)|\\theta]\n&= \\mathbb{E}[f(X)|\\theta] + \\mathbb{E}[\\epsilon(X)|\\theta] = \\mu_{\\psi}(X) \\newline\n\\text{Cov}[y(X)|\\theta]\n&= \\text{Cov}[f(X)|\\theta] + \\text{Cov}[\\epsilon(X)|\\theta]\n= k_{\\phi}(X) + \\sigma^2 I_n.\n\\end{align}\n\\] We have thus found that \\[\n\\begin{align}\ny(X)|\\theta \\sim \\mathcal{N}\\left(\\mu_{\\psi}(X), C_{\\phi, \\sigma^2}(X)\\right), \\tag{14}\n\\end{align}\n\\] recalling the definition \\(C_{\\phi, \\sigma^2}(X) := k_{\\phi}(X) + \\sigma^2 I_n\\). We will let \\(\\mathcal{L}(\\theta)\\) denote the log density of this Gaussian distribution; i.e. the log marginal likelihood: \\[\n\\begin{align}\n\\mathcal{L}(\\theta)\n&:= -\\frac{1}{2} \\log \\text{det}\\left(2\\pi C_{\\phi, \\sigma^2}(X) \\right) -\n\\frac{1}{2} (y(X) - \\mu_{\\psi}(X))^\\top C_{\\phi, \\sigma^2}(X)^{-1} (y(X) - \\mu_{\\psi}(X)) \\tag{15}\n\\end{align}\n\\] The function \\(\\mathcal{L}(\\theta)\\) plays a central role in the typical approach to hyperparameter optimization, as we will explore below. Also note that the above derivations also apply to the noiseless setting (i.e., \\(y(X) = f(X)\\)) by setting \\(\\sigma^2 = 0\\). In this case, the marginal likelihood is simply the GP distribution restricted to the inputs \\(X\\).\nI have henceforth been a bit verbose with the notation in (15) to make very explicit the dependence on the inputs \\(X\\) and the hyperparameters. To lighten notation a bit, we define \\(y_n := y(X)\\), \\(\\mu_{\\psi} := \\mu_{\\psi}(X)\\), and \\(C_{\\phi, \\sigma^2} := C_{\\phi, \\sigma^2}(X)\\), allowing us to rewrite (15) as \\[\n\\begin{align}\n\\mathcal{L}(\\theta)\n&:= -\\frac{1}{2} \\log \\text{det}\\left(2\\pi C_{\\phi, \\sigma^2} \\right) -\n\\frac{1}{2} (y_n - \\mu_{\\psi})^\\top C_{\\phi, \\sigma^2}^{-1} (y_n - \\mu_{\\psi}). \\tag{16}\n\\end{align}\n\\] We have simply suppressed the explicit dependence on \\(X\\) in the notation."
  },
  {
    "objectID": "blog/posts/gp-specifications.html#maximum-marginal-likelihood-or-empirical-bayes",
    "href": "blog/posts/gp-specifications.html#maximum-marginal-likelihood-or-empirical-bayes",
    "title": "Introduction to Gaussian Process Priors and Hyperparameter Estimation",
    "section": "Maximum Marginal Likelihood, or Empirical Bayes",
    "text": "Maximum Marginal Likelihood, or Empirical Bayes\nRecall that (16) gives the expression for the log marginal likelihood \\(\\mathcal{L}(\\theta)\\), which is just the log density of \\(y(X)|\\theta\\) viewed as a function of \\(\\theta\\). A natural approach is to set the hyperparameters \\(\\theta\\) to their values that maximize \\(\\mathcal{L}(\\theta)\\): \\[\n\\begin{align}\n\\hat{\\theta} := \\text{argmax} \\ \\mathcal{L}(\\theta). \\tag{17}\n\\end{align}\n\\] At first glance, the Gaussian form of \\(\\mathcal{L}(\\theta)\\) might look quite friendly to closed-form optimization. After all, maximum likelihood estimates of the mean and covariance of Gaussian vectors are indeed available in closed-form. However, upon closer inspection notice that the covariance is not being directly optimized; we are optimizing \\(\\phi\\), and the covariance \\(C_{\\phi, \\sigma^2}\\) is a nonlinear function of this parameter. Thus, in general some sort of iterative numerical scheme is is used for the optimization. Typically, gradient-based approaches are preferred, meaning we must be able to calculate quantities like \\(\\frac{\\partial}{\\partial \\phi} C_{\\phi, \\sigma^2}\\). The exact gradient calculations will thus depend on the choice of kernel; specifics on kernels and optimization schemes are not the focus of this post. We will instead focus on the high level ideas here. The general approach to GP regression that we have outlined so far can be summarized as: 1. Solve the optimization problem (17) and fix the hyperparameters at their optimized values \\(\\hat{\\theta}\\). The hyperparameters will be fixed from this point onward. 2. Use the GP predictive equations (7) to perform inference at a set of locations of interest \\(\\tilde{X}\\).\nOne might object to the fact that we are estimating the hyperparameters from data, and then neglecting the uncertainty in \\(\\hat{\\theta}\\) during the prediction step. It is true that this uncertainty is being ignored, but it is also very computationally convenient to do so. We will discuss alternatives later on, but this simple approach is probably the most commonly used in practice today. One way to think about this strategy is in an empirical Bayes context; that is, we can view this approach as an approximation to a fully Bayesian hierarchical model, which would involve equipping the hyperparameters with their own priors. Instead of marginalizing the hyperparameters, we instead fix them at their most likely values with respect to the observed data. We are using the data to “fine tune” the GP prior distribution. In the literature you will see this general hyperparameter optimization strategy referred to as either empirical Bayes, maximum marginal likelihood, or even just maximum likelihood."
  },
  {
    "objectID": "blog/posts/gp-specifications.html#special-case-closed-form-solutions-mean-function",
    "href": "blog/posts/gp-specifications.html#special-case-closed-form-solutions-mean-function",
    "title": "Introduction to Gaussian Process Priors and Hyperparameter Estimation",
    "section": "Special Case Closed-Form Solutions: Mean Function",
    "text": "Special Case Closed-Form Solutions: Mean Function\nAs mentioned above, in general the maximization of \\(\\mathcal{L}(\\theta)\\) requires numerical methods. However, in certain cases elements of \\(\\theta\\) can be optimized in closed-form, meaning that numerical optimization may only be required for a subset of the hyperparameters. We start by considering closed form optimizers for the parameters defining the mean functions.\n\nConstant Mean: Plug-In MLE\nWith the choice of constant mean \\(\\mu_{\\psi}(x) \\equiv \\beta_0\\) the log marginal likelihood becomes\n\\[\n\\begin{align}\n\\mathcal{L}(\\theta)\n&:= -\\frac{1}{2} \\log \\text{det}\\left(2\\pi C_{\\phi, \\sigma^2} \\right) -\n\\frac{1}{2} (y_n - \\beta_0 1_n)^\\top C_{\\phi, \\sigma^2}(X)^{-1} (y_n - \\beta_0 1_n),\n\\end{align}\n\\]\nwith \\(1_n \\in \\mathbb{R}^n\\) denoting a vector of ones. We now consider optimizing \\(\\mathcal{L}(\\theta)\\) as a function of \\(\\beta_0\\) only. The partial derivative with respect to the constant mean equals \\[\n\\begin{align}\n\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\beta_0}\n&= y_n^\\top C_{\\phi, \\sigma^2}^{-1}1_n - \\beta_0 1_n^\\top C_{\\phi, \\sigma^2}^{-1} 1_n.    \\tag{18}\n\\end{align}\n\\] Setting (18) equal to zero and solving for \\(\\beta_0\\) gives the optimum \\[\n\\begin{align}\n\\hat{\\beta}_0(\\phi, \\sigma^2) = \\frac{y_n^\\top C_{\\phi, \\sigma^2}^{-1} 1_n}{1_n^\\top C_{\\phi, \\sigma^2}^{-1} 1_n}. \\tag{19}\n\\end{align}\n\\] Notice that \\(\\hat{\\beta}_0(\\phi, \\sigma^2)\\) depends on the values of the other hyperparameters \\(\\phi\\) and \\(\\sigma^2\\). Therefore, while this does not give us the outright value for the mean, we can plug \\(\\hat{\\beta}_0(\\phi, \\sigma^2)\\) in place of \\(\\beta_0\\) in the marginal likelihood. This yields the profile likelihood (aka the concentrated likelihood), which is no longer a function of \\(\\beta_0\\) and hence the dimensionality of the subsequent numerical optimization problem has been reduced.\n\n\nLinear Model Coefficients: Plug-In MLE\nLet’s try to do the same thing with the mean function \\(\\mu_{\\psi}(x) = h(x)^\\top \\beta\\). The constant mean function is actually just a special case of this more general setting, but its common enough that it warranted its own section. If we denote by \\(H \\in \\mathbb{R}^{n \\times p}\\) the feature matrix with rows equal to \\(h(x_i)^\\top\\), \\(i = 1, \\dots, n\\) then the marginal likelihood becomes \\[\n\\begin{align}\n\\mathcal{L}(\\theta)\n&:= -\\frac{1}{2} \\log \\text{det}\\left(2\\pi C_{\\phi, \\sigma^2} \\right) -\n\\frac{1}{2} (y_n - H\\beta)^\\top C_{\\phi, \\sigma^2}^{-1} (y_n - H\\beta), \\tag{20}\n\\end{align}\n\\]\nwith gradient \\[\n\\begin{align}\n\\nabla_{\\beta} \\mathcal{L}(\\theta)\n&= H^\\top C_{\\phi, \\sigma^2}^{-1}y_n - (H^\\top C_{\\phi, \\sigma^2}^{-1} H)\\beta. \\tag{21}\n\\end{align}\n\\] Setting the gradient equal to zero and solving for \\(\\beta\\) yields the optimality condition \\[\n\\begin{align}\n\\left(H^\\top C_{\\phi, \\sigma^2}^{-1} H\\right)\\hat{\\beta} &= H^\\top C_{\\phi, \\sigma^2}^{-1}y_n. \\tag{22}\n\\end{align}\n\\] A unique solution for \\(\\hat{\\beta}\\) thus exists when \\(H^\\top C_{\\phi, \\sigma^2}^{-1} H\\) is invertible. When does this happen? First note that this matrix is positive semidefinite, since \\[\n\\begin{align}\n\\beta^\\top \\left(H^\\top C_{\\phi, \\sigma^2}^{-1} H\\right) \\beta\n&= \\beta^\\top (H^\\top [LL^\\top]^{-1} H) \\beta\n= \\lVert L^{-1} H\\beta \\rVert_2^2 \\geq 0,\n\\end{align}\n\\] where we have used the fact that \\(C_{\\phi, \\sigma^2}\\) is positive definite and hence admits a decomposition \\(LL^\\top\\). The matrix \\(H^\\top C_{\\phi, \\sigma^2}^{-1} H\\) is thus positive definite when \\(L^{-1}H\\) has linearly independent columns; i.e., when it is full rank. We already know that \\(L^{-1}\\) is full rank. If we assume that \\(H\\) is also full rank and \\(n \\geq p\\) then we can conclude that \\(L^{-1}H\\) is full rank; see this post for a quick proof. Thus, under these assumptions we conclude that \\(H^\\top C_{\\phi, \\sigma^2}^{-1} H\\) is invertible and so \\[\n\\begin{align}\n\\hat{\\beta}(\\phi, \\sigma^2) &= \\left(H^\\top C_{\\phi, \\sigma^2}^{-1} H\\right)^{-1} H^\\top C_{\\phi, \\sigma^2}^{-1}y_n.\n\\tag{23}\n\\end{align}\n\\] Notice that (23) is simply a generalized least squares estimator. As with the constant mean, we can plug \\(\\hat{\\beta}(\\phi, \\sigma^2)\\) into the marginal likelihood to concentrate out the parameter \\(\\beta\\). The resulting concentrated likelihood can then be numerically optimized as a function of the remaining hyperparameters.\n\n\nLinear Model Coefficients: Closed-Form Marginalization\nThe above section showed that, conditional on fixed kernel hyperparameters, the coefficients of a linear mean function can be optimized in closed form. We now show a similar result: if the mean coefficients are assigned a Gaussian prior then, conditional on fixed kernel hyperparameters, the coefficients can be marginalized in closed form. To this end, we consider the same linear mean function as above, but now equip the coefficients with a Gaussian prior: \\[\n\\begin{align}\n\\mu_{\\psi}(x) &= h(x)^\\top \\beta, &&\\beta \\sim \\mathcal{N}(b, B).\n\\end{align}\n\\] Restricted to the model inputs \\(X\\), the model is thus \\[\n\\begin{align}\ny_n|\\beta &\\sim \\mathcal{N}\\left(H\\beta, C_{\\phi, \\sigma^2} \\right) \\newline\n\\beta &\\sim \\mathcal{N}(b, B).\n\\end{align}\n\\] Our goal here is derive the marginal distribution of \\(y_n\\). We could resort to computing the required integral by hand, but an easier approach is to notice that under the above model \\([y_n, \\beta]\\) is joint Gaussian distributed. Therefore, the marginal distribution of \\(y_n\\) must also be Gaussian. It thus remains to identify the mean and covariance of this distribution. We obtain \\[\n\\begin{align}\n\\mathbb{E}[y_n]\n&= \\mathbb{E}\\mathbb{E}[y_n|\\beta] = \\mathbb{E}[H\\beta] = Hb \\newline\n\\text{Cov}[y_n]\n&= \\mathbb{E}[y_n y_n^\\top] - \\mathbb{E}[y_n]\\mathbb{E}[y_n]^\\top \\newline\n&= \\mathbb{E} \\mathbb{E}\\left[y_n y_n^\\top | \\beta\\right] - (Hb)(Hb)^\\top \\newline\n&= \\mathbb{E}\\left[\\text{Cov}[y_n|\\beta] + \\mathbb{E}[y_n|\\beta] \\mathbb{E}[y_n|\\beta]^\\top \\right] - Hbb^\\top H^\\top \\newline\n&= \\mathbb{E}\\left[C_{\\phi, \\sigma^2} + (H\\beta)(H\\beta)^\\top \\right] - Hbb^\\top H^\\top \\newline\n&= C_{\\phi, \\sigma^2} + H\\mathbb{E}\\left[\\beta \\beta^\\top \\right]H^\\top - Hbb^\\top H^\\top \\newline\n&= C_{\\phi, \\sigma^2} + H\\left[B + bb^\\top \\right]H^\\top - Hbb^\\top H^\\top \\newline\n&= C_{\\phi, \\sigma^2} + HBH^\\top,\n\\end{align}\n\\] where we have used the law of total expectation and the various equivalent definitions for the covariance matrix. To summarize, we have found that the above hierarchical model implies the marginal distribution \\[\n\\begin{align}\ny_n &\\sim \\mathcal{N}\\left(Hb, C_{\\phi, \\sigma^2} + HBH^\\top \\right).\n\\end{align}\n\\] Since this holds for any set of inputs, we obtain the analogous result for the GP prior: \\[\n\\begin{align}\ny(x) &= f(x) + \\epsilon(x) \\newline\nf &\\sim \\mathcal{GP}\\left(\\mu^\\prime, k^\\prime \\right) \\newline\n\\epsilon(x) &\\overset{iid}{\\sim} \\mathcal{N}(0, \\sigma^2),\n\\end{align}\n\\] where \\[\n\\begin{align}\n\\mu^\\prime(x) &= h(x)^\\top b \\newline\nk^\\prime(x_1, x_2) &= k(x_1, x_2) + h(x_1)^\\top B h(x_2).\n\\end{align}\n\\] After marginalizing, we again end up with a mean function that is linear in the basis functions \\(h(\\cdot)\\). The basis function coefficients are now given by the prior mean \\(b\\). The mean \\(b\\) is something that we can prescribe, or we could again entertain an empirical Bayes approach to set its value. Note that we have descended another step in the hierarchical ladder. The kernel that appears from the marginalization is now a sum of two kernels: the original kernel \\(k\\) and the kernel \\(h(x_1)^\\top B h(x_2)\\). The latter can be viewed as a linear kernel in the transformed inputs \\(h(x_1)\\), \\(h(x_2)\\) and weighted by the positive definite matrix \\(B\\). It serves to account for the uncertainty in the coefficients of the mean function."
  },
  {
    "objectID": "blog/posts/gp-specifications.html#special-case-closed-form-solutions-marginal-variance",
    "href": "blog/posts/gp-specifications.html#special-case-closed-form-solutions-marginal-variance",
    "title": "Introduction to Gaussian Process Priors and Hyperparameter Estimation",
    "section": "Special Case Closed-Form Solutions: Marginal Variance",
    "text": "Special Case Closed-Form Solutions: Marginal Variance\nWe now consider a closed-form plug-in estimate for the marginal variance \\(\\alpha^2\\), as mentioned in (12). The takeaway from this section will be that a closed-form estimate is only available when the covariance matrix appearing in the marginal likelihood (16) is of the form \\[\n\\begin{align}\nC_{\\phi} &= \\alpha^2 C. \\tag{24}\n\\end{align}\n\\] This holds for any kernel of the form \\(\\alpha^2 k(\\cdot, \\cdot)\\) provided that \\(\\sigma^2 = 0\\). For example, the exponentiated quadratic kernel in (12) satisfies this requirement. With this assumption, the marginal likelihood is given by \\[\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log\\left(2\\pi \\alpha^2 \\right) - \\frac{1}{2}\\log\\text{det}(C) -\n\\frac{1}{2\\alpha^2} (y_n - \\mu_{\\psi})^\\top C^{-1} (y_n - \\mu_{\\psi}). \\tag{25}\n\\end{align}\n\\] The analytical derivations given below go through for a log marginal likelihood of this form. However, this doesn’t work for the common setting with an observation variance \\(\\sigma^2 &gt; 0\\), since in this case the covariance assumes the form \\[\n\\begin{align}\nC &= \\left(\\alpha^2 k(X) + \\sigma^2 I_n \\right).\n\\end{align}\n\\] This can be addressed via the simple reparameterization \\[\n\\begin{align}\n\\tilde{\\alpha}^2 C &:= \\tilde{\\alpha}^2\\left(k(X) + \\tilde{\\sigma}^2 I_n \\right).\n\\end{align}\n\\] This gives the required form of the covariance, and maintains the same number of parameters as before. The one downside is that we lose the straightforward interpretation of the noise variance; the observation noise is now given by the product \\(\\tilde{\\alpha}^2 \\tilde{\\sigma}^2\\) instead of being encoded in the single parameter \\(\\sigma^2\\). This reparameterization is utilized in the R package hetGP.\n\nPlug-In MLE\nLet’s consider optimizing the log marginal likelihood with respect to \\(\\alpha^2\\). The partial derivative of (25) with respect to \\(\\alpha^2\\) is given by \\[\n\\begin{align}\n\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\alpha^2}\n&= -\\frac{n}{2}\\frac{2\\pi}{2\\pi \\alpha^2} - \\frac{(y_n - \\mu_{\\psi})^\\top C^{-1} (y_n - \\mu_{\\psi})}{2\\alpha^4} \\newline\n&= -\\frac{n}{2\\alpha^2} - \\frac{(y_n - \\mu_{\\psi})^\\top C^{-1} (y_n - \\mu_{\\psi})}{2\\alpha^4}.\n\\end{align}\n\\] Setting this expression equal to zero and solving for \\(\\alpha^2\\) yields \\[\n\\begin{align}\n\\hat{\\alpha}^2 &= \\frac{(y_n - \\mu_{\\psi})^\\top C^{-1} (y_n - \\mu_{\\psi})}{n}.\n\\end{align}\n\\] Following the same procedure as before, the estimate \\(\\hat{\\alpha}^2\\) can be subbed in for \\(\\alpha^2\\) in \\(\\mathcal{L}(\\theta)\\) to obtain the concentrated log marginal likelihood.\n\n\nClosed-Form Marginalization"
  },
  {
    "objectID": "blog/posts/gp-specifications.html#bias-corrections",
    "href": "blog/posts/gp-specifications.html#bias-corrections",
    "title": "Introduction to Gaussian Process Priors and Hyperparameter Estimation",
    "section": "Bias Corrections",
    "text": "Bias Corrections"
  },
  {
    "objectID": "blog/posts/gp-specifications.html#log-marginal-likelihood",
    "href": "blog/posts/gp-specifications.html#log-marginal-likelihood",
    "title": "Introduction to Gaussian Process Priors and Hyperparameter Estimation",
    "section": "Log Marginal Likelihood",
    "text": "Log Marginal Likelihood\nWe start by considering the computation of the log marginal likelihood (16), \\[\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log(2\\pi) -\\frac{1}{2} \\log \\text{det}\\left(C \\right) -\n\\frac{1}{2} (y - \\mu)^\\top C^{-1} (y - \\mu),\n\\end{align}\n\\] where we now suppress all dependence on hyperparameters in the notation for succinctness. Since \\(C = k(X) + \\sigma^2 I_n\\) is positive definite, we may Cholesky decompose it as \\(C = L L^\\top\\). Plugging this decomposition into the log marginal likelihood yields \\[\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log(2\\pi) - \\frac{1}{2} \\log\\text{det}\\left(C\\right) -\n\\frac{1}{2} (y_n - \\mu)^\\top \\left(LL^\\top \\right)^{-1} (y_n - \\mu).\n\\end{align}\n\\] The log determinant and the quadratic term can both be conveniently written in terms of the Cholesky factor. These terms are given respectively by \\[\n\\begin{align}\n\\log\\text{det}\\left(LL^\\top\\right)\n&= \\log\\text{det}\\left(L\\right)^2\n= 2 \\log \\prod_{i=1}^{n} L_{ii}\n= 2 \\sum_{i=1}^{n} \\log\\left(L_{ii} \\right),\n\\end{align}\n\\] and \\[\n\\begin{align}\n(y_n - \\mu)^\\top \\left(LL^\\top \\right)^{-1} (y_n - \\mu)\n&= (y_n - \\mu)^\\top \\left(L^{-1}\\right)^\\top L^{-1} (y_n - \\mu)\n= \\lVert L^{-1}(y - \\mu)\\rVert_2^2.\n\\end{align}\n\\] The linear solve \\(L^{-1}(y - \\mu)\\) can be computed in \\(\\mathcal{O}(n^2)\\) by exploiting the fact that the linear system has lower triangular structure. Plugging these terms back into the log marginal likelihood gives \\[\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log(2\\pi) - \\sum_{i=1}^{n} \\log\\left(L_{ii}\\right) -\n\\frac{1}{2} \\lVert L^{-1}(y - \\mu)\\rVert_2^2.\n\\end{align}\n\\] Note that the Cholesky factor \\(L\\) is a function of \\(\\phi\\) and \\(\\sigma^2\\) and hence must be re-computed whenever the kernel hyperparameters or noise variances change."
  },
  {
    "objectID": "blog/posts/gp-specifications.html#profile-log-marginal-likelihood-with-linear-mean-function",
    "href": "blog/posts/gp-specifications.html#profile-log-marginal-likelihood-with-linear-mean-function",
    "title": "Introduction to Gaussian Process Priors and Hyperparameter Estimation",
    "section": "Profile Log Marginal Likelihood with Linear Mean Function",
    "text": "Profile Log Marginal Likelihood with Linear Mean Function\nWe now consider computation of the concentrated marginal log-likelihood under a mean function of the form (11), \\(\\mu(x) = h(x)^\\top \\beta\\), where the generalized least squares (GLS) estimator \\(\\hat{\\beta} = \\left(H^\\top C^{-1} H\\right)^{-1} H^\\top C^{-1}y\\) (see (23)) is inserted in place of \\(\\beta\\). We are thus considering the profile log marginal likelihood \\[\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log(2\\pi) -\\frac{1}{2} \\log \\text{det}\\left(C \\right) -\n\\frac{1}{2} (y - H\\hat{\\beta})^\\top C^{-1} (y - H\\hat{\\beta}).\n\\end{align}\n\\] We will derive a numerically stable implementation of this expression in two steps, first applying a Cholesky decomposition (as in the previous section), and then leveraging a QR decomposition as in a typical ordinary least squares (OLS) computation. We first write \\(\\hat{\\beta}\\) in terms of the Cholesky factor \\(L\\), where \\(C = LL^\\top\\): \\[\n\\begin{align}\n\\hat{\\beta}\n&= \\left(H^\\top C^{-1} H\\right)^{-1} H^\\top C^{-1}y \\newline\n&= \\left(H^\\top \\left[LL^\\top\\right]^{-1} H\\right)^{-1} H^\\top \\left[LL^\\top\\right]^{-1}y \\newline\n&= \\left(\\left[L^{-1}H \\right]^\\top \\left[L^{-1}H \\right] \\right)^{-1} \\left[L^{-1}H \\right]^\\top\n\\left[L^{-1}y\\right].\n\\end{align}\n\\] Notice that the GLS computation boils down to two lower-triangular linear solves: \\(L^{-1}H\\) and \\(L^{-1}y\\). However, the above expression still requires one non-triangular linear solve that we will now address via the QR decomposition. The above expression for \\(\\hat{\\beta}\\) can be viewed as a standard OLS estimator with design matrix \\(L^{-1}H\\) and response vector \\(L^{-1}y\\). At this point, we could adopt a standard OLS technique of taking the QR decomposition of the design matrix \\(L^{-1}H\\). This was my original thought, but I found a nice alternative looking through the code in the R kergp package (see the function .logLikFun0 in the file kergp/R/logLikFuns.R). The approach is to compute the QR decomposition \\[\n\\begin{align}\n\\begin{bmatrix} L^{-1}H & L^{-1}y \\end{bmatrix} &= QR = Q \\begin{bmatrix} \\tilde{R} & r \\end{bmatrix}.\n\\end{align}\n\\] That is, we compute QR on the matrix formed by concatenating \\(L^{-1}y\\) as an additional column on the design matrix \\(L^{-1}H\\). We have written the upper triangular matrix \\(R \\in \\mathbb{R}^{(p+1) \\times (p+1)}\\) as the concatenation of \\(\\tilde{R} \\in \\mathbb{R}^{(p+1) \\times p}\\) and the vector \\(r \\in \\mathbb{R}^{p+1}\\) so that \\(L^{-1}H = Q\\tilde{R}\\) and \\(L^{-1}y = Qr\\). We recall the basic properties of the QR decomposition: \\(R\\) is upper triangular and invertible, and \\(Q\\) has orthonormal columns with span equal to the column space of \\(\\begin{bmatrix} L^{-1}H & L^{-1}y \\end{bmatrix}\\). Taking the QR decomposition of this concatenated matrix leads to a very nice expression for the quadratic form term of the profile log marginal likelihood. But first let’s rewrite \\(\\hat{\\beta}\\) in terms of these QR factors: \\[\n\\begin{align}\n\\hat{\\beta}\n&= \\left(\\left[L^{-1}H \\right]^\\top \\left[L^{-1}H \\right] \\right)^{-1} \\left[L^{-1}H \\right]^\\top\n\\left[L^{-1}y\\right] \\newline\n&= \\left(\\left[Q\\tilde{R} \\right]^\\top \\left[Q\\tilde{R} \\right] \\right)^{-1} \\left[Q\\tilde{R} \\right]^\\top \\left[Qr\\right] \\newline\n&= \\left(\\tilde{R}^\\top Q^\\top Q\\tilde{R} \\right)^{-1} \\tilde{R}^\\top Q^\\top Qr \\newline\n&= \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top r,\n\\end{align}\n\\] where we have used the fact that \\(Q^\\top Q\\) is the identity since \\(Q\\) is orthogonal. Plugging this into the quadratic form term of the log likelihood gives \\[\n\\begin{align}\n(y - H\\hat{\\beta})^\\top C^{-1} (y - H\\hat{\\beta})\n&= (y - H\\hat{\\beta})^\\top \\left[LL^\\top \\right]^{-1} (y - H\\hat{\\beta}) \\newline\n&= \\lVert L^{-1}(y - H\\hat{\\beta}) \\rVert_2^2 \\newline\n&= \\lVert L^{-1}y - L^{-1}H\\hat{\\beta} \\rVert_2^2 \\newline\n&= \\lVert Qr - Q\\tilde{R} \\hat{\\beta} \\rVert_2^2 \\newline\n&= \\left\\lVert Qr - Q\\tilde{R} \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top r \\right\\rVert_2^2 \\newline\n&= \\left\\lVert Q\\left[r - \\tilde{R} \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top r \\right] \\right\\rVert_2^2 \\newline\n&= \\left\\lVert r - \\tilde{R} \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top \\right\\rVert_2^2 \\newline\n&= \\left\\lVert \\left[I - \\tilde{R} \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top \\right]r \\right\\rVert_2^2,\n\\end{align}\n\\] where the penultimate line follows from the fact that \\(Q\\) is orthogonal, and hence an isometry. At this point, notice that the matrix \\(P := \\tilde{R} \\left(\\tilde{R}^\\top \\tilde{R} \\right)^{-1} \\tilde{R}^\\top\\) is the standard OLS projection matrix (i.e., hat matrix) constructed with the design matrix \\(\\tilde{R}\\). Also, take care to notice that \\(\\tilde{R}\\) is not invertible (it is not even square). Using standard properties of the projection matrix, we know that \\(P\\) has rank \\(p\\), since \\(\\tilde{R}\\) has rank \\(p\\). Also, since \\(R\\) is upper triangular, then the last row of \\(\\tilde{R}\\) contains all zeros. Letting, \\(e_j\\) denote the \\(j^{\\text{th}}\\) standard basis vector of \\(\\mathbb{R}^{p+1}\\), this means that \\[\n\\begin{align}\n\\mathcal{R}(P) \\perp \\text{span}(e_{p+1}),\n\\end{align}\n\\] where \\(\\mathcal{R}(P)\\) denotes the range (i.e., column space) of \\(P\\). The only subspace of \\(\\mathbb{R}^{p+1}\\) with rank \\(p\\) and satisfying this property is \\(\\text{span}(e_1, \\dots, e_p)\\). The conclusion is that \\(P\\) projects onto \\(\\text{span}(e_1, \\dots, e_p)\\), and thus the annihilator \\(I - P\\) projects onto the orthogonal complement \\(\\text{span}(e_{p+1})\\). We thus conclude, \\[\n\\begin{align}\n\\left\\lVert \\left[I - P\\right]r \\right\\rVert_2^2\n&= \\lVert \\langle r, e_{p+1} \\rangle e_{p+1} \\rVert_2^2 \\newline\n&= \\lVert r_{p+1} e_{p+1} \\rVert_2^2 \\newline\n&= r_{p+1}^2,\n\\end{align}\n\\] where \\(r_{p+1}\\) is the last entry of \\(r\\); i.e., the bottom right entry of \\(R\\). We finally arrive at the expression for the concentrated log marginal likelihood \\[\n\\begin{align}\n\\mathcal{L}(\\theta)\n&= -\\frac{n}{2} \\log(2\\pi) - \\sum_{i=1}^{n} \\log\\left(L_{ii}\\right) -\n\\frac{1}{2} r_{p+1}^2.\n\\end{align}\n\\]"
  },
  {
    "objectID": "blog/posts/basis-func-emulator.html",
    "href": "blog/posts/basis-func-emulator.html",
    "title": "Basis Expansions for Black-Box Function Emulation",
    "section": "",
    "text": "{% katexmm %} # Setup Suppose we are working with a function \\[\\begin{align}\n\\mathcal{G}: \\mathcal{U} \\subset \\mathbb{R}^d \\to \\mathbb{R}^p, \\tag{1}\n\\end{align}\\] where the output dimension \\(p\\) is presumed to be “large”, potentially in the hundreds or thousands. We treat \\(\\mathcal{G}\\) generically as a black-box function, but in common applications of interest it takes the form of an expensive computer simulation model. We will therefore use the terms black-box function, computer model, and simulator interchangeably throughout this post. Our primary goal of interest is to construct a model that approximates the map \\(u \\mapsto \\mathcal{G}(u)\\). We will refer to such a model as an emulator or surrogate model. The implicit assumption here is that computing \\(\\mathcal{G}(u)\\) is quite expensive, so the idea is to replace \\(\\mathcal{G}\\) with a computationally cheaper approximation. If you don’t care about emulating expensive computer models, you can also view this generically as a regression (or interpolation) problem with a very high-dimensional output space. To further elucidate the connection, suppose that we evaluate the function at a set of points \\(u_1, \\dots, u_n \\in \\mathcal{U}\\), resulting in the input-output pairs \\(\\{u_i, \\mathcal{G}(u_i)\\}_{i=1}^{n}\\). We can now treat these pairs as training examples that can be use to fit a predictive model. From this point of view, the primary distinction from more traditional regression is that in this case we get to choose the input points \\(u_i\\) at which we evaluate the function.\nThe methods we discuss below attempt to address two primary challenges: 1. Function evaluations \\(\\mathcal{G}(u)\\) are expensive, and thus we seek to minimize the number of points at which \\(\\mathcal{G}\\) is evaluated. 2. The output space of \\(\\mathcal{G}\\) is very high-dimensional.\nThe second challenge can be problematic for standard regression methods, which are often tailored to scalar-valued outputs. The obvious solution here might be to fit a separate model to predict each individual output; i.e., a model to emulate the map \\(u \\mapsto \\mathcal{G}_j(u)\\) for each \\(j=1, \\dots, p\\). With parallel computing resources, such an approach might even be feasible for very large values of \\(p\\). However, larger numbers of outputs typically come with more structure; e.g., the outputs may consist of a time series or spatial fields. The independent output-by-output regression approach completely fails to leverage this structure. The method discussed in this post seeks to take advantage of such structure by finding a set of basis vectors that explain the majority of the variation in the outputs. We proceed by first generically discussing basis representations of the output space, and how such structure can be leveraged to address the two challenges noted above. With the general method defined, we conclude by discussing details for a specific choice of basis approximation (principal components analysis) and a specific choice of emulator model (Gaussian process regression). {% endkatexmm %}\n{% katexmm %} # Basis Representation From a very generic perspective, the methods we discuss here can be thought of as a method for dimension reduction of the output space of a regression problem. It is perhaps more typical to see dimensionality reduction applied to the input space in such settings, with principal component regression being the obvious example. In this section, we start by discussing a low-dimensional basis representation of the output space, and then explore how such a representation can be leveraged in solving the regression problem. Throughout this introduction, I will assume that the output of \\(\\mathcal{G}\\) is centered; this is made explicit later on when considering a concrete application of PCA."
  },
  {
    "objectID": "blog/posts/basis-func-emulator.html#a-basis-representation-of-the-output-space",
    "href": "blog/posts/basis-func-emulator.html#a-basis-representation-of-the-output-space",
    "title": "Basis Expansions for Black-Box Function Emulation",
    "section": "A Basis Representation of the Output Space",
    "text": "A Basis Representation of the Output Space\nLet’s start by considering approximately representing vectors in the range of \\(\\mathcal{G}\\), denoted \\(\\mathcal{R}(\\mathcal{G})\\), with respect to a set of \\(r \\ll p\\) orthonormal basis vectors \\(\\{b_1, \\dots, b_r\\} \\subset \\mathbb{R}^p\\). Given such a set of vectors, we can approximate \\(g \\in \\mathcal{R}(\\mathcal{G})\\) by its projection onto the subspace \\(\\text{span}(b_1, \\dots, b_r)\\): \\[\n\\hat{g} := \\sum_{j=1}^{r} \\langle g, b_r\\rangle b_r. \\tag{2}\n\\] If we stack the basis vectors as columns in a matrix \\(B \\in \\mathbb{R}^{p \\times r}\\) then we can write this projection compactly as \\[\n\\hat{g}\n= \\sum_{j=1}^{r} \\langle g, b_r\\rangle b_r\n= \\sum_{j=1}^{r} (b_r b_r^\\top)g\n= BB^\\top g, \\tag{3}\n\\] We see that \\(BB^\\top\\) is the projection matrix that projects onto the span of the basis vectors. With regards to dimensionality reduction, the benefit here is that the simulator output can now be (approximately) represented using \\(r \\ll p\\) numbers \\(B^\\top g\\). The question is now: how do we find the basis vectors \\(B\\)? If we are given a set of vectors \\(g_1, \\dots, g_n \\in \\mathcal{R}(\\mathcal{G})\\), we can take an empirical approach and try to use these examples to determine a \\(B\\) that is optimal in some well-defined sense. Assuming that \\(\\mathcal{R}(\\mathcal{G}) \\subseteq \\mathbb{R}^p\\) is indeed a subspace and we define “optimal” in an average squared error sense, the problem we have laid out here is exactly that of principal components analysis (PCA), a topic I discuss in depth in this post. The only difference is that we are applying PCA on the subspace \\(\\mathcal{R}(\\mathcal{G})\\). At this point, we should emphasize that in practice \\(\\mathcal{R}(\\mathcal{G})\\) will often not be a subspace. Computer simulations may produce outputs that are subject to certain constraints, and thus \\(\\mathcal{R}(\\mathcal{G})\\) may represent a more complicated subset of \\(\\mathbb{R}^p\\). In these cases, one can still typically apply the PCA algorithm to obtain \\(B\\), but the result may be sub-optimal. Alternate methods of basis construction may be warranted depending on the problem at hand."
  },
  {
    "objectID": "blog/posts/basis-func-emulator.html#linking-the-basis-representation-with-the-input-parameters",
    "href": "blog/posts/basis-func-emulator.html#linking-the-basis-representation-with-the-input-parameters",
    "title": "Basis Expansions for Black-Box Function Emulation",
    "section": "Linking the Basis Representation with the Input Parameters",
    "text": "Linking the Basis Representation with the Input Parameters\nIn the previous subsection, we considered approximating vectors in the range of the simulator with respect to a set of basis vectors \\(B\\). However, recall that our underlying goal here is to approximate the map \\(u \\mapsto \\mathcal{G}(u)\\). We thus need to consider how to leverage the basis representation of the output space in achieving this goal. Assuming we have already constructed the basis vectors \\(B\\), the map \\(u \\mapsto \\mathcal{G}(u)\\) can be approximated as \\[\nu \\mapsto \\sum_{j=1}^{r} \\langle \\mathcal{G}(u), b_r\\rangle b_r = B [B^\\top \\mathcal{G}(u)]. \\tag{4}\n\\] In words: feed \\(u\\) through the simulator and project the resulting output onto the low-dimensional subspace spanned by the basis vectors. Note that \\(B^\\top \\mathcal{G}(u)\\) stores the \\(r\\) weights defining the projection of \\(\\mathcal{G}(u)\\) onto the subspace generated by \\(B\\), thus providing a low dimensional summary of the simulator output. Let’s introduce the notation \\[\\begin{align}\nw(u) &:= B^\\top \\mathcal{G}(u) = \\left[w_1(u), \\dots, w_r(u) \\right]^\\top \\in \\mathbb{R}^r,\n&& w_r(u) := \\langle \\mathcal{G}(u), b_r \\rangle\n\\end{align}\\] to denote this weights. The basis function approximation to the simulator can thus be written as \\[\n\\hat{\\mathcal{G}}_r(u) := \\sum_{j=1}^{r} w_r(u)b_r = Bw(u) \\in \\mathbb{R}^p. \\tag{5}\n\\] At this point, this isn’t helpful since the expensive simulation still needs to be run every time \\(\\hat{\\mathcal{G}}_r(u)\\) is evaluated. To address this, we now turn back to the idea of using emulators. Recall that such an approach was originally hindered due to the high-dimensional output space of the simulator. However, under the approximation \\(\\hat{\\mathcal{G}}_r(u)\\), the dependence on \\(u\\) has been reduced to \\(w(u)\\), which effectively reduces the output dimension to \\(r\\). The idea is thus to use some sort of statistical model to emulate the map \\(u \\mapsto w(u)\\). Suppressing all details for now, let’s suppose we have fit such a model \\(w^*(u)\\). We can now plug \\(w^*(u)\\) in place of \\(w(u)\\) in (5) to obtain the approximation \\[\n\\hat{\\mathcal{G}}^*_r(u) := \\sum_{j=1}^{r} w^*_r(u)b_r = Bw^*(u) \\in \\mathbb{R}^p. \\tag{6}\n\\] This approximation no longer requires running the full simulator, since evaluating \\(\\hat{\\mathcal{G}}^*_r(u)\\) just requires (1) computing the emulator prediction at \\(u\\); and (2) applying \\(B\\) to the emulator prediction. It is worth emphasizing how this approach compares to the direct emulation method. In place of directly trying to approximate the map from \\(u\\) to the model outputs \\(\\mathcal{G}(u)\\), we are now considering approximating the map from \\(u\\) to inner products of \\(\\mathcal{G}(u)\\) with a small number of basis vectors. The hope is that these inner products are sufficient to capture the majority of information in the model response."
  },
  {
    "objectID": "blog/posts/basis-func-emulator.html#the-general-emulation-model",
    "href": "blog/posts/basis-func-emulator.html#the-general-emulation-model",
    "title": "Basis Expansions for Black-Box Function Emulation",
    "section": "The General Emulation Model",
    "text": "The General Emulation Model\nIn the preceding subsections, we introduced the idea of representing the output space (range) of \\(\\mathcal{G}\\) with respect to a low-dimensional basis in order to facilitate emulation of the map \\(u \\mapsto \\mathcal{G}(u)\\). For concreteness, we considered an orthogonal basis, whereby approximations of \\(\\mathcal{G}(u)\\) take the form of orthogonal projections onto the basis vectors. In this section, we take a step back and define the general model, which encompasses basis methods beyond the orthogonal projection framework.\n\n\nThe Basis Function Emulation Model. Given a set of vectors \\(\\{b_1, \\dots, b_r\\} \\subset \\mathbb{R}^{p}\\), we refer to a decomposition of the form \\[\n  \\mathcal{G}(u) = \\sum_{j=1}^{r} w_j(u) b_j + \\epsilon(u) \\tag{7}\n  \\] as the basis function GP emulation model. We write \\(w^*_j(u)\\) to denote an emulator model that approximates the map \\(w_j(u)\\).\n\n\nThe basis function emulation model decomposes the computer model output \\(\\mathcal{G}(u)\\) into 1. a piece that can be explained by a linear combination of \\(r\\) basis functions that are independent of \\(u\\). 2. the residual \\(\\epsilon(u)\\), representing all variation unaccounted for by the basis functions.\nWe emphasize that the basis functions are independent of the input \\(u\\); the effect of the inputs is restricted to the coefficients \\(w_j(u)\\), with unaccounted for \\(u\\)-dependence absorbed by the residual term \\(\\epsilon(u)\\). As noted in the previous section, if we opt for an orthogonal projection approach, then the true weights assume the form \\[\nw_j(u) = \\langle \\mathcal{G}(u), b_j \\rangle, \\tag{8}\n\\] but the general model (7) allows for other approaches as well. Under different decomposition strategies, the true weights may not be given by the inner products (8). Nonetheless, we can still consider applying statistical models to approximate the underlying weight maps \\(u \\mapsto w_j(u)\\), regardless of what form these maps may take. {% endkatexmm %}"
  },
  {
    "objectID": "blog/posts/basis-func-emulator.html#gaussian-process-emulators",
    "href": "blog/posts/basis-func-emulator.html#gaussian-process-emulators",
    "title": "Basis Expansions for Black-Box Function Emulation",
    "section": "Gaussian Process Emulators",
    "text": "Gaussian Process Emulators\nThe above section shows that the PCA approach yields the basis vectors \\(b_j = v_j\\) with associated weights \\(w_j(u) = \\langle \\mathcal{G}(u)-\\overline{g}, v_j\\rangle\\). In this section we consider approximating the maps \\(u \\mapsto w_j(u)\\) with Gaussian processes (GPs). The specific features of the PCA basis construction provide useful information in designing a reasonable GP model. In particular, we consider why it is typically reasonable emulate the maps \\(u \\mapsto w_j(u)\\) separately using a set of independent GPs. We then discuss how the independent GP approach induces a multi-output GP emulator for \\(\\mathcal{G}(u)\\) with a particular covariance structure.\n\nIndependent GPs\nWe first consider the typical choice to model each \\(w_j(u)\\) separately as an independent GP. For this, we point to a result proved in the post post; namely, if we define \\(w^i := [w_1(u_i), \\dots, w_r(u_i)]^\\top \\in \\mathbb{R}^r\\), then the vectors \\(w^1, \\dots, w^n\\) have empirical covariance \\[\n\\hat{C}_w := \\frac{1}{n-1} \\sum_{i=1}^{n} w^i(w^i)^\\top = \\frac{1}{n-1}\\Lambda_r, \\tag{17}\n\\] with \\(\\Lambda_r\\) given in (13). In words, the weight vectors have zero sample covariance across dimensions. This seems to provide some justification for the independent GP approach, thought we should be careful not to overstate the claim. We can view \\(\\hat{C}_w\\) as an estimate of \\[\nC_w := \\mathbb{E}_{u \\sim \\rho} (w(u) - \\mathbb{E}[w(u)])(w(u) - \\mathbb{E}[w(u)])^\\top, \\tag{18}\n\\] where \\(w(u) = (w_1(u), \\dots, w_r(u))\\). Even if we knew that \\(C_w\\) was actually diagonal, we should take care to note that this means the \\(w_j(u)\\) are uncorrelated on average with respect to \\(u \\sim \\rho\\). The assumption to fit independent GPs entails assuming a probabilistic model for \\(w(u)\\) such that the \\(w_j(u)\\) are pairwise independent conditional on each \\(u\\). Thus, diagonal structure in \\(C_w\\) does not necessarily justify this assumption. In practice, the independent GP approach is often reasonable, and makes things much easier.\n\n\nPrior Mean Specification\nWe next consider the specification of the GP prior for each independent GP. Start by noting that the sample mean of the vectors \\(\\{w_j(u_i)\\}_{i=1}^{n}\\) is zero for each \\(j = 1, \\dots, p\\). Indeed, \\[\n\\frac{1}{n} \\sum_{i=1}^{n} w_j(u_i)\n= \\frac{1}{n} \\sum_{i=1}^{n} \\langle g^c_i,v_j \\rangle\n= \\left\\langle \\frac{1}{n} \\sum_{i=1}^{n} g^c_i,v_j \\right\\rangle\n= \\langle 0,v_j \\rangle = 0. \\tag{19}\n\\] Similar to above, we can view this as an estimate of \\(\\mathbb{E}_{u \\sim \\rho}[w_j(u)]\\). It is tempting to conclude that a constant zero prior mean assumption is thus justified for each GP. Again, this isn’t quite right. If \\(\\mathbb{E}_{u \\sim \\rho}[w_j(u)] \\approx 0\\), then this tells us that on average, over the whole input space, the weight \\(w_j(u)\\) is approximately zero. This does not guarantee the absence of a trend in the map \\(u \\mapsto w_j(u)\\). For example, this map might look linear, but is centered around zero so that the positive and negative values average out. The behavior of the weight maps will depend on the simulator and the basis vectors. If the simulator output is somewhat stationary, then the \\(w_j(u)\\) will typically also look stationary, and so the zero constant mean assumption is probably reasonable. In other cases, one might want to consider using some sort of trend for the prior mean (e.g., polynomial basis functions). Standard GP model checking procedures should be used to determine what is best for a particular application. In the application presented in {% cite HigdonBasisEmulator %}, the zero mean assumption is regarded as reasonable.\n\n\nPrior Covariance Specification\nThe covariance function used for each GP is also a modeling choice that must be specified on a case-by-case basis. If any trends in the functions \\(w_j(u)\\) have been adequately addressed in the prior mean, then it is often reasonable to utilize a stationary covariance function; e.g., one of the form \\[\n\\text{Cov}[w_j(u), w_j(u^\\prime)] = \\alpha^2 c(u-u^\\prime). \\tag{20}\n\\] Typical choices are covariances of the Gaussian or Matérn classes. In {% cite HigdonBasisEmulator %}, the authors consider “fully Bayesian” GPs, whereby priors are placed on the marginal variance \\(\\alpha^2\\) and other hyperparameters characterizing the correlation function \\(c(\\cdot)\\). A common alternative is to opt for an empirical Bayes approach and fix these parameters at their optimized values.\n\n\nError Term\nNext, we consider a statistical model for the error term \\(\\epsilon(u)\\) defined in (16). In {% cite HigdonBasisEmulator %}, it is assumed that \\[\n\\epsilon(u) \\sim \\mathcal{N}(0, \\sigma^2 I), \\tag{21}\n\\] with \\(\\sigma^2\\) treated as an unknown parameter that is assigned an inverse Gamma prior.\n\n\nImplied Multi-Output GP for the Simulator\n{% endkatexmm %}"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "\nAndrew G. Roberts\n",
    "section": "About Me",
    "text": "About Me\nI am a fourth year PhD candidate in Computing and Data Sciences at Boston University (BU), working with Jonathan Huggins and Michael Dietze. I am broadly interested in uncertainty quantification, Bayesian inverse problems, data assimilation, and spatiotemporal statistics, with the goal of developing new methodologies for environmental and ecological applications. My current work involves developing Bayesian methods for calibrating expensive mechanistic models of the terrestrial carbon cycle.\nBeyond my research, I am a trainee in BU URBAN, a graduate program in urban biogeoscience and environmental health. I am also an officer in the BU chapter of the American Statistical Association, and previously co-organized the Environment and Climate working group within the Mechanism Design for Social Good (MD4SG) research intitiative.\nPrior to BU, I worked as a research assistant at the Federal Reserve Bank of Boston. I received my B.S. in Quantitative Economics from Tufts University in 2018."
  },
  {
    "objectID": "blog/posts/nonlinear-LS.html",
    "href": "blog/posts/nonlinear-LS.html",
    "title": "Nonlinear Least Squares",
    "section": "",
    "text": "This post provides an introduction to nonlinear least squares (NLS), which generalizes the familiar least squares problem by allowing for a nonlinear forward map. Unlike least squares, the NLS problem does not admit closed-form solutions, but a great deal of work has been devoted to numerical schemes tailored to this specific problem. We introduce two of the most popular algorithms: Gauss-Newton and Levenberg-Marquardt."
  },
  {
    "objectID": "blog/posts/nonlinear-LS.html#statistical-perspective",
    "href": "blog/posts/nonlinear-LS.html#statistical-perspective",
    "title": "Nonlinear Least Squares",
    "section": "Statistical Perspective",
    "text": "Statistical Perspective\nThe solution \\(u_{\\star}\\) to the optimization problem in (1) can be viewed as the maximum a posterior (MAP) estimate of the Bayesian model \\[\n\\begin{align}\ny|u &\\sim \\mathcal{N}(\\mathcal{G}(u), \\Sigma) \\tag{2} \\newline\nu &\\sim \\mathcal{N}(m, C).\n\\end{align}\n\\] In particular, the posterior density of (2) satisfies \\[\np(u|y) \\propto \\exp\\left[-J(u)\\right], \\tag{3}\n\\] where \\(J(u)\\) is the nonlinear least squares objective (1)."
  },
  {
    "objectID": "blog/posts/nonlinear-LS.html#gradient-and-hessian",
    "href": "blog/posts/nonlinear-LS.html#gradient-and-hessian",
    "title": "Nonlinear Least Squares",
    "section": "Gradient and Hessian",
    "text": "Gradient and Hessian\nThe gradient and Hessian of \\(J(u)\\) play a prominent role in numerical algorithms that seek to solve (1). We provide the expressions for these quantities below, with derivations given in the appendix.\n\n\nGradient and Hessian.  The gradient \\(\\nabla J(u)\\) and Hessian \\(\\nabla^2 J(u)\\) of the cost function defined in (1) are given by \\[\\begin{align}\n  \\nabla J(u) &= -\\nabla \\mathcal{G}(u)\\Sigma^{-1}(y-\\mathcal{G}(u)) + C^{-1}(u-m) \\tag{4} \\newline\n  \\nabla^2 J(u) &= [\\nabla \\mathcal{G}(u)]\\Sigma^{-1}[\\nabla \\mathcal{G}(u)]^\\top  +\n  \\sum_{i=1}^{n} \\epsilon_i(u) \\nabla^2 \\mathcal{G}_i(u) + C^{-1} \\tag{5},\n  \\end{align}\\] where \\(\\epsilon_i(u)\\) is the \\(i^{\\text{th}}\\) entry of the vector \\(\\epsilon(u) := \\Sigma^{-1}(\\mathcal{G}(u) - y)\\).\n\n\nIn contrast to least squares, the Hessian \\(\\nabla^2 J(u)\\) depends on \\(u\\) in general."
  },
  {
    "objectID": "blog/posts/nonlinear-LS.html#newtons-method",
    "href": "blog/posts/nonlinear-LS.html#newtons-method",
    "title": "Nonlinear Least Squares",
    "section": "Newton’s Method",
    "text": "Newton’s Method\nGiven a current iterate \\(u_k\\), Newton’s method constructs a quadratic approximation of \\(J(u)\\) centered at the point \\(u_k\\), then minimizes this quadratic function to select the next iterate \\(u_{k+1}\\). The specific quadratic approximation considered is the second order Taylor expansion \\[\nJ_k(u) := J(u_k) + DJ(u_k)[u-u_k] + \\frac{1}{2}(u-u_k)^\\top [\\nabla^2 J(u_k)] (u-u_k). \\tag{6}\n\\] The next iterate \\(u_{k+1}\\) is thus chosen by solving \\[\n\\nabla J_k(u_{k+1}) = 0, \\tag{7}\n\\] which yields \\[\n\\nabla J_k(u_k) = \\nabla J(u_k) + [\\nabla^2 J(u_k)] (u_{k+1}-u_k) = 0. \\tag{8}\n\\] At this point, note that it is possible \\(J_k\\) doesn’t even have a minimum, which can happen when \\(\\nabla^2 J(u_k)\\) is not positive definite (and hence not invertible). We will not get into this issue here, and instead simply assume that \\(\\nabla^2 J(u_k)\\) is invertible. Then we can solve for \\(u_{k+1}\\) in (7) as \\[\nu_{k+1} = u_k - [\\nabla^2 J(u_k)]^{-1} \\nabla J(u_k). \\tag{9}\n\\] Working with the Hessian here can be difficult in certain settings; note in (5) that computing the Hessian of \\(J\\) requires computing the Hessian of \\(\\mathcal{G}\\) for each output dimension. Since \\(\\mathcal{G}\\) can be a very complicated and computationally-expensive function, this presents challenges. The below methods maintain the spirit of the Newton update (9), but replace the Hessian with something that is easier to compute."
  },
  {
    "objectID": "blog/posts/nonlinear-LS.html#gauss-newton",
    "href": "blog/posts/nonlinear-LS.html#gauss-newton",
    "title": "Nonlinear Least Squares",
    "section": "Gauss-Newton",
    "text": "Gauss-Newton\nWe introduce the Gauss-Newton update from two different perspectives. The first simply approximates the Hessian by ignoring the “difficult” terms, while the second views the update as solving a local least squares problem.\n\nApproximate Hessian\nThe Gauss-Newton procedure performs the Newton update (9) with an approximate Hessian constructed by simply dropping the higher-order terms in (5). Doing so yields the Hessian approximation \\[\n\\hat{H}_k := G_k^\\top \\Sigma^{-1}G_k + C^{-1}, \\tag{10}\n\\] where we have introduced the shorthand for the Jacobian of \\(\\mathcal{G}\\) evaluated at \\(u_k\\), \\[\nG_k := D\\mathcal{G}(u_k). \\tag{11}\n\\] Notice that (10) no longer requires extracting second order derivative information from \\(\\mathcal{G}\\). The resulting Gauss-Newton update takes the form \\[\nu_{k+1} = u_k - \\hat{H}_k^{-1} \\nabla J(u_k). \\tag{12}\n\\] Plugging in the expressions for \\(\\hat{H}_k\\) and \\(\\nabla J(u_k)\\) yields the explicit update\n\n\nGauss Newton Update: Parameter Space \\[\n  u_{k+1}\n  = u_k - \\left(G_k^\\top \\Sigma^{-1}G_k + C^{-1} \\right)^{-1} \\left[C^{-1}(u_k-m) - G_k^\\top \\Sigma^{-1}(y-\\mathcal{G}(u_k)) \\right]. \\tag{13}\n  \\]\n\n\nA nice consequence of using \\(\\hat{H}_k\\) is that, unlike the Hessian, it is guaranteed to be positive definite. This follows from the fact that \\(G^\\top_k \\Sigma^{-1}G_k\\) is positive semidefinite, \\(C^{-1}\\) is positive definite, and the sum of a positive semidefinite and positive definite matrix is positive definite. The invertibility of \\(\\hat{H}_k\\) is thus guaranteed, and the local quadratic function that Gauss-Newton is implicitly minimizing is guaranteed to have a minimum.\nWe refer to (13) as the parameter space version of the Gauss-Newton update, as it requires a \\(d\\)-dimensional linear solve \\(\\hat{H}_k^{-1} \\nabla J(u_k)\\). By applying the Woodbury matrix inversion identity, we obtain an alternative, but equivalent, update that instead requires a linear solve in \\(n\\)-dimensional data space.\n\n\nGauss Newton Update: Data Space \\[\n  u_{k+1}\n  = m + CG_k^\\top \\left(G_k CG_k^\\top + \\Sigma \\right)^{-1}\\left[y-\\left\\{\\mathcal{G}(u_k) + G_k(u_k-m)\\right\\}\\right]. \\tag{14}\n  \\]\n\n\nNotice that the expression in curly brackets is the linearization \\[\n\\begin{align}\n\\mathcal{G}(u_k) + G_k(u_k-m)\n&= \\mathcal{G}(u_k) - G_k(m-u_k) \\newline\n&\\approx \\mathcal{G}(u_k) - \\left[\\mathcal{G}(m) - \\mathcal{G}(u_k) \\right] \\newline\n&= \\mathcal{G}(m), \\tag{15}\n\\end{align}\n\\] and can thus be interpreted as the approximation to \\(\\mathcal{G}(m)\\) induced by the first order Taylor expansion around \\(\\mathcal{G}(u_k)\\). With this in mind, the update (14) looks exactly like a regularized least squares estimator. We explore this perspective below.\n\n\nLocal Least Squares Perspective\nThe Gauss-Newton update can alternatively be viewed as minimizing the following linear least squares cost function: \\[\nJ_k(u) := \\frac{1}{2}\\lVert y - [\\mathcal{G}(u_k)+G_k(u-u_k)] \\rVert^2_{\\Sigma} + \\frac{1}{2}\\lVert u - m\\rVert_C^2. \\tag{16}\n\\] In other words, the NLS objective \\(J(u)\\) is approximated by replacing the true forward model \\(\\mathcal{G}(u)\\) with its linearization, centered at the current iterate \\(u_k\\). It should be emphasized that we are linearizing \\(\\mathcal{G}\\), and then simply plugging the linearization into \\(J(u)\\). This is not the same as directly linearizing \\(J(u)\\), which is the approach taken in gradient descent.\nThis approach of linearizing \\(\\mathcal{G}\\) is quite convenient, since (16) is simply a linear least squares objective function, which we know how to optimize in closed-form. Thus, the Gauss-Newton algorithm can be viewed as iteratively solving a sequence of linear least squares problems. We now try to gain some intuition on the structure of these local least squares problems.\n\\[\n\\begin{align}\ny_k|u &\\sim \\mathcal{N}(G_k u, \\Sigma) \\tag{17} \\newline\nu &\\sim \\mathcal{N}(m, C),\n\\end{align}\n\\] where \\(y_k := y - \\mathcal{G}(u_k) + G_k u_k\\)."
  },
  {
    "objectID": "blog/posts/nonlinear-LS.html#levenberg-marquardt",
    "href": "blog/posts/nonlinear-LS.html#levenberg-marquardt",
    "title": "Nonlinear Least Squares",
    "section": "Levenberg-Marquardt",
    "text": "Levenberg-Marquardt"
  },
  {
    "objectID": "blog/posts/simplex-transform.html",
    "href": "blog/posts/simplex-transform.html",
    "title": "Transforming Simplex-values Parameters",
    "section": "",
    "text": "In this post we consider a \\(d\\)-dimensional random vector \\(x\\) that is constrained to lie in the unit simplex, \\[\n\\Delta_d := \\left\\{x \\in \\mathbb{R}^d : x_j \\geq 0, \\sum_{j=1}^{d} x_j = 1 \\right\\}. \\tag{1}\n\\] In words, the \\(d\\) values \\(x_1, \\dots, x_d\\) must be nonnegative and sum to one. Performing statistical inference with respect to a parameter \\(x \\in \\Delta_d\\) can be tricky; a common solution is to consider an invertible transformation \\[\ny = \\phi(x) \\tag{2}\n\\] such that \\(y\\) is unconstrained. Inference can then be performed with respect to \\(y\\) and we can transform back using \\(\\phi^{-1}\\) afterwards. This is the approach used by Stan for all constrained variables. In this post we walk through the transformation \\(\\phi\\) Stan uses for simplex constraints. Stan’s linked documentation is already quite detailed; I simply walk through the derivations in a bit more depth for my own benefit."
  },
  {
    "objectID": "blog/posts/simplex-transform.html#defining-phi_1-1-stick-breaking-procedure",
    "href": "blog/posts/simplex-transform.html#defining-phi_1-1-stick-breaking-procedure",
    "title": "Transforming Simplex-values Parameters",
    "section": "Defining \\(\\phi_1^{-1}\\): Stick-breaking procedure",
    "text": "Defining \\(\\phi_1^{-1}\\): Stick-breaking procedure\nThe first part of the transformation \\(\\phi_1^{-1}\\) arises naturally when viewing simplex-valued variables through a stick-breaking procedure: 1. Start with a stick of length one. 2. Break off a portion of the stick, and let \\(x_1\\) denote the length of this portion. 3. From the remaining piece, break off another portion and let \\(x_2\\) denote its length. 4. Repeat the procedure to obain \\(x_1, \\dots, x_{d-1}\\). 5. Set \\(x_d := 1-\\sum_{j=1}^{d-1} x_j\\), the length of the final remaining piece.\nThe nice thing about this viewpoint is that it enforces the sum-to-one constraint by construction. To take advantage of this, we define intermediate variables \\(z_1, \\dots, z_{d-1}\\) where \\(z_j\\) represents the proportion of the \\(j^{\\text{th}}\\) broken piece, relative to the size of the stick from which it was broken. Note that this is in contrast to \\(x_j\\), which is the proportion relative to the size of the original unit length stick. Mathematically, \\[\\begin{align}\n&x_1 = z_1, &&x_j = \\left(1 - \\sum_{i=1}^{j-1} x_i \\right)z_j, \\qquad j=1,\\dots,d-1. \\tag{6}\n\\end{align}\\] As they are proportions, these intermediate variables are constrained to lie in \\([0,1]\\) but are not subject to any sum constraints. The equations in (6) provide the definition for the first part of the inverse map: \\(x = \\phi_1^{-1}(z)\\)."
  },
  {
    "objectID": "blog/posts/simplex-transform.html#defining-phi_2-1-logit-transformation",
    "href": "blog/posts/simplex-transform.html#defining-phi_2-1-logit-transformation",
    "title": "Transforming Simplex-values Parameters",
    "section": "Defining \\(\\phi_2^{-1}\\): Logit transformation",
    "text": "Defining \\(\\phi_2^{-1}\\): Logit transformation\nWe now define the second portion of the inverse map, \\(z = \\phi_2^{-1}(y)\\). Dealing with the bound constraints \\(z_j \\in [0,1]\\) is straightforward; a standard approach involves using the logit map \\(\\text{logit}: (0,1) \\to \\mathbb{R}\\), defined by \\[\n\\text{logit}(t) := \\log \\frac{t}{1-t}, \\qquad t \\in (0,1). \\tag{7}\n\\] The inverse \\(\\text{logit}^{-1}: \\mathbb{R} \\to (0,1)\\) is given by the sigmoid \\[\n\\text{logit}^{-1}(t) = \\frac{1}{1+e^{-t}}. \\tag{8}\n\\] At this point, we could define \\(z := \\text{logit}^{-1}(y)\\) (where the inverse logit map is applied elementwise). This would be fine, but note that the zero vector \\(y=0\\) would map to \\(z = (1/2, \\dots, 1/2)\\). This corresponds to all of the cut proportions, relative to the piece from which they are cut, being equal. This implies stick lengths \\(x_1=\\frac{1}{2}\\), \\(x_2=\\frac{1}{4}\\), etc. Since \\(y=0\\) corresponds to a sort of “middle” value for \\(y\\), it would be nice for this to correspond to the balanced case where \\(x_j=\\frac{1}{d}\\) for \\(j=1, \\dots, d\\) (i.e., the case where all of the pieces are equal length). To achieve this, we need only make the slight adjustment \\[\nz_j := \\text{logit}^{-1}\\left(y_j + \\log\\left(\\frac{1}{d-j}\\right) \\right),\n\\qquad j=1, \\dots, d-1. \\tag{9}\n\\] Notice that the correction term can also be written as a logit, since \\[\\begin{align}\n\\text{logit}([d-j+1]^{-1})\n= \\log\\left(\\frac{[d-j+1]^{-1}}{1-[d-j+1]^{-1}}\\right)\n&= \\log\\left(\\frac{1}{(d-j+1)-1}\\right) \\newline\n&= \\log\\left(\\frac{1}{d-j}\\right). \\tag{10}\n\\end{align}\\]\nThis adjustment implies that the zero vector \\(y=0\\) maps to the relative cut proportions \\[\\begin{align}\nz_1 &= \\text{logit}^{-1}\\left(\\text{logit}\\left(\\frac{1}{d}\\right) \\right) = \\frac{1}{d} \\newline\nz_2 &= \\text{logit}^{-1}\\left(\\text{logit}\\left(\\frac{1}{d-1}\\right) \\right) = \\frac{1}{d-1} \\tag{11} \\newline\n&\\vdots\n\\end{align}\\] Feeding these values back through the map (6), we then see that \\(y=0\\) maps to \\(x=(1/d, \\dots, 1/d)\\), as desired."
  },
  {
    "objectID": "blog/posts/simplex-transform.html#density-of-y",
    "href": "blog/posts/simplex-transform.html#density-of-y",
    "title": "Transforming Simplex-values Parameters",
    "section": "Density of \\(y\\)",
    "text": "Density of \\(y\\)\nLet \\(p_x(x)\\) denote a probability density on \\(x\\). The density of \\(y = \\phi(x)\\) (where \\(\\phi\\) is invertible and differentiable) is then given by the change-of-variables formula \\[\np_y(y) = p_x(\\phi^{-1}(y)) \\lvert \\text{det} D\\phi^{-1}(y) \\rvert. \\tag{12}\n\\] Thus, we must compute the determinant of the Jacobian of the inverse transformation. In our present setting, notice that the stick-breaking procedure implies that \\(x_j\\) depends only on \\(y_1, \\dots, y_{j-1}\\). This means that \\(D\\phi^{-1}(y)\\) is a \\((d-1) \\times (d-1)\\) lower-triangular matrix. The determinant term is therefore given by \\[\n\\text{det} D\\phi^{-1}(y)\n= \\prod_{j=1}^{d} [D\\phi^{-1}(y)]_{jj}\n= \\prod_{j=1}^{d} \\frac{\\partial x_j}{\\partial y_j}\n= \\prod_{j=1}^{d} \\frac{\\partial x_j}{\\partial z_j} \\frac{\\partial z_j}{\\partial y_j}, \\tag{13}\n\\] where we have used the fact that the determinant of a triangular matrix is the product of its diagonal entries. The final step is an application of the chain rule for derivatives. We therefore need only concern ourselves with the diagonal entries of the Jacobian. The partial derivatives in (13) are computed as \\[\\begin{align}\n\\frac{\\partial x_j}{\\partial z_j}\n&= \\frac{\\partial}{\\partial z_j}\\left[\\left(1 - \\sum_{i=1}^{j-1} x_i \\right)z_j \\right]\n= \\left(1 - \\sum_{i=1}^{j-1} x_i \\right) \\tag{14}\n\\end{align}\\] and \\[\n\\frac{\\partial z_j}{\\partial y_j}\n= \\frac{\\partial}{\\partial y_j}\\left[\\text{logit}^{-1}\\left(y_j + \\log\\left(\\frac{1}{d-j}\\right) \\right) \\right]\n= z_j(1-z_j). \\tag{15}\n\\] In (14) we have used the fact that \\(x_i\\) does not depend on \\(z_j\\) for \\(i &lt; j\\). In the \\(j=1\\) case we treat the summation as equaling zero, so that the derivative is one (recall that \\(x_1=z_1\\)). In (15) we have used the fact that the derivative of the inverse logit is itself times one minus itself. Putting these two expressions together, we obtain \\[\n\\frac{\\partial x_j}{\\partial y_j}\n= \\left(1 - \\sum_{i=1}^{j-1} x_i \\right)z_j(1-z_j). \\tag{16}\n\\] This expression can then be combined with (12) and (13) to compute the density \\(p_y(y)\\). Notice that (16) is defined recursively with respect to the intermediate variables \\(z_j\\). We now provide an algorithm for computing both \\(x\\) and these partial derivatives. The helper variable \\(\\ell\\) tracks the length remaining from the original stick.\n\n\nAlgorithmic implementation of \\(\\phi^{-1}\\) and its derivative. \nInput: $ y = (y_1, , y_{d-1})$.  Returns: \\(x=\\phi^{-1}(y)\\), \\(g := \\text{diag}\\{D\\phi^{-1}(y)\\}\\). \n\n\n\\(z := \\phi_2^{-1}(y)\\), using (9).\n\n\n\\(x_1 := z_1\\) and \\(g_1 := 1\\).\n\n\n\\(\\ell := 1-x_1\\).\n\n\nFor \\(j=2, \\dots, d-1\\):\n\n&lt;ol type=\"i\"&gt;\n        &lt;li&gt;$x_j := \\ell z_j$.&lt;/li&gt;\n        &lt;li&gt;$g_j := \\ell z_j(1-z_j)$&lt;/li&gt;\n        &lt;li&gt;$\\ell := \\ell - x_j$&lt;/li&gt;\n    &lt;/ol&gt;\n&lt;li&gt;Return $x, g$.&lt;/li&gt;"
  },
  {
    "objectID": "blog/posts/MH-update-from-kernel.html",
    "href": "blog/posts/MH-update-from-kernel.html",
    "title": "Deriving the Metropolis-Hastings Update from the Transition Kernel",
    "section": "",
    "text": "The Metropolis-Hastings (MH) Markov Chain Monte Carlo (MCMC) method is typically introduced in the form of a practical algorithm. In a more theoretically-oriented context, one might prove that the algorithm defines a Markov chain and derive the associated transition (i.e. probability) kernel. I have found it insightful to also work through the derivations in the reverse order; given only the transition kernel, how could one derive the well-known MH update? In other words, how can you simulate the Markov chain implied by the transition kernel? In this post, I work through the required derivations."
  },
  {
    "objectID": "blog/posts/MH-update-from-kernel.html#setup-and-notation",
    "href": "blog/posts/MH-update-from-kernel.html#setup-and-notation",
    "title": "Deriving the Metropolis-Hastings Update from the Transition Kernel",
    "section": "Setup and notation",
    "text": "Setup and notation\nWe consider drawing samples from a target probability distribution \\(\\mu\\) supported on a state space \\(\\mathcal{X} \\subseteq \\mathbb{R}^D\\) with Borel sigma algebra \\(\\mathcal{B}\\). Let \\(\\pi: \\mathcal{X} \\to [0,\\infty]\\) denote the Lebesgue density of \\(\\mu\\), i.e. \\[\n\\mu(A) = \\int_A \\mu(d\\mathbf{x}) = \\int_A \\pi(\\mathbf{x}) d\\mathbf{x}, \\qquad \\forall A \\in \\mathcal{B}.\n\\] Let \\(Q: \\mathcal{X} \\times \\mathcal{B} \\to [0,1]\\) denote the proposal kernel for the MH algorithm, with \\(q(\\mathbf{x}, \\cdot)\\) the Lebesgue density of the measure \\(Q(\\mathbf{x}, \\cdot)\\). For current state \\(\\mathbf{x} \\in \\mathcal{X}\\) and proposal \\(\\mathbf{y} \\sim Q(\\mathbf{x}, \\cdot)\\) we recall the MH acceptance probability \\[\n\\alpha(\\mathbf{x}, \\mathbf{y})\n=\\min\\left(1, \\frac{\\pi(\\mathbf{y})q(\\mathbf{y},\\mathbf{x})}{\\pi(\\mathbf{x})q(\\mathbf{x},\\mathbf{y})} \\right).\n\\] Throughout this post I will let \\(A \\in \\mathcal{B}\\) denote an arbitrary Borel set. The transition kernel \\(P:\\mathcal{X} \\times \\mathcal{B} \\to [0,1]\\) implied by the MH algorithm is then given by \\[\\begin{align}\nP(\\mathbf{x},A)\n= \\int_A q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y} + \\delta_{\\mathbf{x}}(A) \\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})[1-\\alpha(\\mathbf{x},\\mathbf{y})] d\\mathbf{y} \\tag{1}\n\\end{align}\\] where \\(\\delta_{\\mathbf{x}}(A) := \\mathbf{1}(\\mathbf{x} \\in A)\\) denotes the Dirac measure. The first term in the kernel is the probability of accepting a proposal in the set \\(A\\), while the second accounts for the probability of rejection in the case that the current state \\(\\mathbf{x}\\) is already in \\(A\\). I will denote the overall probability of acceptance by \\[\n\\overline{a}(\\mathbf{x})\n:= \\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y}. \\tag{2}\n\\]"
  },
  {
    "objectID": "blog/posts/MH-update-from-kernel.html#mixture-of-kernels",
    "href": "blog/posts/MH-update-from-kernel.html#mixture-of-kernels",
    "title": "Deriving the Metropolis-Hastings Update from the Transition Kernel",
    "section": "Mixture of Kernels",
    "text": "Mixture of Kernels\nSuppose we don’t already know the MH algorithm, and are given the task of writing an algorithm to draw a sample from the distribution \\(P(\\mathbf{x},\\cdot)\\) defined in (1). A reasonable place to start is to try writing \\(P(\\mathbf{x},\\cdot)\\) as a mixture of kernels from which we already know how to sample. Let’s first try this idea, attempting to write \\[\nP(\\mathbf{x},A) = wP_1(\\mathbf{x},A) + (1-w)P_2(\\mathbf{x},A), \\qquad w \\in [0,1], \\tag{3}\n\\] with \\(P_1\\), \\(P_2\\) transition kernels we already know how to sample from. If we are able to do this, then we could easily sample from \\(P(\\mathbf{x},\\cdot)\\) via the following simple algorithm: 1. Select \\(P_1\\) with probability \\(w\\), else select \\(P_2\\). 2. Sample from the selected kernel.\nTo this end, let’s manipulate (1) to write it in the form of a kernel mixture. We have \\[\\begin{align}\nP(\\mathbf{x},A)\n&= \\int_A q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y} + \\delta_{\\mathbf{x}}(A) \\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})[1-\\alpha(\\mathbf{x},\\mathbf{y})] d\\mathbf{y} \\newline\n&= \\overline{a}(\\mathbf{x}) \\int_A \\frac{q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})}{\\overline{a}(\\mathbf{x})}d\\mathbf{y} + \\left[1-\\overline{a}(\\mathbf{x})\\right] \\delta_{\\mathbf{x}}(A) \\tag{4}\n\\end{align}\\] which is a kernel mixture of the form (3) with \\[\\begin{align}\nw = \\overline{a}(\\mathbf{x}), \\qquad P_1(\\mathbf{x},A)=\\int_A \\frac{q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})}{\\overline{a}(\\mathbf{x})}d\\mathbf{y},\n\\qquad P_2(\\mathbf{x},A)=\\delta_{\\mathbf{x}}(A).\n\\end{align}\\] All we did here was to multiply and divide by the acceptance probability \\(\\overline{a}(\\mathbf{x})\\) in the first term (under typical assumptions on \\(Q\\) this will be non-zero when \\(\\mathbf{y}\\) is in the support of \\(\\pi\\)) and to rearrange the second term using the fact that \\(q(\\mathbf{x},\\cdot)\\) is a probability density; hence, \\[\n\\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y} = 1.\n\\] Note that the mixture weight \\(\\overline{a}(\\mathbf{x})\\) is the overall acceptance probability, and is thus in \\([0,1]\\) as required. Moreover, \\(P_2\\) is the Dirac measure centered at \\(\\mathbf{x}\\) and is thus a valid kernel. To check that \\(P_1(\\mathbf{x},\\cdot)\\) is a valid probability measure, we recall the form of \\(\\overline{a}(\\mathbf{x})\\) from (2) to verify that \\[\\begin{align}\nP_1(\\mathbf{x},A)\n&= \\int_{\\mathcal{X}} \\frac{q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})}{\\overline{a}(\\mathbf{x})}d\\mathbf{y} \\newline\n&= \\frac{\\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y}}{\\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y}} \\newline\n&= 1.\n\\end{align}\\] The other required properties (countable additivity and non-negativity) are similarly verified. Thus, \\(P_1(\\mathbf{x},\\cdot)\\) is a probability measure with Lebesgue density proportional to \\(q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})\\); the overall acceptance probability \\(\\overline{a}(\\mathbf{x})\\) is the normalizing constant for this density.\nThis representation of the MH kernel as a mixture distribution is conceptually useful, but it does not directly help us determine a sampling algorithm. Indeed, we cannot implement the simple mixture sampling algorithm described above since (i.) computing the mixture weight \\(\\overline{a}(\\mathbf{x})\\) requires evaluating an intractable integral, and (ii.) we don’t know how to directly sample from the probability distribution with density proportional to \\(q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})\\). While this approach seems to be a dead end from a practical point of view, we should keep in mind that the MH algorithm derived below does sample from the mixture (4), but does so in a way that avoids having to compute the mixture weight or to directly sample from \\(P_1(\\mathbf{x},\\cdot)\\)."
  },
  {
    "objectID": "blog/posts/MH-update-from-kernel.html#marginalized-mixture-of-kernels",
    "href": "blog/posts/MH-update-from-kernel.html#marginalized-mixture-of-kernels",
    "title": "Deriving the Metropolis-Hastings Update from the Transition Kernel",
    "section": "Marginalized Mixture of Kernels",
    "text": "Marginalized Mixture of Kernels\nIn the previous section, we feigned ignorance of the MH algorithm in order to approach the problem of simulating (1) as a generic sampling problem. We found that \\(P\\) can indeed be written as a mixture of kernels, but the problem of sampling from the resulting mixture was also intractable. To take a step in the right direction, it is useful to cheat a little bit and recall some of the mechanics of the MH algorithm. The proposal \\(\\mathbf{y} \\sim Q(\\mathbf{x},\\cdot)\\) is accepted with probability \\(\\alpha(\\mathbf{x},\\mathbf{y})\\); if rejected, the next state is set to the current state \\(\\mathbf{x}\\). Thus, it seems that we should be looking for a mixture of kernels of the form \\[\n\\alpha(\\mathbf{x},\\mathbf{y})\\delta_{\\mathbf{y}}(\\cdot) + [1-\\alpha(\\mathbf{x},\\mathbf{y})]\\delta_{\\mathbf{x}}(\\cdot). \\tag{5}\n\\] Of course, this can’t represent the whole picture since the mixture weight in (5) depends on \\(\\mathbf{y}\\) and the proposal kernel \\(Q\\) is completely missing from the expression. The key insight is that the MH kernel \\(P(\\mathbf{x},\\cdot)\\) can be viewed as the expectation of (5) averaged with respect to \\(Q(\\mathbf{x},\\cdot)\\); i.e., \\(P(\\mathbf{x},\\cdot)\\) is derived by marginalizing the mixture (5) with respect to \\(\\mathbf{y} \\sim Q(\\mathbf{x},\\cdot)\\). To show this, we return to the original expression (1) for the MH kernel. We have \\[\\begin{align}\nP(\\mathbf{x},A)\n&= \\int_A q(\\mathbf{x},\\mathbf{y})\\alpha(\\mathbf{x},\\mathbf{y})d\\mathbf{y} + \\delta_{\\mathbf{x}}(A) \\int_{\\mathcal{X}} q(\\mathbf{x},\\mathbf{y})[1-\\alpha(\\mathbf{x},\\mathbf{y})] d\\mathbf{y} \\newline\n&= \\int_{\\mathcal{X}} \\alpha(\\mathbf{x},\\mathbf{y}) \\mathbf{1}(\\mathbf{y} \\in A) q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y} +  \\int_{\\mathcal{X}} [1-\\alpha(\\mathbf{x},\\mathbf{y})] \\delta_{\\mathbf{x}}(A) q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y} \\newline\n&= \\int_{\\mathcal{X}} \\alpha(\\mathbf{x},\\mathbf{y}) \\delta_{\\mathbf{y}}(A) q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y} +  \\int_{\\mathcal{X}} [1-\\alpha(\\mathbf{x},\\mathbf{y})] \\delta_{\\mathbf{x}}(A) q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y} \\newline\n&= \\int_{\\mathcal{X}} \\left[\\alpha(\\mathbf{x},\\mathbf{y})\\delta_{\\mathbf{y}}(A) + [1-\\alpha(\\mathbf{x},\\mathbf{y})] \\delta_{\\mathbf{x}}(A) \\right] q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y}, \\tag{6}\n\\end{align}\\] which is precisely the mixture (5) averaged (in \\(\\mathbf{y}\\)) with respect to the weights \\(q(\\mathbf{x},\\mathbf{y})\\). We now have three different representations of the MH transition kernel \\(P\\): (1) is the most natural to derive when starting from the MH algorithm, (4) represents \\(P\\) as a mixture of two distributions, and (6) represents \\(P\\) as a marginalized mixture of two distributions. It is this final representation which proves useful for developing a practical simulation algorithm.\nAll that remains is to recall how to sample from a marginalized distribution. First note that \\(\\mathbf{x}\\) is essentially a fixed parameter in the integral (6); the averaging is done with respect to \\(\\mathbf{y}\\). Now, if we condition on a fixed \\(\\mathbf{y}\\) as well, then the expression \\[\n\\alpha(\\mathbf{x},\\mathbf{y})\\delta_{\\mathbf{y}}(A) + [1-\\alpha(\\mathbf{x},\\mathbf{y})] \\delta_{\\mathbf{x}}(A)\n\\] is simply a mixture of two distributions, which we know how to sample from. Thus, a sample can be drawn from \\(P(\\mathbf{x},\\cdot)\\) via the following algorithm: 1. Sample \\(\\mathbf{y} \\sim Q(\\mathbf{x}, \\cdot)\\). 2. Conditional on \\(\\mathbf{y}\\), sample from \\(\\alpha(\\mathbf{x},\\mathbf{y})\\delta_{\\mathbf{y}} + [1-\\alpha(\\mathbf{x},\\mathbf{y})] \\delta_{\\mathbf{x}}\\).\nFor the second step, the mixture can be sampled from using the simple algorithm discussed in the previous section: randomly select one of the two kernels with probabilities equal to their mixture weights, then return a sample from the selected kernel. Since sampling from the Dirac measure \\(\\delta_{\\mathbf{x}}\\) simply means returning the value \\(\\mathbf{x}\\) (and similarly for \\(\\delta_{\\mathbf{y}}\\)) then this step will simply return \\(\\mathbf{x}\\) or \\(\\mathbf{y}\\) according to their respective probabilities \\(\\alpha(\\mathbf{x},\\mathbf{y})\\) and \\(1-\\alpha(\\mathbf{x},\\mathbf{y})\\). This is precisely the MH accept-reject mechanism!\nIt might be helpful to make this more concrete by letting \\(\\mathbf{X}_k\\) denote the value of the MCMC algorithm at iteration \\(k\\) and letting \\(\\mathbf{Y}|\\mathbf{X}_k \\sim Q(\\mathbf{X}_k, \\cdot)\\) be the random variable representing the proposal. Then the above mixture corresponds to the probability \\(\\mathbb{P}\\left(\\mathbf{X}_{k+1} \\in A | \\mathbf{X}_k=\\mathbf{x}, \\mathbf{Y}=\\mathbf{y}\\right)\\) so we can re-write (6) as \\[\nP(\\mathbf{x},A)\n= \\int_{\\mathcal{X}} \\mathbb{P}\\left(\\mathbf{X}_{k+1} \\in A | \\mathbf{X}_k=\\mathbf{x}, \\mathbf{Y}=\\mathbf{y}\\right) q(\\mathbf{x},\\mathbf{y}) d\\mathbf{y}.\n\\] Once we condition on \\(\\mathbf{X}_k\\) and \\(\\mathbf{Y}\\), the only remaining randomness in the probability above is coming from the selection of one of the two kernels."
  },
  {
    "objectID": "blog/posts/MH-update-from-kernel.html#conclusion",
    "href": "blog/posts/MH-update-from-kernel.html#conclusion",
    "title": "Deriving the Metropolis-Hastings Update from the Transition Kernel",
    "section": "Conclusion",
    "text": "Conclusion\nWhile all of this might appear to be overcomplicating the very simple MH algorithm, I have found it a quite worthwhile exercise to contemplate some different perspectives on the method, as well as to get some practice manipulating expressions involving probability kernels and thinking through sampling schemes. The MH transition kernel (1) can easily be derived by thinking through the mechanics of the MH algorithm. In this post, I showed in (4) how the kernel can be re-written as mixture of two distributions and in (6) as a marginalized mixture of two distributions. It is this final expression which provides the basis for a tractable simulation algorithm."
  },
  {
    "objectID": "blog/posts/ls-psd-cov.html",
    "href": "blog/posts/ls-psd-cov.html",
    "title": "Regularized Least Squares with Singular Prior",
    "section": "",
    "text": "Recall the \\(L^2\\) regularized least squares problem (i.e., ridge regression),\n\\[\n\\begin{align}\nu_{\\star} &:= \\text{argmin}_{u \\in \\mathbb{R}^d} J(u) \\tag{1} \\newline\nJ(u) &:= \\frac{1}{2} \\lVert y - Gu\\rVert^2_{\\Sigma} + \\frac{1}{2}\\lVert u-m \\rVert^2_{C}\n\\end{align}\n\\]\nwhere \\(y \\in \\mathbb{R}^n\\) and \\(m \\in \\mathbb{R}^d\\) are fixed vectors, \\(G \\in \\mathbb{R}^{n \\times d}\\) is the linear forward model, and \\(\\Sigma \\in \\mathbb{R}^{n \\times n}\\) and \\(C \\in \\mathbb{R}^{d \\times d}\\) are positive definite matrices. In (1) we have used the following notation for inner products and norms weighted by a positive definite matrix \\(C\\): \\[\n\\begin{align}\n\\langle v, v^\\prime \\rangle_C &:= \\langle C^{-1}v, v^\\prime\\rangle = v^\\top C^{-1}v^\\prime \\newline\n\\lVert v \\rVert_C^2 &:= \\langle v, v\\rangle_C.\n\\end{align}\n\\]\nA solution to (1) can be viewed as a maximum a posteriori (MAP) estimate under the Bayesian model \\[\n\\begin{align}\ny|u &\\sim \\mathcal{N}(Gu, \\Sigma) \\tag{3} \\newline\nu &\\sim \\mathcal{N}(m, C). \\tag{4}\n\\end{align}\n\\]\nI discuss this problem in depth in my post, on the linear Gaussian model, where I show that the solution can be written as the following equivalent expressions, \\[\n\\begin{align}\nu_{\\star} &= \\left(G^\\top \\Sigma^{-1}G + C^{-1}\\right)^{-1}\\left(G^\\top \\Sigma^{-1}y + C^{-1}m \\right) \\tag{5} \\newline\nu_{\\star} &= m + CG^\\top \\left(GCG^\\top + \\Sigma \\right)^{-1}(y-Gm). \\tag{6}\n\\end{align}\n\\]\nIn this post we will consider a generalization of problem (1), where \\(C\\) is only required to be positive semidefinite (not strictly positive definite). In particular, this means that the inverse \\(C^{-1}\\) may not exist. It is interesting to note that, although (5) depends on \\(C^{-1}\\), the inverse does not appear anywhere in the equivalent expression (6). This leads to the natural question of whether (6) still provides a valid solution to the optimization problem even when \\(C\\) is singular. The first step in investigating this question will be providing a suitable generalization of the objective function in (1), since the expression \\(\\lVert u - m\\rVert^2_{C} = \\lVert C^{-1/2}(u - m)\\rVert^2\\) is not well-defined when \\(C\\) is singular."
  },
  {
    "objectID": "blog/posts/ls-psd-cov.html#constrained-formulation",
    "href": "blog/posts/ls-psd-cov.html#constrained-formulation",
    "title": "Regularized Least Squares with Singular Prior",
    "section": "Constrained Formulation",
    "text": "Constrained Formulation\nWe start by considering the constrained formulation. This section follows the approach in section 8.1.2 of {% cite InvProbDA %}. To start, note that when \\(C\\) is positive definite, we have \\[\n\\lVert u - m\\rVert^2_C\n= \\lVert u^\\prime \\rVert^2_C\n= \\langle C^{-1}u^\\prime, u^\\prime\\rangle\n= \\langle b, u^\\prime \\rangle, \\tag{9}\n\\] where \\(b \\in \\mathbb{R}^d\\) solves \\[\nCb = u^\\prime. \\tag{10}\n\\] When \\(C\\) is invertible, the unique \\(b\\) solving (10) is simply found by multiplying both sides by \\(C^{-1}\\). When \\(C\\) is not invertible, then there may be zero or infinitely many such solutions. As long as there is at least one solution to (10) we can give meaning to expression (9) by picking a particular \\(b\\) that solves (10). Now that we have introduced a new variable \\(b\\), we might consider generalizing (1) by jointly optimizing over \\((u,b)\\) subject to the linear constraint \\(Cb = u^\\prime\\).\n\n\nConstrained Joint Optimization.  \\[\\begin{align}\n  (u_{\\star}, b_{\\star}) &\\in \\text{argmin}_{(u,b) \\in \\mathcal{S}} J(u,b) \\tag{11} \\newline\n  J(u,b) &:= \\frac{1}{2} \\lVert y - Gu\\rVert^2_{\\Sigma} + \\frac{1}{2}\\langle b, u-m \\rangle \\newline\n  \\mathcal{S} &:= \\left\\{(u,b) : Cb = u-m \\right\\}.\n  \\end{align}\\]\n\n\nNote that if \\(C\\) is positive definite, then (11) reduces to (1). If the problem is solved for \\((u_{\\star}, b_{\\star})\\), then the desired solution can be obtained by extracting \\(u_{\\star}\\) and discarding the nuisance parameter \\(b_{\\star}\\)."
  },
  {
    "objectID": "blog/posts/ls-psd-cov.html#lagrange-multipliers",
    "href": "blog/posts/ls-psd-cov.html#lagrange-multipliers",
    "title": "Regularized Least Squares with Singular Prior",
    "section": "Lagrange Multipliers",
    "text": "Lagrange Multipliers\nObserve that (11) can be viewed as optimizing \\(J(u,b)\\) subject to the constraint \\(g(u,b) = 0\\), where \\(g(u,b) = Cb - (u-m)\\). This is a typical setup for Lagrange multipliers. We therefore introduce the Lagrange multiplier \\(\\lambda \\in \\mathbb{R}^d\\), which allows us to cast the constrained optimization over \\((u,b)\\) as an unconstrained optimization over \\((u,b,\\lambda)\\).\n\n\nLagrange Multiplier Formulation.  \\[\\begin{align}\n  (u_{\\star}, b_{\\star}, \\lambda_{\\star}) &\\in \\text{argmin}_{u,b,\\lambda} J(u,b,\\lambda) \\tag{12} \\newline\n  J(u,b,\\lambda) &:= \\frac{1}{2} \\lVert y - Gu\\rVert^2_{\\Sigma} + \\frac{1}{2}\\langle b, u-m \\rangle + \\langle \\lambda, Cb - u + m \\rangle.\n  \\end{align}\\]\n\n\nWe have succeeding in converting the problem to one that can be solved by analytical means; namely, by solving the system of equations \\[\n\\nabla_{u} J = \\nabla_{b} J = \\nabla_{\\lambda} J = 0. \\tag{13}\n\\]\nThis derivation is provided in the appendix, and the result is summarized below.\n\n\nSolution of Lagrange Multiplier Formulation.  A solution \\((u_{\\star}, b_{\\star}, \\lambda_{\\star})\\) of (12), projected onto the \\(u\\)-component, is given by \\[\n  u_{\\star} = m + Cb_{\\star}, \\tag{14}\n  \\] where \\[\n  b_{\\star} = \\left[(G^\\top \\Sigma^{-1}G)C + I \\right]^{-1}G^\\top \\Sigma^{-1}(y-Gm). \\tag{15}\n  \\] It also holds that \\[\n  C\\left[(G^\\top \\Sigma^{-1}G)C + I \\right]^{-1}G^\\top \\Sigma^{-1} = CG^\\top \\left(GCG^\\top + \\Sigma \\right)^{-1}, \\tag{16}\n  \\] which implies that (14) agrees with expression (6)."
  },
  {
    "objectID": "blog/posts/ls-psd-cov.html#basis-approach",
    "href": "blog/posts/ls-psd-cov.html#basis-approach",
    "title": "Regularized Least Squares with Singular Prior",
    "section": "Basis Approach",
    "text": "Basis Approach\nWe now consider an alternative approach that avoids the joint optimization over \\((u,b)\\). Similar exposition can be found in section 8.3 of {% cite DAFundamentals %}. Looking back to (9), instead of choosing to optimize over all \\(b\\) satisfying (10), we will instead simply choose a particular \\(b\\) satisfying this constraint. This only makes sense if the particular choice of \\(b\\) does not matter; i.e., if the value of \\(\\langle b, u^\\prime \\rangle\\) is the same for any \\(b\\) solving (10). The below propostion shows that this is indeed the case.\n\n\nProposition.  Let \\(u \\in \\mathbb{R}^d\\) be a vector such that there is at least one solution \\(b \\in \\mathbb{R}^d\\) to \\(Cb = u^\\prime\\). Then \\(\\langle b, u^\\prime \\rangle\\) and \\(J(u,b)\\) are constant for any choice of \\(b\\) solving this linear system.\n\n\nProof. Let \\(b, b^\\prime \\in \\mathbb{R}^d\\) satisfy \\(Cb = Cb^\\prime = u^\\prime\\). It thus follows that \\[\n\\langle b, u^\\prime\\rangle - \\langle b^\\prime, u^\\prime\\rangle\n= \\langle b - b^\\prime, u^\\prime\\rangle\n= \\langle b - b^\\prime, Cb\\rangle\n= \\langle C(b - b^\\prime), b\\rangle\n= \\langle 0, b\\rangle\n= 0,\n\\] where we have used the linearity of the inner product and the fact that \\(C\\) is symmetric. Since the inner product term in (11) is the only portion with dependence on \\(b\\), it follows that \\(J(v,b) = J(v,b^\\prime)\\). \\(\\qquad \\blacksquare\\)\nThus, for each \\(u\\) that yields a consistent system \\(Cb = u^\\prime\\), we can simply pick any solution \\(b\\) to insert into \\(\\langle b, u^\\prime\\rangle\\). The objective will be well-defined since the above result verifies that the specific choice of \\(b\\) is inconsequential. A natural choice is to select the \\(b\\) of minimal norm; that is, \\[\nb^{\\dagger} := \\text{argmin}_{Cb=u^\\prime} \\lVert b \\rVert. \\tag{17}\n\\] This unique minimal norm solution is guaranteed to exist and is conveniently given by the Moore-Penrose pseudoinverse \\[\nb^{\\dagger} = C^{\\dagger}u^\\prime. \\tag{18}\n\\] Note that when \\(C\\) is positive definite, \\(C^{\\dagger} = C^{-1}\\). We can now eliminate the requirement to optimize over \\(b\\) by considering the following optimization problem.\n\n\nPseudoinverse Formulation.  \\[\\begin{align}\n  u_{\\star} &\\in \\text{argmin}_{u \\in \\mathcal{S}} J(u) \\tag{19} \\newline\n  J(u) &:= \\frac{1}{2} \\lVert y - Gu\\rVert^2_{\\Sigma} + \\frac{1}{2}\\langle C^{\\dagger}(u-m), u-m \\rangle \\newline\n  \\mathcal{S} &:= \\left\\{u \\in \\mathbb{R}^d: u-m \\in \\mathcal{R}(C) \\right\\}\n  \\end{align}\\]\n\n\nNote that we are now required to reintroduce a constraint set \\(\\mathcal{S}\\). This is due to the fact that the pseudoinverse produces a solution \\(b\\) to \\(Cb = u^\\prime\\) when a solution exists, but we must still explicitly restrict the search space to \\(u\\) that admit a consistent system \\(Cb = u^\\prime\\); i.e., \\(u^\\prime \\in \\mathcal{R}(C)\\). The previous approach dealt with this issue by introducing a Lagrange multiplier \\(\\lambda\\). We now show that (18) can be written as an unconstrained problem without extending the parameter space. This relies on the following important fact.\n\n\nSubspace Property.  Let \\(A \\in \\mathbb{R}^{d \\times r}\\) be a matrix of rank \\(r \\leq d\\) satisfying \\(C = AA^\\top\\). Then a solution to the optimization problem (19) is constrained to the \\(r\\)-dimensional affine subspace \\[\n  m + \\mathcal{R}(C) = m + \\mathcal{R}(A). \\tag{20}\n  \\]\n\n\nThis is simply a rewriting of the requirement \\(u^\\prime = u-m \\in \\mathcal{R}(C)\\). The solution is constrained by the rank of the subspace \\(\\mathcal{R}(C)\\). If \\(r = d\\) (i.e., \\(C\\) is full-rank) then we recover the typical unconstrained least squares problem. In general, \\(r &lt; d\\) so the columns of \\(C\\) are linearly dependent. We therefore introduce the matrix \\(A\\) whose columns \\(a_1, \\dots, a_r\\) form a basis for the range of \\(C\\); i.e., the columns of \\(C\\) and \\(A\\) have the same span.\nTo implicitly encode the constraint \\(u^\\prime \\in \\mathcal{R}(C)\\), we can look for solutions in \\(m + \\mathcal{R}(A)\\). Any vector in this space can be written as \\[\nu = m + \\sum_{j=1}^{r} w_j a_j = m + Aw, \\tag{21}\n\\] for some weight vector \\(w := (w_1, \\dots, w_r)^\\top \\in \\mathbb{R}^r\\). We can now substitute \\(m + Aw\\) for \\(u\\) and \\(AA^\\top\\) for \\(C\\) in (19) and thus formulate the optimization over the weights \\(w\\): \\[\nJ(w) := \\frac{1}{2}\\lVert y - G(m+Aw)\\rVert^2_{\\Sigma} +\n\\frac{1}{2}\\langle (AA^\\top)^{\\dagger}Aw, Aw \\rangle. \\tag{22}\n\\] The optimization is now over \\(w \\in \\mathbb{R}^r\\) so we have succeeded in writing (18) as an unconstrained problem. We can simplify this even further using the following properties of the pseudoinverse:\n\nBy definition, the pseudoinverse satisfies \\(A = A(A^{\\dagger}A)\\).\nThe matrix \\(A^{\\dagger}A\\) is a projection onto the rowspace of \\(A\\); in particular it is idempotent (i.e., \\((A^{\\dagger}A)^2 = A^{\\dagger}A\\)) and symmetric.\nThe following identity holds: \\(A^{\\dagger} = A^\\top (AA^\\top)^{\\dagger}\\).\n\nWe can thus simplify the second term in (22) as \\[\n\\begin{align}\n\\langle (AA^\\top)^{\\dagger}Aw, Aw\\rangle\n= \\langle A^\\top(AA^\\top)^{\\dagger}Aw, w\\rangle\n= \\langle (A^{\\dagger}A)w, w\\rangle\n&= \\langle (A^{\\dagger}A)^2w, w\\rangle \\newline\n&= \\langle (A^{\\dagger}A)w, (A^{\\dagger}A)w\\rangle \\newline\n&= \\lVert (A^{\\dagger}A)w \\rVert^2, \\tag{23}\n\\end{align}\n\\] where the second equality uses the third pseudinverse property above. The third and fourth equalities follow from the fact that \\(A^{\\dagger}A\\) is idempotent and symmetric. We can thus re-parameterize as \\[\n\\tilde{w} := (A^{\\dagger}A)w, \\tag{24}\n\\] and note that by the first pseudoinverse property we have \\[\nAw = A(A^{\\dagger}A)w = A\\tilde{w}, \\tag{25}\n\\] which allows us to replace \\(Aw\\) by \\(A\\tilde{w}\\) in the first term in (22). We obtain \\[\nJ(\\tilde{w})\n:= \\frac{1}{2} \\lVert y - G(m + A\\tilde{w})\\rVert_{\\Sigma}^2 +\n\\frac{1}{2} \\lVert \\tilde{w} \\rVert^2 . \\tag{26}\n\\] We see that (26) is the sum of a model-data fit term plus a simple \\(L^2\\) penalty on the weights. This final formulation is summarized below, where we have re-labelled \\(\\tilde{w}\\) as \\(w\\) to lighten notation.\n\n\nUnconstrained Pseudoinverse Formulation.  The optimization problem in (19) can equivalently be formulated as the unconstrained problem \\[\\begin{align}\n  w_{\\star} &\\in \\text{argmin}_{w \\in \\mathbb{R}^r} J(w) \\tag{27} \\newline\n  J(w) &:= \\frac{1}{2} \\lVert y^\\prime - GAw\\rVert_{\\Sigma}^2 + \\frac{1}{2}\\lVert w \\rVert^2\n  \\end{align}\\]\n\n\nOne might initially take issue with the claim that (27) is actually unconstrained given that it relies on the re-parameterization (24), which constrains the weights to lie in the range of \\(A^{\\dagger}A\\). However, recalling that \\(A^{\\dagger}A\\) is an orthogonal projection, we have the following projection property: \\[\n\\lVert \\tilde{w} \\rVert^2\n= \\lVert (A^{\\dagger}A)w \\rVert^2 \\leq \\lVert w \\rVert^2. \\tag{28}\n\\] In other words, allowing the weights to be unconstrained can only increase the objective function (since switching \\(w\\) and \\(\\tilde{w}\\) has no effect on the first term in (26)), so we are justified in considering the weights to be unconstrained in (27). In fact, we could have jumped right to this conclusion from (23) and avoided needing to define \\(\\tilde{w}\\) at all.\nThe following result provides the solution to (27), which immediately follows from the observation that (27) is a standard least squares problem.\n\n\nSolution of Unconstrained Pseudoinverse formulation.  The minimizing weight of the optimization problem in (26) is given by \\[\n  w_{\\star} = A^\\top G^\\top \\left(GCG^\\top + \\Sigma \\right)^{-1}y^\\prime, \\tag{29}\n  \\] which implies the optimal parameter \\[\\begin{align}\n  u_{\\star}\n  &= m + Aw_{\\star} \\newline\n  &= m + CG^\\top \\left(GCG^\\top + \\Sigma \\right)^{-1}(y - Gm)\n  \\end{align}\\] agrees with the typical least squares solution in (6).\n\n\nProof. Observe that (27) is of the form (1) with \\(u := w\\), \\(y^\\prime := y\\), \\(G := GA\\), and \\(C := I\\). Thus, we apply (6) to obtain \\[\n\\begin{align}\nw_{\\star}\n&:= (GA)^\\top \\left[(GA)(GA)^\\top + \\Sigma \\right]^{-1}y^\\prime \\newline\n&= A^\\top G^\\top \\left[GCG^\\top + \\Sigma \\right]^{-1}(y-Gm),\n\\end{align}\n\\] where the second equality uses \\(C = AA^\\top\\). We thus have \\[\n\\begin{align}\nu_{\\star}\n&= m + Aw_{\\star} \\newline\n&= m + (AA^\\top) G^\\top \\left[GCG^\\top + \\Sigma \\right]^{-1}(y-Gm) \\newline\n&= C G^\\top \\left[GCG^\\top + \\Sigma \\right]^{-1}(y-Gm),\n\\end{align}\n\\] which exactly agrees with (6). \\(\\qquad \\blacksquare\\)"
  },
  {
    "objectID": "blog/posts/ls-psd-cov.html#uniqueness",
    "href": "blog/posts/ls-psd-cov.html#uniqueness",
    "title": "Regularized Least Squares with Singular Prior",
    "section": "Uniqueness",
    "text": "Uniqueness\nOur derivations show that the optimum \\(u_{\\star}\\) is unique, even when \\(C\\) is singular. Note that this does not imply that a solution \\((u_{\\star}, b_{\\star})\\) to (11) is unique, since many choices of \\(b\\) may lead to the same value of the objective. Indeed, \\(u_{\\star} = m + Cb_{\\star}\\) but it may be that \\(u_{\\star} = m + Cb^\\prime\\) for some other \\(b^\\prime \\in \\mathcal{S}\\). n particular, if \\(u_{\\star}\\) is optimal, then \\((u_{\\star}, b^\\prime)\\) minimizes \\(J(u,b)\\) for any \\(b^\\prime\\) satisfying \\(Cb^\\prime = u_{\\star} - m\\). This follows immediately from the result that the particular \\(b\\) solving this linear system does not change the value of the objective. Given that the particular choice of \\(b\\) is inconsequential, then a particular rule for choosing \\(b\\) will not affect the optimal \\(u_{\\star}\\). The specific approach of choosing the minimal norm \\(b\\) led to the least squares problem (27), which we know has a unique solution. Therefore, if (11) has a solution, then it is unique. We summarize this below.\n\n\nUniqueness.  If the solution set \\(\\mathcal{S}\\) is non-empty, then there is a unique optimal value \\(u_{\\star}\\) solving (11)."
  },
  {
    "objectID": "blog/posts/ls-psd-cov.html#representations-of-the-solution",
    "href": "blog/posts/ls-psd-cov.html#representations-of-the-solution",
    "title": "Regularized Least Squares with Singular Prior",
    "section": "Representations of the solution",
    "text": "Representations of the solution\nContinuing the above discussion, it should also be emphasized that, while the solution is unique, there may be many different ways to represent it. To see this, consider that (14) gives the solution in the form \\(u_{\\star} = m + Cb_{\\star}\\), for a weight vector \\(b_{\\star}\\). But in general \\(C\\) has linearly dependent columns, and thus there may be multiple sets of weights that give rise to the same vector. In (29) the solution is instead represented in the form \\(u_{\\star} = m + Aw_{\\star}\\). If we assume that the columns of \\(A\\) are linearly independent, thus providing a basis for \\(\\mathcal{R}(C)\\), then this provides the unique representation of the solution with respect to the particular basis \\(a_1, \\dots, a_r\\). Note that the two representations are connected by \\[\nu_{\\star} = m + Cb_{\\star} = m + AA^\\top b_{\\star} = m + A(A^\\top b_{\\star}) \\tag{29}.\n\\] If the columns of \\(A\\) are independent, then (29) implies \\(w_{\\star} = A^\\top b_{\\star}\\)."
  },
  {
    "objectID": "blog/posts/ls-psd-cov.html#applications",
    "href": "blog/posts/ls-psd-cov.html#applications",
    "title": "Regularized Least Squares with Singular Prior",
    "section": "Applications",
    "text": "Applications\nWhy should we even care about the case when \\(C\\) is singular? The main application that motivated this post comes from the field of data assimilation. In this context, the parameter and data dimensions \\(d\\) and \\(n\\) can be massive, potentially precluding storing the matrices \\(\\Sigma\\) or \\(C\\). A common solution to this problem is to replace the true \\(C\\) with a low-rank approximation of the form \\[\nC = \\frac{1}{r-1} \\sum_{j=1}^{r} (u^{(j)} - m)(u^{(j)} - m)^\\top, \\tag{30}\n\\] where \\(r &lt; d\\). This Monte Carlo approximation results in a sample covariance matrix \\(C\\) that has rank at most \\(r\\). Here, \\(\\{u^{(j)}\\}\\) is a set of \\(r\\) samples used to compute the sample covariance. If we define \\(A\\) to be the \\(d \\times r\\) matrix with \\(j^{\\text{th}}\\) row equal to \\(\\frac{1}{\\sqrt{r-1}}\\left(u^{(j)} - m\\right)\\), then we see that \\(C = AA^\\top\\). The matrix \\(A\\) is full rank if the vectors \\(\\{u^{(j)}\\}\\) are independent."
  },
  {
    "objectID": "blog/posts/ls-psd-cov.html#proof-solution-for-lagrange-multiplier-formulation",
    "href": "blog/posts/ls-psd-cov.html#proof-solution-for-lagrange-multiplier-formulation",
    "title": "Regularized Least Squares with Singular Prior",
    "section": "Proof: Solution for Lagrange Multiplier Formulation",
    "text": "Proof: Solution for Lagrange Multiplier Formulation\nWe derive expressions (14), (15) and (16), the latter showing that the solution agrees with (6).\n\nDeriving (14) and (15)\nWe start by computing the gradients with respect to \\(u\\), \\(b\\), and \\(\\lambda\\): \\[\\begin{align}\n\\nabla_{u}J &= -G^\\top \\Sigma^{-1}(y-Gu) + \\frac{1}{2}b - \\lambda \\tag{31} \\newline\n\\nabla_{b}J &= \\frac{1}{2}u^\\prime + C\\lambda \\newline\n\\nabla_{\\lambda}J &= Cb - u^\\prime.\n\\end{align}\\] where we recall the definition \\(u^\\prime := u - m\\). We now solve the system of equations \\(\\nabla_{u}J = \\nabla_{b}J = \\nabla_{\\lambda}J = 0\\) for the three unknowns. Focusing on the last two equations, if we compute \\(2\\nabla_{b}J + \\nabla_{\\lambda}J = 0\\) we obtain the optimality criterion \\[\nC(b + 2\\lambda) = 0, \\tag{32}\n\\] or equivalently, \\[\nb + 2\\lambda \\in \\mathcal{N}(C), \\tag{33}\n\\] where \\(\\mathcal{N}(C)\\) denotes the null space of \\(C\\). The null space of \\(C\\) may be nontrivial, which aligns with the above discussion that many values of \\(b\\) may yield the optimal solution. We consider taking the minimal norm solution \\[\nb_{\\star} + 2\\lambda_{\\star} = 0, \\tag{34}\n\\] which implies \\(\\lambda_{\\star} = -\\frac{1}{2}b_{\\star}\\). Note also that \\[\ny - Gu_{\\star} = (y-Gm) - G(u_{\\star}-m) =: y^\\prime - Gu_{\\star}^\\prime = y^\\prime - GCb_{\\star}. \\tag{35}\n\\] We plug in the expression for \\(\\lambda_{\\star}\\) to \\(\\nabla_{u}J = 0\\), which yields \\[\n-G^\\top \\Sigma^{-1}(y^\\prime - GCb_{\\star}) + b_{\\star} = 0. \\tag{36}\n\\] Solving for \\(b_{\\star}\\), we obtain \\[\nb_{\\star} = \\left[(G^\\top \\Sigma^{-1}G)C + I \\right]^{-1} G^\\top \\Sigma^{-1}(y-Gm), \\tag{35}\n\\] which is equation (15). Recalling the definition \\(u^\\prime_{\\star} := u_{\\star} - m\\), we obtain equation (14): \\[\nu_{\\star} = m + u^\\prime_{\\star} = m + Cb_{\\star}. \\tag{36}\n\\]\n\n\nDeriving (16)\nWe now show that (36) agrees with the standard least squares solution (6). Differentiating the latter with a tilde, we want to show equality between the two expressions \\[\n\\begin{align}\nu_{\\star} &= m + C\\left[(G^\\top \\Sigma^{-1}G)C + I \\right]^{-1} G^\\top \\Sigma^{-1}(y-Gm) \\newline\n\\tilde{u}_{\\star} &= m + CG^\\top \\left(GCG^\\top + \\Sigma \\right)^{-1}(y-Gm).\n\\end{align}\n\\] We therefore see that it suffices to show \\[\n\\left[(G^\\top \\Sigma^{-1}G)C + I \\right]^{-1} G^\\top \\Sigma^{-1} =\nG^\\top \\left[GCG^\\top + \\Sigma \\right]^{-1}, \\tag{37}\n\\] which implies (16). To show (37), we multiply both sides by each of the matrices in brackets, which gives \\[\nG^\\top \\Sigma^{-1}\\left[GCG^\\top + \\Sigma \\right]\n= \\left[G^\\top \\Sigma^{-1}GC + I \\right] G^\\top. \\tag{38}\n\\] The righthand side of (38) can be factored as \\[\n\\left[G^\\top \\Sigma^{-1}GC + I \\right] G^\\top =\nG^\\top \\Sigma^{-1} \\left[GCG^\\top + \\Sigma \\right], \\tag{39}\n\\] completing the proof. \\(\\qquad \\blacksquare\\)"
  },
  {
    "objectID": "blog/posts/reversed-Cholesky.html",
    "href": "blog/posts/reversed-Cholesky.html",
    "title": "The Reversed Cholesky Decomposition",
    "section": "",
    "text": "The Cholesky decomposition of a positive definite matrix \\(C\\) is the unique factorization of the form \\[\nC = LL^\\top, \\tag{1}\n\\] such that \\(L\\) is lower triangular with positive entries on its diagonal. We refer to \\(L\\) as the Cholesky factor of \\(C\\), and denote this relationship by \\(L := \\text{chol}(C)\\). Intuitively, it seems that the focus on lower triangular matrices is just a convention, and that we could alternatively consider factorizations of the form \\[\nC = UU^\\top , \\tag{2}\n\\] where \\(U\\) is upper triangular with positive diagonal entries. This is no longer what is commonly called the Cholesky decomposition, but is similar in spirit. We will refer to this as the reversed Cholesky decomposition, or rCholesky for short. We call \\(U\\) the rCholesky factor, and write \\(U := \\text{rchol}(C)\\). In this post, we explore this alternative factorization, and demonstrate (i.) its close connections to the Cholesky factorization of \\(C^{-1}\\); and (ii.) its interpretation as the Cholesky factorization under a reverse ordering of the variable indices. These connections have various applications, including in high-dimensional covariance estimation; see, e.g., {% cite SparseCholeskyVecchia %} for one such example."
  },
  {
    "objectID": "blog/posts/reversed-Cholesky.html#the-reversal-operator",
    "href": "blog/posts/reversed-Cholesky.html#the-reversal-operator",
    "title": "The Reversed Cholesky Decomposition",
    "section": "The Reversal Operator",
    "text": "The Reversal Operator\nAs we will see, the rCholesky decomposition can be interpreted as a Cholesky decomposition of a matrix under reverse ordering. By reverse ordering, we mean that the order of both the rows and the columns of \\(C\\) are reversed. This notion is more intuitive when viewing \\(C\\) as a \\(n \\times n\\) covariance matrix for some random variables \\(x_1, \\dots, x_n\\), such that \\(C_{ij} = \\text{Cov}[x_i,x_j]\\). We thus see that the ordering of the variables determines the ordering of the matrix. Let \\(x := (x_1, \\dots, x_n)^\\top\\) be the vector of variables such that \\(C = \\text{Cov}[x]\\). We will denote by \\[\n\\tilde{x} := (x_n, \\dots, x_1)^\\top \\tag{6}\n\\] the reversed vector. The reversal operation \\(x \\mapsto \\tilde{x}\\) is linear and can thus be represented by a matrix. In particular, \\(\\tilde{x}\\) is given by \\[\n\\tilde{x} = Px \\tag{7}\n\\] where \\(P\\) is the square permutation matrix with ones on the anti-diagonal; i.e., the non-main diagonal going from the lower-left to the upper-right corner. For example, if \\(n=3\\) then \\[\n\\begin{align}\nP &= \\begin{bmatrix}\n0 & 0 & 1 \\newline 0 & 1 & 0 \\newline 1 & 0 & 0\n\\end{bmatrix}. \\tag{8}\n\\end{align}\n\\]\nWe will make use of the following properties of the matrix \\(P\\).\n\n\nProperties of Reversal Operator.  The matrix \\(P\\) that reverses the order of a vector satisfies \\[\n  P = P^\\top, \\qquad P = P^{-1}, \\qquad P^2 = P \\tag{9}\n  \\]\n\n\nThe first property is true of any anti-diagonal matrix, and the latter two simply reflect the fact that applying the reversal operation twice results in the original vector. With these properties in hand, note that \\[\n\\tilde{C} := \\text{Cov}[\\tilde{x}] = \\text{Cov}[Px]\n= P\\text{Cov}[x]P^\\top = PCP^\\top = PCP. \\tag{10}\n\\]\nIn words, this says that the covariance matrix of the reversed vector is given by \\(PCP\\), where \\(C\\) is the covariance of the original vector. If you prefer to avoid probabilistic language, then \\(PCP\\) is simply the result of reversing the order of the columns and rows of \\(C\\). Reversing \\(C\\) induces the same operation on its inverse, since \\[\n(\\tilde{C})^{-1} := (PCP)^{-1} = P^{-1}C^{-1}P = PC^{-1}P, \\tag{11}\n\\] where we have used (9)."
  },
  {
    "objectID": "blog/posts/reversed-Cholesky.html#cholesky-factorization-of-reversed-matrix",
    "href": "blog/posts/reversed-Cholesky.html#cholesky-factorization-of-reversed-matrix",
    "title": "The Reversed Cholesky Decomposition",
    "section": "Cholesky Factorization of Reversed Matrix",
    "text": "Cholesky Factorization of Reversed Matrix\nWe now derive the form of the Cholesky and rCholesky decompositions of the reversed matrix \\(\\tilde{C}\\). Notation becomes a bit confusing here, so we separate the two results for clarity.\n\n\nCholesky under reverse ordering.  Let \\(C = UU^\\top\\) be the rCholesky decomposition of \\(C\\). Then the Cholesky decomposition of \\(\\tilde{C} = PCP\\) is given by \\[\n  \\tilde{C} = (PUP)(PUP)^\\top. \\tag{12}\n  \\] This can be equivalently be written as \\[\n  \\text{chol}(\\tilde{C}) = \\text{chol}(PCP) = PUP = P\\text{rchol}(C)P. \\tag{13}\n  \\] In words, this says that the Cholesky factor of \\(\\tilde{C}\\) is given by reversing the rCholesky factor of \\(C\\).\n\n\nProof. Using the fact that \\(P^2 = I\\) we have \\[\n\\tilde{C} = PCP = P(UU^\\top)P = PUP^2 U^\\top P = (PUP)(PUP)^\\top.\n\\] The result is now immediate upon noticing that \\(PUP\\) is lower triangular and has positive diagonal entries. \\(\\qquad \\blacksquare\\)\n\n\nrCholesky under reverse ordering.  Let \\(C = LL^\\top\\) be the Cholesky decomposition of \\(C\\). Then the rCholesky decomposition of \\(\\tilde{C} = PCP\\) is given by \\[\n  \\tilde{C} = (PLP)(PLP)^\\top. \\tag{14}\n  \\] This can be equivalently be written as \\[\n  \\text{rchol}(\\tilde{C}) = \\text{rchol}(PCP) = PLP = P\\text{chol}(C)P. \\tag{15}\n  \\] In words, this says that the rCholesky factor of \\(\\tilde{C}\\) is given by reversing the Cholesky factor of \\(C\\).\n\n\nProof. Using the fact that \\(P^2 = I\\) we have \\[\n\\tilde{C} = PCP = P(LL^\\top)P = PLP^2 L^\\top P = (PLP)(PLP)^\\top.\n\\] The result is now immediate upon noticing that \\(PLP\\) is upper triangular and has positive diagonal entries. \\(\\qquad \\blacksquare\\)\nThese results tell us that we can use a Cholesky factorization of \\(C\\) to immediately compute the rCholesky factorization of the reversed matrix \\(\\tilde{C}\\). This statement is the same with the roles of Cholesky and rCholesky swapped. In (13) and (15) we can left and right multiply by \\(P\\) to obtain the equivalent expressions \\[\n\\begin{align}\n\\text{chol}(C) &= P\\text{rchol}(PCP)P \\tag{16} \\newline\n\\text{rchol}(C) &= P\\text{chol}(PCP)P. \\tag{17} \\newline\n\\end{align}\n\\] See remark 1 in {% cite SparseCholeskyVecchia %} for an example of an expression of this form, though the authors are decomposing \\(C^{-1}\\) in place of \\(C\\)."
  },
  {
    "objectID": "blog/about.html",
    "href": "blog/about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "blog/posts/eki_1/eki_introduction.html",
    "href": "blog/posts/eki_1/eki_introduction.html",
    "title": "Ensemble Kalman Methods for Solving Inverse Problems: An Introduction",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\Pr}{\\mathbb{P}}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\Def}{:=}\n\\newcommand{\\Cov}{\\mathrm{Cov}}\n\\newcommand{\\Var}{\\mathrm{Var}}\n\\newcommand{\\Gaussian}{\\mathcal{N}}\n\\newcommand{\\fwd}{\\mathcal{G}}\n\\newcommand{\\u}{u}\n\\newcommand{\\yobs}{y^{\\dagger}}\n\\newcommand{\\y}{y}\n\\newcommand{\\noise}{\\epsilon}\n\\newcommand{\\covNoise}{\\Sigma}\n\\newcommand{\\meanVec}{m}\n\\newcommand{\\covMat}{C}\n\\newcommand{\\dimObs}{n}\n\\newcommand{\\dimPar}{d}\n\\newcommand{\\parSpace}{\\mathcal{U}}\n\\newcommand{\\misfit}{\\Phi}\n\\newcommand{\\misfitReg}{\\Phi_R}\n\\newcommand{\\misfitPost}{\\Phi_{\\pi}}\n\\newcommand{\\covPrior}{\\covMat}\n\\newcommand{\\meanPrior}{\\meanVec}\n\\newcommand{\\dens}{\\pi}\n\\newcommand{\\priorDens}{\\pi_0}\n\\newcommand{\\postDens}{\\pi}\n\\newcommand{\\normCst}{Z}\n\\newcommand{\\joint}{\\overline{\\pi}}\n\\newcommand{\\meanObs}{\\meanVec^{\\y}}\n\\newcommand{\\covObs}{\\covMat^{\\y}}\n\\newcommand{\\covCross}{\\covMat^{\\u \\y}}\n\\newcommand{\\tcovCross}{\\covMat^{\\y \\u}}\n\\newcommand{\\GaussProj}{\\mathcal{P}_{\\Gaussian}}\n\\newcommand{\\meanPost}{\\meanVec_{\\star}}\n\\newcommand{\\covPost}{\\covMat_{\\star}}\n\\newcommand{\\transport}{T}\n\\newcommand{\\nens}{J}\n\\]\nThe ensemble Kalman filter (EnKF) is a well-established algorithm for state estimation in high-dimensional state space models. More recently, it has gained popularity as a general-purpose derivative-free tool for optimization and approximate posterior sampling; i.e., for the solution of inverse problems. The label ensemble Kalman inversion (EKI) is generally used to refer to the class of algorithms that adapt the EnKF methodology for such purposes. While these algorithms are typically quite simple – mostly relying on slight modifications of the standard EnKF update formula – there are quite a few subtleties required in designing and analyzing EKI methods. In particular, while much of the EKI literature is focused on optimization, small modifications of optimization-focused algorithms can be made to instead target the goal of posterior sampling. In a series of posts, we will walk through these subtleties, exploring the potential of the EnKF both as a derivative-free (approximate) optimizer and sampler. We start in this post by outlining the basic setup and goals, and then proceed to introduce a basic EnKF algorithm for approximate posterior sampling."
  },
  {
    "objectID": "blog/posts/eki_1/eki_introduction.html#optimization",
    "href": "blog/posts/eki_1/eki_introduction.html#optimization",
    "title": "Ensemble Kalman Methods for Solving Inverse Problems: An Introduction",
    "section": "1.1 Optimization",
    "text": "1.1 Optimization\nWe start by formulating the solution to the inverse problem as an optimization problem. One of the most basic approaches we might take is to seek the value of the parameter that minimizes the quadratic error between the data and the model prediction.\n\n\n\n\n\n\nGoal 1: Least-squares minimization\n\n\n\nDefine the least squares model-data misfit function \\[\n\\misfit(u) := \\frac{1}{2}\\lVert \\yobs - \\fwd(\\u)\\rVert^2_{\\covNoise} := \\frac{1}{2}(\\yobs - \\fwd(\\u))^\\top \\covNoise^{-1}(\\yobs - \\fwd(\\u)),\n\\tag{1}\\] weighted by a positive definite matrix \\(\\covNoise\\). The (nonlinear) least squares minimization problem is then given by \\[\nu_{\\star} \\in \\text{argmin}_{u \\in \\parSpace} \\ \\misfit(\\u).\n\\tag{2}\\]\n\n\nThe above definition also serves to define the notation we will be using throughout this series to denote weighted Euclidean norms. Note also that \\(\\misfit(u)\\) depends on the observed data \\(\\yobs\\), but we suppress this dependence in the notation. A natural extension to the least-squares problem is to add a regularization term to the objective function. We will focus on quadratic regularization terms, which is referred to as Tikhonov regularization and ridge regression in the inverse problems and statistical literatures, respectively.\n\n\n\n\n\n\nGoal 2: Tikhonov-regularized Least-squares minimization\n\n\n\nDefine the Tikhonov-regularized least squares function by \\[\n\\misfitReg(\\u) := \\frac{1}{2}\\lVert \\yobs - \\fwd(\\u)\\rVert^2_{\\covNoise} + \\frac{1}{2}\\lVert \\u - \\meanPrior\\rVert^2_{\\covPrior}.\n\\tag{3}\\] The Tikhonov-regularized least squares optimization problem is given by \\[\nu_{\\star} \\in \\text{argmin}_{u \\in \\parSpace} \\misfitReg(\\u).\n\\tag{4}\\]\n\n\nThe Tikhonov loss function balances the model fit to the data with the requirement to keep \\(\\u\\) “close” to \\(\\meanPrior\\), where the relative weights of these objectives are determined by the (positive definite) covariances \\(\\covNoise\\) and \\(\\covPrior\\)."
  },
  {
    "objectID": "blog/posts/eki_1/eki_introduction.html#sampling",
    "href": "blog/posts/eki_1/eki_introduction.html#sampling",
    "title": "Ensemble Kalman Methods for Solving Inverse Problems: An Introduction",
    "section": "1.2 Sampling",
    "text": "1.2 Sampling\nWe next consider the Bayesian formulation of the inverse problem, whereby the goal is no longer to identify a single value \\(\\u_{\\star}\\), but instead to construct a probability distribution over all possible \\(\\u\\). The Bayesian approach requires the definition of a joint distribution over the data and the parameter, \\((\\u,\\y)\\). We view the observed data \\(\\yobs\\) as a particular realization of the random variable \\(\\y\\). The solution of the Bayesian inverse problem is given by the conditional distribution \\(\\u \\given [\\y=\\yobs]\\), known as the posterior distribution. We will often shorten this notation by writing \\(\\u \\given \\yobs\\).\nThroughout this series, we will primarily focus on the joint distribution on \\((\\u, \\y)\\) induced by the following model: \\[\n\\begin{align}\n\\y &= \\fwd(\\u) + \\noise \\newline\n\\u &\\sim \\priorDens \\newline\n\\noise &\\sim \\Gaussian(0, \\covNoise),\n\\end{align}\n\\tag{5}\\] where \\(\\priorDens\\) is a prior distribution on the parameter, \\(\\covNoise\\) is the fixed (known) covariance of the additive Gaussian noise, and \\(\\u\\) and \\(\\noise\\) are independent. The EnKF methodology we will discuss is particularly well-suited to such additive Gaussian models with known noise covariance, but there has been work on relaxing these restrictions. The above model defines a joint distribution \\(\\joint\\) on \\((\\u,\\y)\\) via the product of densities \\[\np(\\u,\\y) := p(\\y \\given \\u) \\priorDens(\\u) = \\Gaussian(\\y \\given \\fwd(\\u), \\covNoise)\\priorDens(\\u),\n\\tag{6}\\] with the posterior density given by Bayes’ theorem \\[\n\\begin{align}\n\\postDens(\\u) &:= p(\\u \\given \\yobs) = \\frac{1}{\\normCst}\\Gaussian(\\yobs \\given \\fwd(\\u), \\covNoise)\\priorDens(\\u),\n&&\\normCst := \\int_{\\parSpace} \\Gaussian(\\yobs \\given \\fwd(\\u), \\covNoise)\\priorDens(\\u) d\\u.\n\\end{align}\n\\tag{7}\\] We omit the dependence on \\(\\yobs\\) in the notation \\(\\postDens(\\u)\\) and \\(Z\\).\n\n\n\n\n\n\nGoal 3: Posterior Sampling\n\n\n\nWe seek to draw samples from the posterior distribution \\(\\u \\given \\yobs\\) under the model Equation 5. We can phrase this as the task of sampling the probability distribution with density \\[\n\\postDens(\\u) \\propto \\exp\\left\\{\\misfitPost(\\u)\\right\\},\n\\tag{8}\\] where \\[\n\\misfitPost(\\u) := -\\log \\postDens(\\u)\n= -\\log p(\\y \\given \\u) - \\log \\priorDens(\\u)\n= \\frac{1}{2}\\lVert \\yobs - \\fwd(\\u)\\rVert^2_{\\covNoise} - \\log \\priorDens(\\u) + C,\n\\tag{9}\\] is the (unnormalized) negative log posterior density, up to an additive constant \\(C\\) that is independent of \\(\\u\\).\n\n\nWe introduce the notation \\(\\misfitPost(\\u)\\) in order to draw a connection with the optimization goals. Indeed, note that the log-likelihood term in Equation 9 is precisely the least squares misfit function Equation 1 (up to an additive constant). Moreover, if we choose Gaussian prior \\(\\priorDens(\\u) = \\Gaussian(\\u \\given \\meanPrior, \\covPrior)\\), then \\(\\misfitPost(\\u)\\) agrees with \\(\\misfitReg(\\u)\\) (again, up to an additive constant). We will explore certain algorithms that assume the prior is Gaussian, but in general we will allow \\(\\priorDens\\) to be non-Gaussian."
  },
  {
    "objectID": "blog/posts/eki_1/eki_introduction.html#roadmap",
    "href": "blog/posts/eki_1/eki_introduction.html#roadmap",
    "title": "Ensemble Kalman Methods for Solving Inverse Problems: An Introduction",
    "section": "1.3 Roadmap",
    "text": "1.3 Roadmap\nWith the setup and goals established, we will now take steps towards practical algorithms. It is important to recognize that the application of EnKF methodology to the optimization and sampling problems will yield approximate algorithms in general. The methods will be exact (in a manner which will be made precise) in the linear Gaussian setting, where the forward model \\(\\fwd\\) is linear and the prior \\(\\priorDens\\) is Gaussian. The EKI algorithms we consider are derivative-free, suitable for the black-box setting whereby we can only evaluate \\(\\fwd(\\cdot)\\) pointwise. In typical applications, function evaluations \\(\\fwd(\\cdot)\\) may be quite computationally expensive; e.g., they might require numerically solving partial differential equations. Another benefit of the EnKF methodology is that they allow for many model evaluations to be performed in parallel. These features will become clear as we dive into the methods. We start in this post by focusing on the sampling problem; the optimization setting will be explored in future posts."
  },
  {
    "objectID": "blog/posts/eki_1/eki_introduction.html#gaussian-projection",
    "href": "blog/posts/eki_1/eki_introduction.html#gaussian-projection",
    "title": "Ensemble Kalman Methods for Solving Inverse Problems: An Introduction",
    "section": "2.1 Gaussian Projection",
    "text": "2.1 Gaussian Projection\nRecall that we write \\(\\joint\\) to denote the joint distribution of \\((\\u,\\y)\\) under Equation 5. In general, this distribution will be non-Gaussian, rendering the conditioning \\(u \\given \\yobs\\) a challenging task. A simple idea is to consider approximating \\(\\joint\\) with a Gaussian, which is a distribution for which conditioning is easy. To this end, consider the approximation \\[\n\\begin{align}\n\\GaussProj\\joint \\Def\n\\Gaussian\\left(\n\\begin{bmatrix} \\meanPrior \\newline \\meanObs \\end{bmatrix},\n\\begin{bmatrix} \\covPrior & \\covCross \\newline\n                \\tcovCross & \\covObs \\end{bmatrix}\n\\right)\n\\end{align}\n\\tag{10}\\] where the means and covariances are simply given by the first two moments of \\((\\u, \\y)\\). In particular, \\[\n\\begin{align}\n&\\meanPrior \\Def \\E[\\u], &&\\covPrior \\Def \\Cov[\\u]\n\\end{align}\n\\tag{11}\\]\nare the moments of the \\(\\u\\)-marginal, and \\[\n\\begin{align}\n&\\meanObs \\Def \\E[\\y], &&\\covObs \\Def \\Cov[\\y]\n\\end{align}\n\\tag{12}\\]\nare the moments of the \\(\\y\\)-marginal. Finally, \\[\n\\begin{align}\n&\\covCross \\Def \\Cov[\\u,\\y], &&\\tcovCross \\Def [\\covCross]^\\top\n\\end{align}\n\\tag{13}\\]\nis the cross-covariance between \\(\\u\\) and \\(\\y\\). Following Calvello, Reich, and Stuart (2024), we refer to \\(\\GaussProj\\joint\\) as the Gaussian “projection” of \\(\\joint\\). This terminology is motivated by the fact that \\(\\GaussProj\\joint\\) can be seen to minimize the Kullback-Leibler divergence \\(\\text{KL}(\\joint \\parallel q)\\) over the space of Gaussians \\(q = \\Gaussian(\\meanVec^\\prime, \\covMat^\\prime)\\) (see Sanz-Alonso, Stuart, and Taeb (2023), chapter 4 for details).\nHaving invoked the joint Gaussian approximation in Equation 10, we can now approximate \\(\\u \\given \\yobs\\) with the Gaussian conditional. Gaussian conditionals are also Gaussian, and are thus characterized by the well-known conditional mean and covariance formulas given below.\n\n\n\n\n\n\nPosterior Approximation: Gaussian Conditional\n\n\n\nLet \\((\\tilde{\\u}, \\tilde{y})\\) be a random vector with distribution \\(\\GaussProj\\joint\\). We consider approximating the posterior \\(\\u \\given (\\y=\\yobs)\\) with \\(\\tilde{\\u} \\given (\\tilde{\\y}=\\yobs)\\). As it is the conditional of a Gaussian distribution, this posterior approximation is Gaussian, with moments \\[\n\\begin{align}\n\\meanPost &= \\meanPrior + \\covCross [\\covObs]^{-1} (\\yobs - \\meanObs) \\newline\n\\covPost &= \\covPrior + \\covCross [\\covObs]^{-1} \\tcovCross.\n\\end{align}\n\\tag{14}\\]"
  },
  {
    "objectID": "blog/posts/eki_1/eki_introduction.html#monte-carlo-approximations",
    "href": "blog/posts/eki_1/eki_introduction.html#monte-carlo-approximations",
    "title": "Ensemble Kalman Methods for Solving Inverse Problems: An Introduction",
    "section": "2.2 Monte Carlo Approximations",
    "text": "2.2 Monte Carlo Approximations\nWith the closed-form approximation Equation 14 in hand, we can simulate approximate posterior draws by constructing \\(\\meanPost\\) and \\(\\covPost\\), then sampling from \\(\\Gaussian(\\meanPost, \\covPost)\\). Interestingly, we can actually bypass the step of computing conditional moments and directly sample from the Gaussian conditional using a result known as Matheron’s rule.\n\n\n\n\n\n\nMatheron’s Rule: Gaussian Conditional Simulation\n\n\n\nLet \\((\\tilde{\\u}, \\tilde{y})\\) be random variables with distribution \\(\\GaussProj\\joint\\). Then the following equality holds in distribution. \\[\n\\begin{align}\n(\\tilde{\\u} \\given [\\tilde{\\y} = \\yobs]) &\\overset{d}{=} \\tilde{\\u} + \\covCross [\\covObs]^{-1} (\\yobs - \\tilde{\\y}).\n\\end{align}\n\\tag{15}\\]\nThis implies that independent samples from \\(\\tilde{\\u} \\given [\\tilde{\\y} = \\yobs]\\) can be simulated via the following algorithm.\n\nSample \\((\\u^\\prime, \\y^\\prime) \\sim \\GaussProj\\joint\\)\nReturn \\(\\transport(\\u^\\prime, \\y^\\prime)\\)\n\nwhere \\[\n\\transport(\\u^\\prime, \\y^\\prime) \\Def \\u^\\prime + \\covCross [\\covObs]^{-1} (\\yobs - \\y^\\prime)\n\\tag{16}\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe distribution of the lefthand side of Equation 15 is given in Equation 14. Notice that the righthand side is a linear function of the Gaussian random vector \\((\\tilde{\\u}, \\tilde{\\y})\\), and is thus Gaussian. It remains to verify that the mean and covariance of the righthand side agrees with Equation 14. The mean is given by \\[\n\\begin{align}\n\\E[\\transport(\\tilde{\\u}, \\tilde{\\y})]\n&= \\E[\\tilde{\\u}] + \\covCross [\\covObs]^{-1} (\\yobs - \\E[\\tilde{\\y}]) \\newline\n&= \\meanPrior + \\covCross [\\covObs]^{-1} (\\yobs - \\meanObs) \\newline\n&= \\E[\\tilde{\\u} \\given \\tilde{\\y} = \\yobs].\n\\end{align}\n\\] Similarly, the covariance is \\[\n\\begin{align}\n\\Cov[\\transport(\\tilde{\\u}, \\tilde{\\y})]\n\\end{align}\n\\] □\n\n\n\nThe map \\(\\transport(\\cdot, \\cdot)\\) is a deterministic function that transports samples from the joint Gaussian to its conditional distribution. Note that this map depends on the first two moments of \\(\\GaussProj\\joint\\).\nWe next consider a slight adjustment to the Matheron update, which results in (potentially) non-Gaussian approximate posterior samples. This yields the classical EnKF update equation.\n\n\n\n\n\n\nEnKF Update: Conditional Simulation\n\n\n\nAn alternative Monte Carlo posterior approximation can be obtained by modifying the above sampling strategy as follows:\n\nSample \\((u^\\prime, \\y^\\prime) \\sim \\joint\\)\nReturn \\(\\transport(\\u^\\prime, \\y^\\prime)\\)\n\nHere, \\(\\transport(\\cdot, \\cdot)\\) is the same transport map as defined in Equation 16. Sampling from the joint distribution in step one above entails sampling a parameter from the prior, then sampling from the likelihood: \\[\n\\begin{align}\n&\\u^\\prime \\sim \\priorDens \\newline\n&\\y^\\prime \\Def \\fwd(\\u^\\prime) + \\noise^\\prime, &&\\noise^\\prime \\sim \\Gaussian(0, \\covNoise)\n\\end{align}\n\\tag{17}\\]\n\n\nNote that the difference between the two above algorithms is that the former samples \\((u^\\prime, \\y^\\prime)\\) from the Gaussian projection \\(\\GaussProj\\joint\\), while the latter samples from the true joint distribution \\(\\joint\\). In both cases, the form of the transport map \\(\\transport\\) is derived from the Gaussian approximation \\(\\GaussProj\\joint\\). The EnKF update thus combines exact Monte Carlo sampling from the joint distribution with approximate conditioning motivated by a Gaussian ansatz. Since the samples \\((u^\\prime, \\y^\\prime)\\) are no longer Gaussian in general, then the approximate conditional samples \\(\\transport(u^\\prime, \\y^\\prime)\\) can also be non-Gaussian. One might hope that this additional flexibility leads to an improved approximation. We conclude this section by defining the Kalman gain, which forms the core of the Matheron transport map \\(\\transport\\).\n\n\n\n\n\n\nKalman Gain\n\n\n\nThe Kalman gain associated with the inverse problem in Equation 5 is defined as \\[\nK := \\covCross [\\covObs]^{-1}.\n\\tag{18}\\] The transport map in Equation 16 can thus be written as \\[\n\\transport(\\u^\\prime, \\y^\\prime) = \\u^\\prime + K(\\yobs - \\y^\\prime).\n\\tag{19}\\]\n\n\nWe thus see that the transport map takes the prior sample \\(\\u^\\prime\\) and adds a “correction” term based on the data. The correction is the linear map \\(K\\) applied to the residual \\(\\yobs - \\y^\\prime\\) (i.e., the difference between the observed and predicted data)."
  },
  {
    "objectID": "blog/posts/eki_1/eki_introduction.html#practical-algorithms",
    "href": "blog/posts/eki_1/eki_introduction.html#practical-algorithms",
    "title": "Ensemble Kalman Methods for Solving Inverse Problems: An Introduction",
    "section": "2.3 Practical Algorithms",
    "text": "2.3 Practical Algorithms\nThe methods presented above do not yet constitute algorithms, as we have not specified how to compute the moments defining the Gaussian projection Equation 10. By replacing the exact moments with Monte Carlo estimates, we obtain an algorithm which we will refer to as single-step ensemble Kalman inversion (EKI). Given samples, \\(\\{u^{(j)}, \\y^{(j)}\\}_{j=1}^{\\nens} \\sim \\joint\\), we can estimate the required moments via the standard Monte Carlo estimates: \\[\n\\begin{align}\n&\\meanPrior \\Def \\frac{1}{\\nens} \\sum_{j=1}^{\\nens} \\u^{(j)},\n&&\\covPrior \\Def \\frac{1}{\\nens-1} \\sum_{j=1}^{\\nens} (\\u^{(j)}-\\meanPrior)(\\u^{(j)}-\\meanPrior)^\\top \\newline\n&\\meanObs \\Def \\frac{1}{\\nens} \\sum_{j=1}^{\\nens} \\y^{(j)},\n&&\\covObs \\Def \\frac{1}{\\nens-1} \\sum_{j=1}^{\\nens} (\\y^{(j)}-\\meanObs)(\\y^{(j)}-\\meanObs)^\\top\n\\end{align}\n\\tag{20}\\]\n\\[\n\\begin{equation}\n\\covCross \\Def \\frac{1}{\\nens-1} \\sum_{j=1}^{\\nens} (\\u^{(j)}-\\meanPrior)(\\y^{(j)}-\\meanObs)^\\top\n\\end{equation}\n\\]\nNote that we utilize the same notation for the exact moments and their empirical estimates; the precise meaning of the notation will be made clear from context.\n\nAlgorithm: Single Step EKI \\[\n\\begin{array}{ll}\n\\textbf{Input:} & \\text{Sample size } \\nens \\\\\n\\textbf{Output:} & \\text{Approximate posterior samples } \\{\\u_{\\star}^{(j)}\\}_{j=1}^{\\nens} \\\\\n\\hline\n\\textbf{1:} & \\text{Sample prior: } &&\\u^{(j)} \\overset{\\text{iid}}{\\sim} \\priorDens, \\ j = 1, \\dots, \\nens \\\\\n\\textbf{2:} & \\text{Sample likelihood: } &&\\y^{(j)} \\Def \\fwd(\\u^{(j)}) + \\noise^{(j)}, \\ \\noise^{(j)} \\overset{\\text{iid}}{\\sim} \\Gaussian(0, \\covNoise) \\\\\n\\textbf{3:} & \\text{Estimate moments: } &&\\meanPrior, \\covPrior, \\meanObs, \\covObs \\\\\n\\textbf{4:} & \\text{Transport samples: } &&\\u_{\\star}^{(j)} \\Def \\u^{(j)} + \\covCross [\\covObs]^{-1}(\\yobs - \\y^{(j)})\n\\end{array}\n\\]\nThe definition of the empirical moments is given in Equation 20.\n\nOther considerations: - If prior is Gaussian, then no need to estimate m, C - Reducing sampling bias in covariance estimate of Cy. - Gaussian algorithm algorithm can be defined by returning conditional moments. - Cite Higdon paper."
  },
  {
    "objectID": "blog/posts/eki_1/eki_introduction.html#interpolating-between-probability-densities",
    "href": "blog/posts/eki_1/eki_introduction.html#interpolating-between-probability-densities",
    "title": "Ensemble Kalman Methods for Solving Inverse Problems: An Introduction",
    "section": "3.1 Interpolating between probability densities",
    "text": "3.1 Interpolating between probability densities\nThere are many ways to construct the sequence Equation 21; i.e., to interpolate between two probability distributions \\(\\priorDens\\) and \\(\\postDens\\). We will start by considering the following basic tempering schedule. We start with a generic result for two arbitrary densities, and then specialize to our particular setting.\n\n\n\n\n\n\nInterpolating between two densities\n\n\n\nLet \\(\\priorDens(\\u)\\) and \\(\\postDens(\\u) = \\frac{1}{\\normCst}\\tilde{\\dens}(\\u)\\) be two probability densities on \\(\\parSpace\\). For a positive integer \\(K\\), define the sequence of densities \\(\\dens_0, \\dens_1, \\dots, \\dens_K\\) recursively by\n\\[\n\\begin{align}\n\\dens_{k+1}(\\u) &:= \\frac{1}{\\normCst_{k+1}}\\dens_k(\\u)L(\\u)^{1/K},\n&&\\normCst_{k+1} := \\int \\dens_k(\\u)L(\\u)^{1/K} d\\u,\n\\end{align}\n\\tag{22}\\]\nfor \\(k = 0, \\dots, K-1\\), where \\[\nL(\\u) \\Def \\frac{\\tilde{\\dens}(\\u)}{\\priorDens(\\u)}.\n\\tag{23}\\] Then the final density satisfies \\(\\dens_K = \\postDens\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nTo start, note that the density ratio Equation 23 can be written as \\[\nL(\\u) = \\frac{\\tilde{\\dens}(\\u)}{\\priorDens(\\u)}\n= \\frac{Z \\postDens(\\u)}{\\priorDens(\\u)}.\n\\]\nWe use this fact, and the recursion Equation 22 to obtain \\[\n\\begin{align}\n\\normCst_K\n&= \\int \\dens_{K-1}(\\u)L(\\u)^{1/K} d\\u \\\\\n&= \\int \\dens_{0}(\\u)L(\\u)^{1/K} \\prod_{k=1}^{K-1} \\frac{L(\\u)^{1/K}}{Z_k} d\\u \\\\\n&= \\frac{1}{\\normCst_1 \\cdots \\normCst_{K-1}} \\int \\priorDens(\\u)L(\\u) d\\u \\\\\n&= \\frac{1}{\\normCst_1 \\cdots \\normCst_{K-1}} \\int \\priorDens(\\u)\\frac{\\normCst \\postDens(\\u)}{\\priorDens(\\u)} d\\u \\\\\n&= \\frac{\\normCst}{\\normCst_1 \\cdots \\normCst_{K-1}}  \\int \\postDens(\\u) d\\u \\\\\n&= \\frac{\\normCst}{\\normCst_1 \\cdots \\normCst_{K-1}}.\n\\end{align}\n\\]\nThe density recursion similarly yields \\[\n\\begin{align}\n\\dens_K(\\u)\n&= \\frac{1}{\\normCst_{K}}\\dens_{K-1}(\\u)L(\\u)^{1/K} \\\\\n&= \\frac{\\normCst_1 \\cdots \\normCst_{K-1}}{\\normCst} \\priorDens(\\u)\\frac{L(\\u)}{\\normCst_1 \\cdots \\normCst_{K-1}} \\\\\n&= \\frac{1}{\\normCst} \\priorDens(\\u) L(\\u) \\\\\n&= \\postDens(\\u),\n\\end{align}\n\\] where we have plugged in the expressions for \\(\\normCst_K\\) and \\(L(\\u)\\) derived above. □\n\n\n\nThe above result constructs a sequence between two arbitrary densities \\(\\priorDens\\) and \\(\\postDens\\). If we choose these to be the prior and posterior distributions, then we obtain a prior-to-posterior map as a corollary.\n\n\n\n\n\n\nPrior-to-Posterior Map in Finite Time\n\n\n\nConsider a Bayesian joint distribution \\(p(\\u, \\y) = \\priorDens(\\u)p(\\y \\given \\u)\\) with posterior \\(\\postDens(\\u) \\Def \\frac{1}{\\normCst} \\priorDens(\\u) p(\\yobs \\given \\u)\\) where \\(\\normCst = p(\\y)\\). Then the sequence \\(\\dens_0, \\dens_1, \\dots, \\dens_K\\) defined by \\[\n\\begin{align}\n\\dens_{k+1}(\\u) &:= \\frac{1}{\\normCst_{k+1}}\\dens_k(\\u)p(\\yobs \\given \\u)^{1/K},\n&&\\normCst_{k+1} := \\int \\dens_k(\\u)p(\\yobs \\given \\u)^{1/K} d\\u,\n\\end{align}\n\\tag{24}\\] for \\(k = 0, \\dots, K-1\\), satisfies \\(\\dens_{K} = \\postDens\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThis is a special case of Equation 22 where \\(\\tilde{\\postDens}(\\u) = \\priorDens(\\u)p(\\yobs \\given \\u)\\). Thus, the density ratio simplifies to \\[\nL(\\u) = \\frac{\\priorDens(\\u)p(\\yobs \\given \\u)}{\\priorDens(\\u)} = p(\\yobs \\given \\u).\n\\] □\n\n\n\nWe conclude this section with another corollary that specializes the result even further to the particular Bayesian inverse problem Equation 5.\n\n\n\n\n\n\nPrior-to-Posterior Map with Gaussian Likelihood\n\n\n\nConsider the particular Bayesian joint distribution \\(p(\\u, \\y)\\) defined by the model in Equation 5. Then the updates in Equation 24 take the particular form \\[\n\\begin{align}\n\\dens_{k+1}(\\u) &:= \\frac{1}{\\normCst_{k+1}}\\dens_k(\\u)\\Gaussian(\\yobs \\given \\fwd(\\u), K\\covNoise),\n&&\\normCst_{k+1} := \\int \\dens_k(\\u)\\Gaussian(\\yobs \\given \\fwd(\\u), K\\covNoise) d\\u.\n\\end{align}\n\\tag{25}\\]\nThis update can equivalently be written as \\[\n\\begin{align}\n\\dens_{k+1}(\\u) &:= \\frac{1}{\\normCst_{k+1}}\\dens_k(\\u)\\exp\\left\\{-\\frac{1}{K}\\misfit(\\u) \\right\\},\n&&\\normCst_{k+1} := \\int \\dens_k(\\u)\\exp\\left\\{-\\frac{1}{K}\\misfit(\\u) \\right\\} d\\u,\n\\end{align}\n\\tag{26}\\] where \\(\\misfit(\\u)\\) is defined in Equation 1. In either case, \\(\\dens_{K} = \\postDens\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nRecall the Gaussian density \\[\n\\begin{align}\n&\\Gaussian(\\y \\given \\fwd(\\u), \\covNoise) =\n\\text{det}(2\\pi\\covNoise)^{-1/2} \\exp\\left\\{-\\misfit(\\u) \\right\\},\n&&\\misfit(\\u) = \\frac{1}{2} \\lVert \\y - \\fwd(\\u)\\rVert^2_{\\covNoise}.\n\\end{align}\n\\]\nRaising this density to the power \\(K^{-1}\\) thus gives \\[\n\\begin{align}\n\\Gaussian(\\y \\given \\fwd(\\u), \\covNoise)^{1/K}\n&= \\text{det}(2\\pi K \\covNoise)^{-1/2} \\exp\\left\\{-\\frac{1}{K}\\misfit(\\u) \\right\\}, \\\\\n&= \\text{det}(2\\pi K \\covNoise)^{-1/2} \\exp\\left\\{-\\frac{1}{2} \\lVert \\y - \\fwd(\\u)\\rVert^2_{K \\covNoise} \\right\\} \\\\\n&= \\Gaussian(\\yobs \\given \\fwd(\\u), K \\covNoise).\n\\end{align}\n\\] The two updates Equation 25 thus follow, and differ only in whether we treat the determinant term as part of the normalizing constant \\(\\normCst\\) or the likelihood. □\n\n\n\nAs noted above, the approximation algorithm summarized in (27) can be viewed as performing an iteration of the EnKF algorithm. In this section, we take this idea further by encoding the structure of the inverse problem (1) in a dynamical system. Once these artificial dynamics are introduced, we can then consider applying standard algorithms for Bayesian estimation of time-evolving systems (e.g., the EnKF) to solve the inverse problem. We emphasize that (1), the original problem we are trying to solve, is static. The dynamics we introduce here are purely artificial, introduced for the purpose of making a mathematical connection with the vast field of time-varying systems. This allows us to port methods that have demonstrated success in the time-varying domain to our current problem of interest."
  },
  {
    "objectID": "blog/posts/eki_1/eki_introduction.html#prior-to-posterior-map-in-finite-time",
    "href": "blog/posts/eki_1/eki_introduction.html#prior-to-posterior-map-in-finite-time",
    "title": "Ensemble Kalman Methods for Solving Inverse Problems: An Introduction",
    "section": "3.2 Prior-to-Posterior Map in Finite Time",
    "text": "3.2 Prior-to-Posterior Map in Finite Time\nWe start by generically considering how to construct a sequence of probability distributions that maps from the prior \\(\\pi_0\\) to the posterior \\(\\pi\\) in a finite number of steps. We will find it convenient to write the likelihood with respect to the potential \\(\\Phi(u)\\) defined in (3), such that the posterior distribution solving (1) is given by"
  },
  {
    "objectID": "blog/posts/eki_1/eki_introduction.html#notation",
    "href": "blog/posts/eki_1/eki_introduction.html#notation",
    "title": "Ensemble Kalman Methods for Solving Inverse Problems: An Introduction",
    "section": "4.1 Notation",
    "text": "4.1 Notation\nWe write \\[\n\\lVert x \\rVert^2_A := \\langle x, x\\rangle_A := x^\\top A^{-1}x\n\\] to denote the Euclidean norm weighted by the inverse of a positive definite matrix \\(A\\)."
  },
  {
    "objectID": "blog/posts/eki_1/eki_introduction.html#joint-gaussian-assumption",
    "href": "blog/posts/eki_1/eki_introduction.html#joint-gaussian-assumption",
    "title": "Ensemble Kalman Methods for Solving Inverse Problems: An Introduction",
    "section": "5.1 Joint Gaussian Assumption",
    "text": "5.1 Joint Gaussian Assumption\nWe now begin by considering approximation of the posterior \\(u|y\\) by way of a certain Gaussian approximation. In particular, we will assume that \\((u,y)\\) are jointly Gaussian distributed, in which case standard Gaussian conditioning identities can be implied to yield an approximation of \\(u|y\\). Given that conditionals of Gaussians are also Gaussian, this approach produces a Gaussian approximation to the posterior \\(u|y\\). However, borrowing an idea from EnKF methodology, we will consider a slight modification with the ability to produce non-Gaussian approximations. To avoid notational confusion between exact and approximate distributions, we will denote by \\((\\hat{u}, \\hat{y})\\) the random variables defining the joint Gaussian approximation. The Gaussian assumption thus takes the form \\[\n\\begin{align}\n\\begin{bmatrix} \\hat{u} \\newline \\hat{y} \\end{bmatrix}\n\\sim\n\\mathcal{N}\\left(\n\\begin{bmatrix} \\overline{u} \\newline \\overline{y} \\end{bmatrix},\n\\begin{bmatrix} \\hat{C} & \\hat{C}^{uy} \\newline\n                \\hat{C}^{yu} & \\hat{C}^{y} \\end{bmatrix}\n\\right) \\tag{6}\n\\end{align}\n\\] where the moments \\(\\overline{u}\\), \\(\\overline{y}\\), \\(\\hat{C}\\), \\(\\hat{C}^y\\), and \\(\\hat{C}^{uy}\\) defining this approximation are quantities that we must specify. We use the notation \\(\\hat{C}^{yu} := \\hat{C}^{uy}\\). Note that if the forward model \\(\\mathcal{G}\\) is linear and the prior distribution \\(\\pi_0\\) is Gaussian, then the joint Gaussian approximation (6) is actually exact, and the moments can be computed in closed-form. In other words, with the moments properly defined, \\((\\hat{u},\\hat{y}) \\overset{d}{=} (u,y)\\) and therefore \\((\\hat{u}|\\hat{y}) \\overset{d}{=} (u|y)\\); that is, the posterior approximation is exact. This special case is typically referred to as the linear Gaussian setting, which I discuss in depth in this this post. When \\(\\mathcal{G}\\) is nonlinear and/or \\(\\pi_0\\) is non-Gaussian, then (6) will truly be an approximation and the above equalities will not hold.\nIn the following subsections, we briefly review some properties of joint Gaussians and their conditional distributions. We work with the joint distribution (6), assuming the means and covariances are known. With the necessary background established, we then discuss practical algorithms for estimating these moments and producing approximations of \\(u|y\\)."
  },
  {
    "objectID": "blog/posts/eki_1/eki_introduction.html#gaussian-conditional-moments",
    "href": "blog/posts/eki_1/eki_introduction.html#gaussian-conditional-moments",
    "title": "Ensemble Kalman Methods for Solving Inverse Problems: An Introduction",
    "section": "5.2 Gaussian Conditional Moments",
    "text": "5.2 Gaussian Conditional Moments\nRegardless of whether or not we are truly in the linear Gaussian setting, let us suppose that we have constructed the joint distribution (6). Using standard facts about Gaussian distributions, we know the conditional is also Gaussian \\[\n\\hat{u}|[\\hat{y}=y] \\sim \\mathcal{N}(\\hat{m}_*, \\hat{C}_*), \\tag{7}\n\\] with moments given by\n\\[\n\\hat{m}_* = \\overline{u} + \\hat{C}^{uy}[\\hat{C}^y]^{-1}(y - \\overline{y}) \\tag{8}\n\\] \\[\n\\hat{C}_* = \\hat{C} - \\hat{C}^{uy}[\\hat{C}^y]^{-1}\\hat{C}^{yu}. \\tag{9}\n\\]"
  },
  {
    "objectID": "blog/posts/eki_1/eki_introduction.html#gaussian-conditional-simulation",
    "href": "blog/posts/eki_1/eki_introduction.html#gaussian-conditional-simulation",
    "title": "Ensemble Kalman Methods for Solving Inverse Problems: An Introduction",
    "section": "5.3 Gaussian Conditional Simulation",
    "text": "5.3 Gaussian Conditional Simulation\nAs an alternative to explicitly computing the conditional moments (8) and (9), we can consider a Monte Carlo representation of \\(\\hat{u}|\\hat{y}\\). The conditional distribution can be directly simulated (without computing (8) and (9)) using the fact \\[\n(\\hat{u}|[\\hat{y}=y]) \\overset{d}{=} \\hat{u} + \\hat{C}^{uy}[\\hat{C}^y]^{-1}(y-\\hat{y}), \\tag{10}\n\\] which can be quickly verified by computing the mean and covariance of each side. Note that the randomness in the righthand side is inherited from the random variables \\(\\hat{u}\\) and \\(\\hat{y}\\), while \\(y\\) here is viewed as a specific realization of the data (and is thus non-random). The result (10), known as Matheron’s Rule, provides the basis for the following algorithm to draw independent samples from the conditional distribution \\(\\hat{u}|[\\hat{y}=y]\\).\n\n\nGaussian Conditional Simulation via Matheron’s Rule.  Independent samples \\(\\hat{u}_*^{(1)}, \\dots, \\hat{u}_*^{(J)}\\) can be simulated from the distribution \\(\\hat{u}|[\\hat{y}=y]\\) by repeating the following procedure for each \\(j=1,\\dots,J\\):  \n\nDraw independent samples \\(\\hat{u}^{(j)}\\) and \\(\\hat{y}^{(j)}\\) from the marginal distributions of \\(\\hat{u}\\) and \\(\\hat{y}\\), respectively. That is, \\[\\begin{align}\n  &\\hat{u}^{(j)} \\sim \\mathcal{N}(\\overline{u},\\hat{C})\n  &&\\hat{y}^{(j)} \\sim \\mathcal{N}(\\overline{y},\\hat{C}^y). \\tag{11}\n  \\end{align}\\]\nReturn \\[\n  \\hat{u}^{(j)}_* := \\hat{u}^{(j)} + \\hat{C}^{uy}[\\hat{C}^y]^{-1}(y-\\hat{y}^{(j)}). \\tag{12}\n  \\]"
  },
  {
    "objectID": "blog/posts/eki_1/eki_introduction.html#estimating-the-gaussian-moments",
    "href": "blog/posts/eki_1/eki_introduction.html#estimating-the-gaussian-moments",
    "title": "Ensemble Kalman Methods for Solving Inverse Problems: An Introduction",
    "section": "6.1 Estimating the Gaussian Moments",
    "text": "6.1 Estimating the Gaussian Moments\nThe first step in our approach requires generating the prior ensemble \\[\n\\{(u^{(j)}, \\epsilon^{(j)})\\}, \\qquad j = 1, \\dots, J \\tag{13}\n\\] constructed by sampling according to model (1); i.e., \\[\n\\begin{align}\n&u^{(j)} \\sim \\pi_0, &&\\epsilon^{(j)} \\sim \\mathcal{N}(0, \\Sigma). \\tag{14}\n\\end{align}\n\\] We now consider estimating the first two moments of this joint distribution. Starting with the \\(u\\) marginal, we define the sample estimates \\[\n\\begin{align}\n\\overline{u} &:= \\frac{1}{J}\\sum_{j=1}^{J} u^{(j)} \\tag{15} \\newline\n\\hat{C} &:= \\frac{1}{J-1}\\sum_{j=1}^{J} (u^{(j)}-\\overline{u})(u^{(j)}-\\overline{u})^\\top. \\tag{16}\n\\end{align}\n\\] Alternatively, if the prior \\(\\pi_0\\) takes the form of a well-known distribution, then we can simply set \\(\\overline{u}\\) and/or \\(\\hat{C}\\) to the known moments of this distribution. We can likewise consider such mean and covariance estimates for the \\(\\hat{y}\\) portion of (6), defined with respect to the ensemble \\[\n\\{y^{(j)}\\}_{j=1}^{J}, \\qquad\\qquad y^{(j)} := \\mathcal{G}(u^{(j)}) + \\epsilon^{(j)}. \\tag{17}\n\\] However, we can simplify matters a bit by performing part of the calculations analytically, owing to the simple additive Gaussian error structure. Noting that under (1) we have \\[\n\\begin{align}\n\\mathbb{E}[y]\n&= \\mathbb{E}[\\mathcal{G}(u) + \\epsilon]\n= \\mathbb{E}[\\mathcal{G}(u)] \\tag{18} \\newline\n\\text{Cov}[y]\n&= \\text{Cov}[\\mathcal{G}(u) + \\epsilon]\n= \\text{Cov}[\\mathcal{G}(u)] + \\text{Cov}[\\epsilon]\n= \\text{Cov}[\\mathcal{G}(u)] + \\Sigma, \\tag{19}\n\\end{align}\nwe can focus our efforts on substituting sample-based estimates for the first\nterm in both (18) and (19). Doing so yields,\n\\begin{align}\n\\overline{y} &:= \\frac{1}{J} \\sum_{j=1}^{J} \\mathcal{G}(u^{(j)}) \\tag{20} \\newline\n\\hat{C}^y &:= \\frac{1}{J-1} \\sum_{j=1}^{J} (\\mathcal{G}(u^{(j)})-\\overline{y})(\\mathcal{G}(u^{(j)})-\\overline{y})^\\top + \\Sigma. \\tag{21}\n\\end{align}\n\\] We similarly define the last remaining quantity \\(\\hat{C}^{uy}\\) by noting that \\[\n\\text{Cov}[u,y]\n= \\text{Cov}[u,\\mathcal{G}(u)+\\epsilon]\n= \\text{Cov}[u,\\mathcal{G}(u)] + \\text{Cov}[u,\\epsilon]\n= \\text{Cov}[u,\\mathcal{G}(u)]. \\tag{22}\n\\] We thus consider the estimator \\[\n\\hat{C}^{uy} := \\frac{1}{J-1} \\sum_{j=1}^{J} (u^{(j)}-\\overline{u})(\\mathcal{G}(u^{(j)})-\\overline{y})^\\top. \\tag{23}\n\\]"
  },
  {
    "objectID": "blog/posts/eki_1/eki_introduction.html#gaussian-approximations",
    "href": "blog/posts/eki_1/eki_introduction.html#gaussian-approximations",
    "title": "Ensemble Kalman Methods for Solving Inverse Problems: An Introduction",
    "section": "6.2 Gaussian Approximations",
    "text": "6.2 Gaussian Approximations\nAt this point, we have obtained the joint approximation (6) derived by computing the sample moments as described in the previous subsection. The question is now how best to use this joint approximation in approximating the true posterior. The most straightforward approach is to simply define our approximate posterior to be \\(\\hat{u}|[\\hat{y}=y]\\), the Gaussian conditional. This conditional is available in closed-form and is defined in equations (7), (8), and (9). Suppose, for whatever reason, we are only interested in a sample-based representation of this Gaussian conditional. In this case, we can apply the algorithm summarized in (11) and (12) based on Matheron’s Rule. Explicitly, we construct an approximate posterior ensemble \\(\\{\\hat{u}_*^{(j)}\\}_{j=1}^{J^\\prime}\\) via the update equation in (12), \\[\n\\hat{u}^{(j)}_* := \\hat{u}^{(j)} + \\hat{C}^{uy}[\\hat{C}^y]^{-1}(y-\\hat{y}^{(j)}), \\tag{24}\n\\] for \\(j = 1, \\dots, J^\\prime\\). At this point we should be quite clear about the definition of each term in (24). The mean and covariance estimates appearing in this expression are all derived from the ensemble \\(\\{(u^{(j)}, \\epsilon^{(j)})\\}_{j=1}^{J}\\) sampled from the true joint distribution implied by (1). On the other hand, \\(\\{\\hat{u}^{(j)}, \\hat{y}^{(j)}\\}_{j=1}^{J^\\prime}\\) are samples from the approximate joint distribution defined in (6). This is simply an exact implementation of Matheron’s rule under the joint distribution (6); i.e., the resulting ensemble \\(\\{\\hat{u}^{(j)}_*\\}_{j=1}^{J^\\prime}\\) contains iid samples from \\(\\hat{u}|[\\hat{y}=y]\\). If it seems odd to dwell on this point, we note that we are primarily doing so to provide motivation for the alternative method discussed below."
  },
  {
    "objectID": "blog/posts/eki_1/eki_introduction.html#non-gaussian-approximation-via-enkf-update",
    "href": "blog/posts/eki_1/eki_introduction.html#non-gaussian-approximation-via-enkf-update",
    "title": "Ensemble Kalman Methods for Solving Inverse Problems: An Introduction",
    "section": "6.3 Non-Gaussian Approximation via EnKF Update",
    "text": "6.3 Non-Gaussian Approximation via EnKF Update\nAs discussed above, a direct application of Matheron’s rule to (6) results in an ensemble of samples from a Gaussian distribution, which we interpret as a sample-based approximation to the true posterior. We now consider constructing the ensemble \\(\\{\\hat{u}_*^{(j)}\\}_{j=1}^{J}\\) via the slightly different update rule \\[\n\\hat{u}^{(j)}_* := u^{(j)} + \\hat{C}^{uy}[\\hat{C}^y]^{-1}(y-y^{(j)}), \\tag{25}\n\\] for \\(j = 1, \\dots, J\\). This is precisely the update equation used in the analysis (i.e., update) step of the EnKF, so we refer to (25) as the EnKF update. The only difference from (24) is that we have replaced the samples \\((\\hat{u}^{(j)},\\hat{y}^{(j)})\\) from (6) with the samples \\((u^{(j)}, y^{(j)})\\) from the true joint distribution, as defined in (14) and (17). In other words, we now utilize the samples from the initial ensemble (the same samples used to compute the mean and covariance estimates). Thus, equation (25) can be viewed as a particle-by-particle update to the initial ensemble, \\[\n\\{u^{(j)}\\}_{j=1}^{J} \\mapsto \\{\\hat{u}^{(j)}_*\\}_{j=1}^{J}. \\tag{26}\n\\] The update (25) is somewhat heuristic, and it is difficult to assess the properties of the updated ensemble. Given that the initial ensemble will not in general constitute samples from a Gaussian (since \\(p(u,y)\\) is in general non-Gaussian), then the updated ensemble will also generally be non-Gaussian distributed. If the inverse problem (1) is linear Gaussian, then the updates (24) and (25) are essentially equivalent. Beyond this special case, the hope is that the use of the true prior samples in (25) will better approximate non-Gaussianity in the posterior distribution, as opposed to the Gaussian approximation implied by (24). The EnKF update can alternatively be justified from an optimization perspective, which we won’t discuss here; see my post on the EnKF for details. We conclude this section by summarizing the final algorithm.\n\n\nPosterior Approximation via EnKF Update.  1. Generate the initial ensembles \\(\\{u^{(j)}\\}_{j=1}^{J}\\) and \\(\\{\\mathcal{G}(u^{(j)})\\}_{j=1}^{J}\\), where \\(u^{(j)} \\sim \\pi_0\\).  2. Compute the sample estimates \\(\\overline{u}\\), \\(\\overline{y}\\), \\(\\hat{C}\\), \\(\\hat{C}^y\\), and \\(\\hat{C}^{uy}\\) as defined in (20)-(23).  3. Return the updated ensemble \\(\\{\\hat{u}^{(j)}_*\\}_{j=1}^{J}\\) by applying the EnKF update \\[\\begin{align}\n  \\hat{u}^{(j)}_* &:= u^{(j)} + \\hat{C}^{uy}[\\hat{C}^y]^{-1}(y-y^{(j)}),\n  &&y^{(j)} \\sim \\mathcal{N}(\\mathcal{G}(u^{(j)}), \\Sigma). \\tag{27}\n  \\end{align}\\]"
  },
  {
    "objectID": "blog/posts/eki_1/eki_introduction.html#prior-to-posterior-map-in-finite-time-1",
    "href": "blog/posts/eki_1/eki_introduction.html#prior-to-posterior-map-in-finite-time-1",
    "title": "Ensemble Kalman Methods for Solving Inverse Problems: An Introduction",
    "section": "7.1 Prior-to-Posterior Map in Finite Time",
    "text": "7.1 Prior-to-Posterior Map in Finite Time\nWe start by generically considering how to construct a sequence of probability distributions that maps from the prior \\(\\pi_0\\) to the posterior \\(\\pi\\) in a finite number of steps. We will find it convenient to write the likelihood with respect to the potential \\(\\Phi(u)\\) defined in (3), such that the posterior distribution solving (1) is given by\n\\[\n\\begin{align}\n\\pi(u) &= \\frac{1}{Z} \\pi_0(u)\\exp(-\\Phi(u)), &&Z = \\int \\pi_0(u)\\exp(-\\Phi(u)) du. \\tag{28}\n\\end{align}\n\\]\nThe following result describes a likelihood tempering approach to constructing the desired sequence of densities.\n\n\nPrior-to-Posterior Sequence of Densities.  For a positive integer \\(K\\), define the sequence of densities \\(\\pi_0, \\pi_1, \\dots, \\pi_K\\) recursively by \\[\\begin{align}\n  \\pi_{k+1}(u) &:= \\frac{1}{Z_{k+1}}\\pi_k(u)\\exp\\left(-\\frac{1}{K}\\Phi(u)\\right),\n  &&Z_{k+1} := \\int \\pi_k(u)\\exp\\left(-\\frac{1}{K}\\Phi(u)\\right) du, \\tag{29}\n  \\end{align}\\] for \\(k = 0, \\dots, K-1\\). Then the final density satisfies \\(\\pi_K = \\pi\\), where \\(\\pi\\) is defined in (28).\n\n\nProof. We first consider the normalizing constant at the final iteration: \\[\n\\begin{align}\nZ_K\n&= \\int \\pi_{K-1}(u)\\exp\\left(-\\frac{1}{K}\\Phi(u)\\right) du \\newline\n&= \\int \\pi_{0}(u)\\frac{1}{Z_{K-1} \\cdots Z_1}\\exp\\left(-\\frac{1}{K}\\Phi(u)\\right)^K du \\newline\n&= \\frac{1}{Z_{K-1} \\cdots Z_1} \\int \\pi_{0}(u)\\exp\\left(-\\Phi(u)\\right) du \\newline\n&= \\frac{Z}{Z_1 \\cdots Z_{K-1}},\n\\end{align}\n\\] where the second equality simply iterates the recursion (29). Rearranging, we see that \\[\nZ = \\prod_{k=1}^{K} Z_k. \\tag{30}\n\\]\nWe similarly iterate the recursion for \\(\\pi_K\\): \\[\n\\begin{align}\n\\pi_K(u)\n&= \\frac{1}{Z_K} \\pi_{K-1}(u) \\exp\\left(-\\frac{1}{K}\\Phi(u)\\right) \\newline\n&= \\frac{1}{Z_K Z_{K-1} \\cdots Z_1} \\pi_{0}(u) \\exp\\left(-\\frac{1}{K}\\Phi(u)\\right)^K \\newline\n&= \\frac{1}{Z} \\pi_{0}(u) \\exp\\left(-\\Phi(u)\\right) \\newline\n&= \\pi(u) \\qquad\\qquad\\qquad\\qquad\\qquad \\blacksquare\n\\end{align}\n\\]\nThe sequence defined by (29) essentially splits the single Bayesian inverse problem (1) into a sequence of \\(K\\) inverse problems. In particular, the update \\(\\pi_k \\mapsto \\pi_{k+1}\\) implies solving the inverse problem \\[\n\\begin{align}\ny|u &\\sim \\tilde{p}(y|u) \\tag{31} \\newline\nu &\\sim \\pi_k(u),\n\\end{align}\n\\] with the tempered likelihood \\[\n\\tilde{p}(y|u) \\propto \\exp\\left(-\\frac{1}{K}\\Phi(u)\\right). \\tag{32}\n\\] Each update thus encodes the action of conditioning on the data \\(y\\), but under the modified likelihood \\(\\tilde{p}(y|u)\\) which “spreads out” the information in the data over the \\(K\\) time steps. If we were to consider using the true likelihood \\(p(y|u)\\) in each time step, this would artificially inflate the information content in \\(y\\), as if we had \\(K\\) independent data vectors instead of just one.\n\n7.1.1 Gaussian Special Case\nSuppose the likelihood is Gaussian \\(\\mathcal{N}(y|\\mathcal{G}(u), \\Sigma)\\), with associated potential \\(\\Phi(u) = \\frac{1}{2}\\lVert y - \\mathcal{G}(u)\\rVert^2_{\\Sigma}\\). The tempered likelihood in this case is \\[\n\\exp\\left(-\\frac{1}{K}\\Phi(u)\\right)\n= \\exp\\left(-\\frac{1}{2K}\\lVert y - \\mathcal{G}(u)\\rVert^2_{\\Sigma}\\right)\n\\propto \\mathcal{N}(y|\\mathcal{G}(u), K\\Sigma). \\tag{33}\n\\] The modified likelihood remains Gaussian, and is simply the original likelihood with the variance inflated by a factor of \\(K\\). This matches the intuition from above; the variance is increased to account for the fact that we are conditioning on the same data vector \\(K\\) times.\nIf, in addition to the Gaussian likelihood, the prior \\(\\pi_0\\) is Gaussian and the map \\(\\mathcal{G}\\) is linear then the final posterior \\(\\pi\\) and each intermediate distribution \\(\\pi_k\\) will also be Gaussian. In this case, the recursion (29) defines a sequence of \\(K\\) linear Gaussian Bayesian inverse problems."
  },
  {
    "objectID": "blog/posts/eki_1/eki_introduction.html#introducing-artificial-dynamics",
    "href": "blog/posts/eki_1/eki_introduction.html#introducing-artificial-dynamics",
    "title": "Ensemble Kalman Methods for Solving Inverse Problems: An Introduction",
    "section": "7.2 Introducing Artificial Dynamics",
    "text": "7.2 Introducing Artificial Dynamics\nThe previous section considered a discrete process on the level of densities; i.e., the dynamics (29) describe the evolution of \\(\\pi_k\\). Our goal is now to design an artificial dynamical system that treats \\(u\\) as the state variable, such that \\(\\pi_k\\) describes the filtering distribution of the state at iteration \\(k\\). In theory, we can then draw samples from \\(\\pi\\) by applying standard filtering algorithms to this artificial dynamical system.\nThere are many approaches we could take here, but let us start with the simplest. We know that the update \\(\\pi_k \\mapsto \\pi_{k+1}\\) should encode the action of conditioning on \\(y\\) with respect to the tempered likelihood. Thus, let’s consider the following dynamics and observation model: \\[\n\\begin{align}\nu_{k+1} &= u_k \\tag{34} \\newline\ny_{k+1} &= \\mathcal{G}(u_{k+1}) + \\epsilon_{k+1}, &&\\epsilon_{k+1} \\sim \\mathcal{N}(0, K\\Sigma) \\tag{35} \\newline\nu_0 &\\sim \\pi_0 \\tag{36}\n\\end{align}\n\\] The lines (34) and (36) define our artificial dynamics in \\(u\\), with the former providing the evolution equation and the latter the initial condition. These dynamics are rather uninteresting; the evolution operator is the identity, meaning that the state remains fixed at its initial condition. All of the interesting bits here come into play in the observation model (34). We observe that, by construction, the filtering distribution of this dynamical system at time step \\(k\\) is given by \\(\\pi_k\\): \\[\n\\pi_k(u_k) = p(u_k | y_1 = y, \\dots, y_k = y). \\tag{37}\n\\] To be clear, we emphasize that the quantities \\(y_1, \\dots, y_K\\) in the observation model (35) are random variables, and \\(y_k = y\\) indicates that the condition that the random variable \\(y_k\\) is equal to the fixed data realizaton \\(y\\).\n\n7.2.1 Extending the State Space\nWe now provide an alternative, but equivalent, formulation that gives another useful perspective. Observe that the observation model (34) can be written as \\[\ny_k\n= \\mathcal{G}(u_k) + \\epsilon_{k}\n= \\begin{bmatrix} 0 & I \\end{bmatrix} \\begin{bmatrix} u_k \\\\ \\mathcal{G}(u_k) \\end{bmatrix} + \\epsilon_{k}\n=: Hv_k + \\epsilon_k, \\tag{38}\n\\] where we have defined \\[\\begin{align}\nH &:= \\begin{bmatrix} 0 & I \\end{bmatrix} \\in \\mathbb{R}^{p \\times (d+p)},\n&&v_k := \\begin{bmatrix} u_k \\newline \\mathcal{G}(u_k) \\end{bmatrix} \\in \\mathbb{R}^{d+p}. \\tag{38}\n\\end{align}\\] We will now adjust the dynamical system (33) to describe the dynamics with respect to the state vector \\(v_k\\): \\[\n\\begin{align}\nv_{k+1} &= v_k \\tag{40} \\newline\ny_{k+1} &= Hv_{k+1} + \\epsilon_{k+1}, &&\\epsilon_{k+1} \\sim \\mathcal{N}(0, K\\Sigma) \\tag{41} \\newline\nu_0 &\\sim \\pi_0. \\tag{42}\n\\end{align}\n\\] We continue to write the initial condition (42) with respect to \\(u\\) but note that the distribution on \\(u_0\\) induces an initial distribution for \\(v_0\\). Why extend the state space in this way? For one, the observation operator in (41) is now linear. Linearity of the observation operator is a common assumption in the data assimilation literature, so satisfying this assumption allows us more flexibility in choosing a filtering algorithm. The main reason I opt to include the state space extension here is that this is the approach taken in Iglesias et al (2012), which is the first paper to systematically propose and analyze the application of the EnKF to inverse problems. In effect, if you have defined the EnKF with respect to a linear observation operator (as is commonly done), then the extended state space formulation allows you to extend the algorithm to the nonlinear case. As we will see, what you ultimately get is identical to the joint Gaussian approximation viewpoint used in deriving (27).\nThis extended state space formulation still gives rise to the sequence \\(\\pi_0, \\dots, \\pi_K\\) as before. However, the distribution \\(\\pi_k\\) is now a marginal of filtering distribution for \\(v_k\\) (the marginal corresponding to the first \\(d\\) entries of \\(v_k\\))."
  },
  {
    "objectID": "blog/posts/eki_1/eki_introduction.html#applying-filtering-algorithms",
    "href": "blog/posts/eki_1/eki_introduction.html#applying-filtering-algorithms",
    "title": "Ensemble Kalman Methods for Solving Inverse Problems: An Introduction",
    "section": "7.3 Applying Filtering Algorithms",
    "text": "7.3 Applying Filtering Algorithms\nWe have now considered a couple options to formulate the Bayesian inverse problem (1) as a discrete dynamical system, such that the filtering distributions of the artificial system are given by \\(\\pi_1, \\dots, \\pi_K\\), as defined in (29). The distribution at the final time step satisfies \\(\\pi_K = \\pi\\). This points to a possible algorithmic approach to posterior inference. If we can propagate an ensemble of particles such that they are distributed according to \\(\\pi_k\\) at time step \\(k\\), then the ensemble will represent the posterior at time step \\(K\\). Particle filtering methods might be considered to exactly implement this Monte Carlo scheme. However, in this post we consider approximate methods rooted in Kalman methodology. Specifically, let’s consider applying the EnKF to the model given in (40). We could also consider (34) and get the same result, but I’ll follow Iglesias et al (2012) in using the extended state space formulation to generalize the linear observation operator EnKF to nonlinear operators. A direct application of the EnKF to the system (40) gives the following recursive algorithm.\n\n\nPosterior Approximation via EnKF Update in Finite Time.   Time k=0:  Generate the initial ensemble \\(\\{v_0^{(j)}\\}_{j=1}^{J}\\), where \\[\n  \\begin{align}\n  &v^{(j)}_0 := \\begin{bmatrix} u^{(j)}_0, \\mathcal{G}(u^{(j)}_0) \\end{bmatrix}^\\top,\n  &&u^{(j)}_0 \\overset{iid}{\\sim} \\pi_0. \\tag{43}\n  \\end{align}\n  \\]  Time k+1:  1. Perform the forecast step: \\[\n  \\hat{v}^{(j)}_{k+1} := v^{(j)}_{k}, \\qquad j = 1, \\dots, J \\tag{44}\n  \\]  2. Compute the sample estimates: \\[\n  \\begin{align}\n  \\overline{v}_{k+1} &:= \\frac{1}{J} \\sum_{j=1}^{J} \\hat{v}^{(j)}_{k+1} \\tag{45} \\newline\n  \\hat{C}^v_{k+1} &:= \\frac{1}{J-1} \\sum_{j=1}^{J} (\\hat{v}^{(j)}_{k+1} - \\overline{v}_{k+1}) (\\hat{v}^{(j)}_{k+1} - \\overline{v}_{k+1})^\\top.\n  \\end{align}\n  \\] 3. Perform the analysis step: \\[\n  \\begin{align}\n  v^{(j)}_{k+1} &:=\n  \\hat{v}^{(j)}_{k+1} + \\hat{C}^{v}_{k+1}H^\\top[H\\hat{C}^{v}_{k+1}H^\\top + K\\Sigma]^{-1}(y-y^{(j)}),\n  &&y^{(j)} \\sim \\mathcal{N}(H\\hat{v}_{k+1}^{(j)}, K\\Sigma). \\tag{46}\n  \\end{align}\n  \\]"
  },
  {
    "objectID": "blog/posts/pseudo-marginal-mcmc/pseudo-marginal-mcmc.html",
    "href": "blog/posts/pseudo-marginal-mcmc/pseudo-marginal-mcmc.html",
    "title": "Pseudo-Marginal MCMC",
    "section": "",
    "text": "Pseudo-marginal Markov chain Monte Carlo (MCMC) is a variant of the Metropolis-Hastings algorithm that works without the ability to evaluate the unnormalized target density, so long as an unbiased sample of this density can be obtained for any input. In this post, we motivate the algorithm by considering a problem of Bayesian inference where the likelihood function is intractable. We then take a step back to understand why the algorithm works, and discuss the method from a more generic and rigorous viewpoint.\n\n1 Pseudo-Marginal MCMC for Bayesian Inference\nWe start by considering a standard problem of Bayesian inference for a parameter of interest \\(u \\in \\mathcal{U}\\). Given a prior density \\(\\pi_0(u)\\) and likelihood function \\(L(u)\\), the unnormalized posterior density is then obtained as the product of these two quantities: \\[\n\\pi(u) := \\pi_0(u) L(u). \\tag{1}\n\\] With the ability to evaluate this unnormalized density, MCMC algorithms can be applied to obtain samples from the posterior distribution. However, suppose we face a situation where \\(L(u)\\) is intractable in the sense that it does not admit an analytic expression that can be computed for any \\(u\\). Suppose, though, that we can draw an unbiased sample of the quantity \\(L(u)\\) for any input \\(u\\); that is, \\[\n\\begin{align}\n&\\ell \\sim P(u, \\cdot), &&\\mathbb{E}[\\ell] = L(u), \\tag{2}\n\\end{align}\n\\] where \\(P(u,\\cdot)\\) is a probability measure on the sample space \\([0, \\infty)\\) for each \\(u \\in \\mathcal{U}\\) (formally, we can think of \\(P\\) as a Markov kernel). It turns out that this is sufficient to define an MCMC algorithm with target distribution equal to \\(u\\)’s posterior. The algorithm that accomplishes this is referred as pseudo-marginal MCMC. A single step of this algorithm is detailed below.\n\n\nPseudo-Marginal MCMC. Let \\(u\\) be the current state of the algorithm, with \\(\\ell \\sim P(u,\\cdot)\\) the associated unbiased likelihood sample. Let \\(Q\\) denote the proposal kernel. The next state is then determined as follows. \n\nPropose a new state: \\[\n  \\tilde{u} \\sim Q(u, \\cdot) \\tag{3}\n  \\]\nDraw an unbiased likelihood sample at the proposed state: \\[\n  \\tilde{\\ell} \\sim P(\\tilde{u}, \\cdot) \\tag{4}\n  \\]\nWith probability \\[\n  \\alpha(u,\\ell; \\tilde{u},\\tilde{\\ell}) := \\min\\left(1, \\frac{\\pi_0(\\tilde{u})\\tilde{\\ell}q(\\tilde{u},u)}{\\pi_0(u)\\ell q(u,\\tilde{u})} \\right), \\tag{5}\n  \\] set the new state to \\(\\tilde{u}\\). Else set it to the current state \\(u\\).\n\n\n\nNotice that the acceptance probability (5) is the typical Metropolis-Hastings acceptance probability but with the unbiased likelihood samples \\(\\ell\\) and \\(\\tilde{\\ell}\\) inserted in place of \\(L(u)\\) and \\(L(\\tilde{u})\\), respectively. The claim is that this algorithm defines a Markov chain with invariant distribution \\(\\pi\\). To see why this is true, the trick is to view the above algorithm as a Metropolis-Hastings scheme operating on the extended state vector \\((u, \\ell)\\). In showing this, I will assume \\(P(u,\\cdot)\\) and \\(Q(u,\\cdot)\\) admit densities \\(p(u,\\cdot)\\) and \\(q(u,\\cdot)\\) with respect to the same base measure for which \\(\\pi\\) is a density (typically, the Lebesgue or counting measure). Now, to view the above algorithm with respect to the extended state space, start by noticing that (3) and (4) can be interpreted as a joint proposal \\[\n(\\tilde{u},\\tilde{\\ell}) \\sim \\overline{Q}(u,\\ell; \\cdot, \\cdot), \\tag{6}\n\\] with \\(\\overline{Q}\\) a Markov kernel on the product space \\(\\mathcal{U} \\times [0,\\infty)\\) with density \\[\n\\overline{q}(u,\\ell; \\tilde{u},\\tilde{\\ell}) := q(u,\\tilde{u})p(\\tilde{u},\\tilde{\\ell}). \\tag{7}\n\\] Notice that \\(\\overline{Q}(u,\\ell; \\cdot, \\cdot)\\) is independent of \\(\\ell\\). It now remains to write the acceptance probability (5) in a form that can be interpreted with respect to the extended state space. To this end, consider \\[\n\\begin{align}\n\\frac{\\pi_0(\\tilde{u})\\tilde{\\ell}q(\\tilde{u},u)}{\\pi_0(u)\\ell q(u,\\tilde{u})}\n&= \\frac{\\pi_0(\\tilde{u})\\tilde{\\ell}}{\\pi_0(u)\\ell}\n\\cdot \\frac{q(\\tilde{u},u)p(u,\\ell)}{q(u,\\tilde{u})p(\\tilde{u},\\tilde{\\ell})}\n\\cdot \\frac{p(\\tilde{u},\\tilde{\\ell})}{p(u,\\ell)} \\newline\n&= \\frac{\\pi_0(\\tilde{u})\\tilde{\\ell}p(\\tilde{u},\\tilde{\\ell})}{\\pi_0(u)\\ell p(u,\\ell)}\n\\cdot \\frac{\\overline{q}(\\tilde{u},\\tilde{\\ell};u,\\ell)}{\\overline{q}(u,\\ell;\\tilde{u},\\tilde{\\ell})}. \\tag{8}\n\\end{align}\n\\] The second term is the proposal density ratio with respect to extended proposal \\(\\overline{q}\\). Thus, the function appearing in the numerator and denominator of the first term must be the (unnormalized) density targeted by this Metropolis-Hastings scheme. In other words, the invariant distribution implied by the above algorithm has unnormalized density \\[\n\\overline{\\pi}(u,\\ell) := \\pi_0(u)p(u,\\ell)\\ell. \\tag{9}\n\\] Notice that \\(\\pi_0(u)\\ell\\) is the unnormalized density (1) with the sample \\(\\ell\\) inserted in place of \\(L(u)\\). This is multiplied by the weight \\(p(u,\\ell)\\), which encodes the probability of sampling \\(\\ell\\) at the input \\(u\\). Our proof of the algorithm’s correctness is concluded by noting that \\(\\overline{\\pi}\\) admits \\(\\pi\\) as a marginal distribution; indeed, \\[\n\\begin{align}\n\\int \\overline{\\pi}(u,\\ell)d\\ell\n&= \\int \\pi_0(u)p(u,\\ell)\\ell d\\ell\n= \\pi_0(u) \\int \\ell \\cdot p(u,\\ell) d\\ell\n= \\pi_0(u) \\mathbb{E}[\\ell|u]\n= \\pi_0(u) L(u), \\tag{10}\n\\end{align}\n\\] following from the unbiasedness of the likelihood sample. This means that, in theory, we can run the above algorithm to obtain joint samples \\((u,\\ell) \\sim \\overline{\\pi}\\), and then simply extract the \\(u\\) portion of these pairs to obtain the desired draws \\(u \\sim \\pi\\). One last thing to note is that we don’t actually need to be able to evaluate the density \\(p(u,\\ell)\\) appearing in (8); we see in the acceptance probability (5) that we need only be able to sample from \\(P(u,\\cdot)\\). As usual, we need to be able to evaluate the density \\(q(u,\\tilde{u})\\).\n\n\n2 A More Generic and Formal Perspective\nThe above idea of course extends beyond the Bayesian example. In this section, we discuss the pseudo-marginal algorithm from a more generic perspective, and fill in some of the measure-theoretic details. Let’s assume \\(\\Pi\\) is some generic target distribution on a measurable space \\((\\mathcal{U}, \\mathcal{B}(\\mathcal{U}))\\). We write \\(\\mathcal{B}(\\mathcal{U})\\) to denote the Borel \\(\\sigma\\)-algebra; that is, the \\(\\sigma\\)-algebra generated by the open sets of \\(\\mathcal{U}\\). We assume \\(\\Pi\\) admits a density (i.e., Radon-Nikodym derivative) \\(\\pi\\) with respect to some reference measure \\(\\nu\\). The density \\(\\pi\\) need not be normalized. All densities considered throughout this section will be with respect to the same reference measure \\(\\nu\\). As before, we consider \\(\\pi(u)\\) intractable, but assume we can draw samples from an unbiased estimator. We could define \\(P(u,\\cdot)\\) as before such that samples drawn from \\(P(u,\\cdot)\\) are unbiased with respect to \\(\\pi(u)\\). However, note that this is equivalent to considering samples \\(w \\sim P(u,\\cdot)\\) with expectation one, such that \\(w \\cdot \\pi(u)\\) is unbiased for \\(\\pi(u)\\). This seems to be a roundabout way to go around this, but for the purposes of analysis it turns out to be convenient. This is the definition used in some of the “noisy MCMC” literature (see, e.g., Medina-Aguayo et al, 2018). Thus, let’s go with this definition and define the Markov kernel \\(P: \\mathcal{U} \\to [0,1]\\) such that (1) \\(P(u,\\cdot)\\) is a probability measure on \\((\\mathcal{W},\\mathcal{B}(\\mathcal{W}))\\) for each \\(u \\in \\mathcal{U}\\), where \\(\\mathcal{W} \\subseteq [0,\\infty)\\); and (2) \\(P\\) produces weights with unit expectation: \\[\n\\begin{align}\n&w \\sim P(u,\\cdot), &&\\mathbb{E}_{P_u}[w] = 1. \\tag{11}\n\\end{align}\n\\] We use \\(P_u\\) as shorthand for \\(P(u,\\cdot)\\) in the subscript. We again emphasize that the sample \\(w\\) from (11) implies that \\(w\\pi(u)\\) is an unbiased estimate of \\(\\pi(u)\\). The pseudo-marginal algorithm proceeds exactly as before. We state it again below to emphasize the new notation.\n\n\nPseudo-Marginal MCMC.  1. Propose a new state: \\[\n  \\tilde{u} \\sim Q(u, \\cdot) \\tag{12}\n  \\] 2. Draw an unbiased weight sample at the proposed state: \\[\n  \\tilde{w} \\sim P(\\tilde{u}, \\cdot) \\tag{13}\n  \\] 3. With probability \\[\n  \\alpha(u,w; \\tilde{u},\\tilde{w}) := \\min\\left(1, \\frac{\\pi(\\tilde{u})\\tilde{w}q(\\tilde{u},u)}{\\pi(u)w q(u,\\tilde{u})} \\right), \\tag{14}\n  \\] set the new state to \\(\\tilde{u}\\). Else set it to the current state \\(u\\).\n\n\nOf course, we can’t evaluate \\(\\pi(u)\\) in (14), but stating the algorithm this way is useful to study its properties. In practice, we can think of drawing a sample to directly approximate \\(\\pi(u)\\). Similar to before, we can think about this algorithm as targeting an invariant distribution on the product space \\((\\mathcal{U} \\times \\mathcal{W}, \\mathcal{B}(\\mathcal{U}) \\times \\mathcal{B}(\\mathcal{W}))\\). The steps (12) and (13) represent a draw from the proposal kernel \\(\\overline{Q}: \\mathcal{U} \\times \\mathcal{W} \\to [0,1]\\) defined by \\[\n\\overline{Q}(u,w; U,W) := \\int_{U} P(\\tilde{u},W)Q(u,d\\tilde{u}), \\tag{15}\n\\] for \\(U \\in \\mathcal{B}(\\mathcal{U})\\) and \\(W \\in \\mathcal{B}(\\mathcal{W})\\).\n\n\n3 References\n\nThe pseudo-marginal approach for efficient Monte Carlo computations (Andrieu and Roberts, 2009)\nConvergence properties of pseudo-marginal Markov chain Monte Carlo algorithms (Andrieu and Vihola, 2015)\nStability of Noisy Metropolis-Hastings (Medina-Aguayo et al, 2018)"
  },
  {
    "objectID": "blog/posts/doubly-intractable-mcmc/doubly-intractable.html",
    "href": "blog/posts/doubly-intractable-mcmc/doubly-intractable.html",
    "title": "Doubly Intractable MCMC",
    "section": "",
    "text": "A typical Bayesian model consists of a joint probability distribution over a parameter \\(u\\) and data \\(y\\) of the form \\[\np(u,y) = \\pi_0(u)L(u;y)\n\\tag{1}\\] where \\(\\pi_0(u)\\) is the prior density on \\(u\\) and \\(L(u;y) = p(y \\mid u)\\) the likelihood. The posterior distribution is then given by \\[\n\\pi(u) := p(u \\mid y) = \\frac{1}{Z}\\pi_0(u)L(u;y)\n\\tag{2}\\] where \\(Z\\) is a normalizing constant (independent of \\(u\\)) that we are not typically able to compute. Fortunately, common algorithms for posterior inference such as Markov chain Monte Carlo (MCMC) only require pointwise evaluations of the unnormalized posterior density \\(\\pi_0(u)L(u;y)\\).\nIn this post, we consider a class of Bayesian models that adds an additional difficulty, rendering these standard inference algorithms infeasible. In particular, we assume a likelihood of the form \\[\nL(u;y) = \\frac{f(y; u)}{C(u)},\n\\tag{3}\\] such that we can evaluate \\(f(y;u)\\) but not the normalizing function \\(C(u)\\). The posterior density in this setting becomes \\[\n\\pi(u) = \\frac{1}{ZC(u)}\\pi_0(u)f(y;u).\n\\tag{4}\\] Distributions of the form Equation 4 are known as doubly intractable owing to the two quantities we are unable to compute: \\(Z\\) and \\(C(u)\\). While the former does not pose a problem for typical inference algorithms, the presence of the latter is problematic."
  },
  {
    "objectID": "blog/posts/doubly-intractable-mcmc/doubly-intractable.html#extending-the-state-space",
    "href": "blog/posts/doubly-intractable-mcmc/doubly-intractable.html#extending-the-state-space",
    "title": "Doubly Intractable MCMC",
    "section": "2.1 Extending the State Space",
    "text": "2.1 Extending the State Space\nThe main idea is to extend the joint probability space over \\((u,y)\\) in Equation 1 to a joint model over \\((u,x,y)\\) for some auxiliary variable \\(x\\). The auxiliary variable will be defined on the same space as \\(y\\), so we might think of it as some sort of “pseudo data”. Once we define the conditional \\(p(x \\mid u, y)\\), then we obtain the extended model \\[\np(u, x, y)\n:= p(x \\mid u, y)p(y \\mid u)p(u)\n= p(x \\mid u, y)f(y; u)\\pi_0(u) / C(u).\n\\tag{7}\\] Notice that \\(\\pi(u) = p(u \\mid y)\\) is a marginal distribution of \\(p(u,x \\mid y)\\). Therefore, if we can draw samples \\((u,x) \\sim p(u,x \\mid y)\\) then the \\(u\\)-component of these samples will have the desired distribution \\(\\pi\\).\nWe now consider a Metropolis-Hastings algorithm targeting the extended posterior \\(p(u,x \\mid y)\\). Letting \\(q(\\tilde{u},\\tilde{x} \\mid u,x)\\) denote a proposal distribution on the extended state space, the acceptance ratio assumes the form \\[\nr(\\tilde{u},\\tilde{x} \\mid u,x)\n= \\frac{\\pi_0(\\tilde{u})f(y; \\tilde{u}) q(u,x \\mid \\tilde{u},\\tilde{x})}{\\pi_0(u) f(y; u)q(\\tilde{u},\\tilde{x} \\mid u,x)} \\cdot\n\\frac{C(u)}{C(\\tilde{u})} \\cdot \\frac{p(\\tilde{x} \\mid \\tilde{u},y)}{p(x \\mid u,y)}.\n\\tag{8}\\] At present, the ratio still depends on \\(C(u)/C(\\tilde{u})\\) and thus remains intractable."
  },
  {
    "objectID": "blog/posts/doubly-intractable-mcmc/doubly-intractable.html#a-clever-choice-of-proposal",
    "href": "blog/posts/doubly-intractable-mcmc/doubly-intractable.html#a-clever-choice-of-proposal",
    "title": "Doubly Intractable MCMC",
    "section": "2.2 A clever choice of proposal",
    "text": "2.2 A clever choice of proposal\nIt would be nice to be able to choose \\(p(x \\mid u,y)\\) such that the dependence of Equation 7 on \\(C(u)\\) is eliminated. However, as pointed out by Murray, Ghahramani, and MacKay (2006), no such choice of \\(p(x \\mid u,y)\\) is known. Instead, Møller et al. (2006) show that the proposal \\(q(\\tilde{u}, \\tilde{x} \\mid u, x)\\) can be chosen to eliminate the normalizing function from the acceptance ratio \\(r(\\tilde{u},\\tilde{x} \\mid u,x)\\). We consider a proposal of the form \\[\nq(\\tilde{u}, \\tilde{x} \\mid u, x) := q(\\tilde{u} \\mid u) q(\\tilde{x} \\mid \\tilde{u}),\n\\tag{9}\\] implying a standard proposal for \\(u\\), followed by a proposal of the auxiliary variable that depends on \\(\\tilde{u}\\) but not \\(x\\). Given this setup, the necessary choice of \\(q(\\tilde{x} \\mid \\tilde{u})\\) to eliminate dependence on the normalizing function is \\[\nq(\\tilde{x} \\mid \\tilde{u}) := f(\\tilde{x};\\tilde{u}) / C(\\tilde{u}).\n\\tag{10}\\] Indeed, plugging Equation 10 into Equation 11 yields \\[\nr(\\tilde{u},\\tilde{x} \\mid u,x)\n= \\frac{\\pi_0(\\tilde{u})f(y; \\tilde{u}) q(u \\mid \\tilde{u})}{\\pi_0(u) f(y; u)q(\\tilde{u} \\mid u)} \\cdot\n\\frac{p(\\tilde{x} \\mid \\tilde{u},y)/f(\\tilde{x};\\tilde{u})}{p(x \\mid u,y)/f(x;u)}.\n\\tag{11}\\]\n\n\n\n\n\n\nDerivation\n\n\n\n\n\n\\[\n\\begin{align}\nr(\\tilde{u},\\tilde{x} \\mid u,x)\n&= \\frac{\\pi_0(\\tilde{u})f(y; \\tilde{u}) q(u \\mid \\tilde{u}) q(x \\mid u)}{\\pi_0(u) f(y; u)q(\\tilde{u} \\mid u) q(\\tilde{x} \\mid \\tilde{u})} \\cdot\n\\frac{C(u)}{C(\\tilde{u})} \\cdot \\frac{p(\\tilde{x} \\mid \\tilde{u},y)}{p(x \\mid u,y)} \\\\\n&= \\frac{\\pi_0(\\tilde{u})f(y; \\tilde{u}) q(u \\mid \\tilde{u}) f(x;u)/ C(u)}{\\pi_0(u) f(y; u)q(\\tilde{u} \\mid u) f(\\tilde{x};\\tilde{u})/ C(\\tilde{u})} \\cdot\n\\frac{C(u)}{C(\\tilde{u})} \\cdot \\frac{p(\\tilde{x} \\mid \\tilde{u},y)}{p(x \\mid u,y)} \\\\\n&= \\frac{\\pi_0(\\tilde{u})f(y; \\tilde{u}) q(u \\mid \\tilde{u}) f(x;u)}{\\pi_0(u) f(y; u)q(\\tilde{u} \\mid u) f(\\tilde{x};\\tilde{u})} \\cdot\n\\frac{p(\\tilde{x} \\mid \\tilde{u},y)}{p(x \\mid u,y)}\n\\end{align}\n\\]\n\n\n\nThe ratio in Equation 11 no longer involves the intractable terms! This Metropolis-Hastings scheme therefore admits \\(p(u,x \\mid y)\\) as a stationary distribution without requiring the ability to evaluate \\(C(u)\\). The algorithm is thus “correct”, but its efficiency will depend heavily on the choice of the auxiliary distribution \\(p(x \\mid u,y)\\), which is a free parameter of this method."
  },
  {
    "objectID": "blog/posts/doubly-intractable-mcmc/doubly-intractable.html#importance-sampling-interpretation",
    "href": "blog/posts/doubly-intractable-mcmc/doubly-intractable.html#importance-sampling-interpretation",
    "title": "Doubly Intractable MCMC",
    "section": "2.3 Importance Sampling Interpretation",
    "text": "2.3 Importance Sampling Interpretation\nWe now aim to build some intuition as to what the algorithm is doing, to help inform the choice of \\(p(x \\mid u,y)\\). The situation we find ourselves in is somewhat backwards when compared to the typical design of Metropolis-Hastings algorithms. In particular, the proposal \\(q(\\tilde{x} \\mid \\tilde{u})\\) (typically a free parameter) has been prescribed, and we instead need to choose the distribution \\(p(x \\mid u,y)\\) (typically prescribed). Ideally, the proposal will look something like the target distribution. This intuition would lead us to set \\(p(x \\mid u,y) := f(x;u) / C(u)\\). This is of course infeasible as it would reintroduce the normalizing function, but it does give a baseline goal to shoot for."
  },
  {
    "objectID": "blog/posts/doubly-intractable-mcmc/doubly-intractable.html#choice-of-auxiliary-distribution",
    "href": "blog/posts/doubly-intractable-mcmc/doubly-intractable.html#choice-of-auxiliary-distribution",
    "title": "Doubly Intractable MCMC",
    "section": "2.3 Choice of Auxiliary Distribution",
    "text": "2.3 Choice of Auxiliary Distribution\nWe now aim to build some intuition as to what the algorithm is doing, which will help inform the choice of \\(p(x \\mid u,y)\\). The situation we find ourselves in is somewhat backwards when compared to the typical design of Metropolis-Hastings algorithms. In particular, the proposal \\(q(x \\mid u)\\) (typically a free parameter) has been prescribed, and we instead need to choose the distribution \\(p(x \\mid u,y)\\) (typically prescribed). Ideally, the proposal will look something like the target distribution. This intuition would lead us to set \\(p(x \\mid u,y) := f(x;u) / C(u)\\). This is of course infeasible as it would reintroduce the normalizing function, but it does give a baseline goal to shoot for.\nTo further understand the workings of this algorithm, notice that the first term in Equation 11 is equal to the intractable ratio in Equation 6 except that it is missing \\(C(u)/C(\\tilde{u})\\). The second term in Equation 11 might therefore be viewed as providing an estimate of \\(C(u)/C(\\tilde{u})\\). Indeed, consider the random ratio \\[\n\\begin{align}\n&\\frac{p(x \\mid u,y)}{f(x;u)}, &&x \\sim f(x;u)/C(u)\n\\end{align}\n\\tag{12}\\] which has expectation \\[\n\\mathbb{E}\\left[\\frac{p(x \\mid u,y)}{f(x;u)}\\right]\n= \\int \\frac{p(x \\mid u,y)}{f(x;u)} \\frac{f(x;u)}{C(u)} dx\n= C(u)^{-1} \\int p(x \\mid u,y) dx = C(u)^{-1}.\n\\] Therefore, the ratio in Equation 12 is a single-sample importance sampling estimate of \\(C(u)^{-1}\\). The second term in Equation 11 can thus be viewed as \\[\n\\frac{p(\\tilde{x} \\mid \\tilde{u},y)/f(\\tilde{x};\\tilde{u})}{p(x \\mid u,y)/f(x;u)}\n\\approx \\frac{C(\\tilde{u})^{-1}}{C(u)^{-1}}\n= \\frac{C(u)}{C(\\tilde{u})},\n\\] a biased estimate of the ratio \\(C(u)/C(\\tilde{u})\\) derived from the two importance sampling estimates. It is interesting that the algorithm is correct despite the use of this plug-in biased estimate. This importance sampling viewpoint further strengthens our intuition that \\(p(x \\mid u,y)\\) should be chosen to approximate \\(f(x;u)/C(u)\\).\nMøller et al. (2006) give two options for choosing \\(p(x \\mid u,y)\\). The simpler of the two is to choose \\[\n\\begin{align}\n&p(x \\mid u,y) := f(x;\\hat{u})/C(\\hat{u}), &&\\hat{u} = \\hat{u}(y)\n\\end{align}\n\\tag{13}\\] where \\(\\hat{u}\\) is a fixed estimate of \\(u\\) derived from the data \\(y\\). Recall that \\(f(x;u)/C(u)\\) describes the data-generating distribution as a function of the parameter \\(u\\). Fixing a single \\(u\\) will therefore be a reasonable approximation if this distribution is not strongly dependent on \\(u\\). Alternatively, this may also work well if the posterior support is concentrated around \\(\\hat{u}\\), so that a reasonable approximation is only required in this neighborhood. The second approach is to construct a more sophisticated \\(u\\)-dependent approximation of \\(f(x;u)/C(u)\\). We will not consider this option here."
  },
  {
    "objectID": "blog/posts/annealed-IS/annealed-IS.html",
    "href": "blog/posts/annealed-IS/annealed-IS.html",
    "title": "Annealed Importance Sampling",
    "section": "",
    "text": "In this post we walk through the annealed importance sampling (AIS) scheme introduced in the classic paper Neal (1998). The algorithm seeks to improve upon standard IS by incorporating Markov chain Monte Carlo (MCMC) updates, and is widely used for estimating intractable expectations and normalizing constants. While I highly recommend reading Neal’s original paper, my presentation here leans towards more recent perspectives. My notation is most similar to Doucet et al. (2022)."
  },
  {
    "objectID": "blog/posts/annealed-IS/annealed-IS.html#footnotes",
    "href": "blog/posts/annealed-IS/annealed-IS.html#footnotes",
    "title": "Annealed Importance Sampling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThroughout this post we will assume all distributions and Markov kernels admit densities with respect to some common dominating measure (typically the Lebesgue measure).↩︎\nWe’ll assume we can evaluate \\(\\pi_0(x)\\) here, but as before we need only be able to evaluate a function that is proportional to it.↩︎"
  },
  {
    "objectID": "blog/posts/annealed-IS/annealed-IS.html#bridging-distributions",
    "href": "blog/posts/annealed-IS/annealed-IS.html#bridging-distributions",
    "title": "Annealed Importance Sampling",
    "section": "1.1 Bridging Distributions",
    "text": "1.1 Bridging Distributions\nThe idea underlying annealed IS is to improve an initial proposal via a sequence of updates that move the proposal closer to \\(\\pi\\). An initial “simple” distribution is slowly annealed towards the intractable target distribution. Let \\((\\pi_k)_{k=0}^{K}\\) denote the sequence of intermediate distributions \\(\\pi_k(x) = f_k(x)/Z_k\\). In this post, we will consider \\(\\pi_0\\) to be a simple distribution from which independent samples can be drawn. The final distribution \\(\\pi_K = \\pi\\) is the target, meaning the intermediate distributions bridge from the simple distribution to \\(\\pi\\). The canonical choice of bridging distributions is the geometric path, defined by the geometric averages \\[\n\\begin{align}\n&f_k(x) := f_0(x)^{1-\\beta_k} f(x)^{\\beta_k},\n&&0 = \\beta_0 &lt; \\beta_1 &lt; \\cdots &lt; \\beta_K = 1.\n\\end{align}\n\\tag{2}\\]\n\n\n\n\n\n\nRemark\n\n\n\nIf \\(\\pi_0\\) is a (potentially improper) uniform distribution, then the geometric path simplifies to \\(f_k(x) := f(x)^{\\beta_k}\\). This is a common path used in simulated annealing algorithms for optimizing \\(f\\), motivating the word “annealed” in annealed importance sampling.\n\n\nAs we will see below, we require the bridging distributions to satisfy \\[\nf_k(x) = 0 \\implies f_{k+1}(x) = 0.\n\\tag{3}\\] In other words, the supports of the distributions cannot be growing as \\(\\pi_0\\) evolves to \\(\\pi_K\\). This assumption is often denoted by \\(\\pi_K \\ll \\pi_{K-1} \\ll \\cdots \\ll \\pi_0\\), where \\(\\pi_{k+1} \\ll \\pi_{k}\\) is defined by Equation 3 and read “\\(\\pi_{k+1}\\) is absolutely continuous with respect to \\(\\pi_{k}\\)”.\n\n\n\n\n\n\nRemark\n\n\n\nIn some treatments (including Neal’s original paper), the sequence of distributions is defined in the reverse order, such that \\(\\pi_K\\) is the simple distribution and \\(\\pi_0 = \\pi\\). This is mostly a matter of taste. As we will see, the reversed sequence will still become relevant here."
  },
  {
    "objectID": "blog/posts/annealed-IS/annealed-IS.html#modified-target-in-extended-state-space",
    "href": "blog/posts/annealed-IS/annealed-IS.html#modified-target-in-extended-state-space",
    "title": "Annealed Importance Sampling",
    "section": "2.2 Modified Target in Extended State Space",
    "text": "2.2 Modified Target in Extended State Space\nOf course, we cannot sample from the approximations \\(\\pi_k\\) directly; if we could, then we could simply sample from \\(\\pi\\) directly. We will instead consider a sequence of Markov steps that approximately track this sequence of distributions. The importance weights will ultimately correct for the fact that we are not exactly tracking the distributions. With the idea of Markov updates in mind, consider defining a joint distribution of the form \\[\n\\rho(x_1, \\dots, x_K) \\propto\n\\pi(x_1)\\rho_2(x_2 \\mid x_1) \\cdots \\rho_{K}(x_K \\mid x_{K-1}).\n\\tag{11}\\] By construction, \\(x_1 \\sim \\pi\\) so that the target appears as a marginal distribution of \\(\\rho\\). We will formulate an IS algorithm over the extended state space \\((x_1, \\dots, x_K)\\) that targets the joint distribution \\(\\rho\\). If we are able to solve this modified (auxiliary variable) problem then we also obtain a solution to the original problem by simply extracting the first component from samples in the extended space. Our new goal is thus to formulate an IS algorithm with respect to the target distribution \\(\\rho\\)."
  },
  {
    "objectID": "blog/posts/annealed-IS/annealed-IS.html#markov-updates",
    "href": "blog/posts/annealed-IS/annealed-IS.html#markov-updates",
    "title": "Annealed Importance Sampling",
    "section": "2.3 Markov Updates",
    "text": "2.3 Markov Updates\nWhile we could have entertained many different target distributions in an extended state space, the conditional independence structure in Equation 2 is particularly attractive from a computational standpoint. Let’s encode this structure by defining a sequence of Markov kernels \\(T_1, \\dots, T_{K-1}\\) such that \\(T_k(x_k,x_{k+1}) = \\rho_{k+1}(x_{k+1} \\mid x_k)\\). 1 The joint \\(\\rho\\) can thus equivalently be written as \\[\n\\rho(x_1, \\dots, x_K) \\propto\n\\pi(x_1)T_1(x_1,x_2) \\cdots T_{K-1}(x_{K-1},x_K).\n\\tag{3}\\]\nWe require that \\(T_k\\) admits \\(\\pi_k\\) as an invariant distribution, meaning that, if \\(x \\sim \\pi_k\\), then the sample \\(x^\\prime \\sim T_k(x,\\cdot)\\) has distribution \\(\\pi_{k}\\)."
  },
  {
    "objectID": "blog/posts/annealed-IS/annealed-IS.html#markov-kernels",
    "href": "blog/posts/annealed-IS/annealed-IS.html#markov-kernels",
    "title": "Annealed Importance Sampling",
    "section": "1.2 Markov Kernels",
    "text": "1.2 Markov Kernels\nIf we could sample directly from the \\(\\pi_k\\) then the problem would already be solved, since \\(\\pi_K\\) is precisely the distribution from which we hope to sample. Given that this is not possible, we will instead try to approximately track the intermediate distributions via Markov chain iterations. The importance weights will ultimately correct for the fact that we are not exactly tracking the distributions. Let \\((F_k)_{k=1}^{K}\\) be a collection of Markov kernels such that \\(F_k\\) is \\(\\pi_k\\)-invariant. In particular, the target \\(\\pi\\) is the stationary distribution of \\(F_K\\).\n\n\n\n\n\n\nRemark\n\n\n\nIn general, the final Markov kernel \\(F_K\\) need not be \\(\\pi\\)-invariant, a fact that will emerge as we derive the algorithm. There is actually no \\(F_K\\) in Neal’s original formulation. However, in most applications I am aware of it is included.\n\n\nAn IS proposal can then be generated via \\[\n\\begin{align}\n&x_0 \\sim \\pi_0, &&x_k \\sim F_k(x_{k-1}, \\cdot), \\qquad k = 1, 2, \\dots, K.\n\\end{align}\n\\tag{4}\\] The resulting vector \\(x_{0:K} := (x_0, \\dots, x_K)\\) has joint distribution 1 \\[\n\\begin{align}\n&q(x_{0:K}) := \\frac{f^q(x_{0:K})}{Z_0}\n&&f^q(x_{0:K}) := f_0(x_0)\\prod_{k=1}^{K} F_k(x_{k-1}, x_k).\n\\end{align}\n\\tag{5}\\] Letting \\(q_k\\) denote the \\(k^{\\text{th}}\\) univariate marginal of \\(q\\), we see that \\[\nq_K(x_K) := \\int \\pi_0(x_0)\\prod_{k=1}^{K} F_k(x_{k-1}, x_k) \\ dx_{0:K-1}\n\\tag{6}\\] is the marginal distribution of \\(x_K\\). This final marginal can be viewed as the best approximation to \\(\\pi\\). If we view the final entry \\(x_K\\) of \\(x_{0:K}\\) as a standard IS proposal, then we would have to compute the importance weight \\(f(x_K)/f^q_K(x_K)\\). In light of Equation 6, the denominator is typically intractable, and we appear out of luck."
  },
  {
    "objectID": "blog/posts/annealed-IS/annealed-IS.html#reversed-markov-kernels",
    "href": "blog/posts/annealed-IS/annealed-IS.html#reversed-markov-kernels",
    "title": "Annealed Importance Sampling",
    "section": "2.4 Reversed Markov Kernels",
    "text": "2.4 Reversed Markov Kernels\nGiven the Markov kernel \\(T_k\\) with invariant distribution \\(\\pi_k\\), the reversed kernel \\(R_k\\) (with respect to \\(\\pi_k\\)) is defined by the identity \\[\n\\pi_k(x)T_k(x,x^\\prime) = \\pi_k(x^\\prime)R_k(x^\\prime, x).\n\\] Rearranging Equation 8, we obtain the explicit expression \\[\nR_k(x^\\prime, x) = \\frac{\\pi_k(x)}{\\pi_k(x^\\prime)} T_k(x,x^\\prime).\n\\tag{13}\\] The kernel \\(R_k\\) defines a valid Markov chain that preserves the stationary distribution \\(\\pi_k\\) when run backwards in time.\n\n\n\n\n\n\nProof\n\n\n\n\n\nPutting aside measure-theoretic technicalities, we simply verify that \\(R_k(x^\\prime, \\cdot)\\) defines a valid probability distribution for each \\(x^\\prime\\). This follows from \\[\n\\begin{align}\n\\int R_k(x^\\prime, x) dx\n&= \\int \\frac{\\pi_k(x)}{\\pi_k(x^\\prime)} T_k(x, x^\\prime) dx \\\\\n&= \\frac{1}{\\pi_k(x^\\prime)} \\int \\pi_k(x) T_k(x, x^\\prime) dx \\\\\n&= \\frac{\\pi_k(x^\\prime)}{\\pi_k(x^\\prime)} \\\\\n&= 1,\n\\end{align}\n\\] where the penultimate equality uses the fact that \\(T_k\\) is \\(\\pi_k\\)-invariant. To show that \\(R_k\\) is also \\(\\pi_k\\)-invariant, see that \\[\n\\begin{align}\n\\int \\pi_k(x^\\prime) R_k(x^\\prime, x) dx^\\prime\n&= \\int \\pi_k(x^\\prime) \\frac{\\pi_k(x)}{\\pi_k(x^\\prime)} T_k(x, x^\\prime) dx^\\prime \\\\\n&= \\int \\pi_k(x) T_k(x, x^\\prime) dx^\\prime \\\\\n&= \\pi_k(x) \\int T_k(x, x^\\prime) dx^\\prime \\\\\n&= \\pi_k(x),\n\\end{align}\n\\] where the final equality uses the fact that \\(T_k\\) is a Markov kernel.\n\n\n\nNote that certain Markov kernels (e.g., Metropolis-Hastings kernels) are explicitly constructed to satisfy \\(T_k = R_k\\) (i.e., the detailed balance condition).\nThe reversed kernels encode the proposal mechanism for the IS scheme \\[\nx_K \\sim \\pi_K, \\ \\pi_{K-1} \\sim R_K(x_K, \\cdot), \\ \\cdots, \\ x_1 \\sim R_2(x_2, \\cdot)\n\\tag{14}\\] where we assume that independent samples can be drawn from \\(\\pi_K\\). The joint distribution implied by Equation 14 is given by \\[\n\\rho^R(x_1, \\dots, x_K) := \\pi_K(x_K)R_{K}(x_K,x_{K-1}) \\cdots R_2(x_2,x_1)\n\\tag{15}\\] In general, \\(\\rho^R\\) differs from \\(\\rho\\). Under the latter, \\(x_1\\) has marginal distribution equal to \\(\\pi\\). Under the former, the marginal distribution of \\(x_1\\) can be seen as an approximation to \\(\\pi\\). If \\(K\\) is large and the pairs \\((R_k, R_{k+1})\\) are quite similar, then we would expect the sequence generated by Equation 15 to converge in distribution to something close to \\(\\pi\\). Alternatively, we would also expect a good approximation to \\(\\pi\\) when the \\(R_k\\) are very fast-mixing. For example, suppose that \\(R_1\\) is the composition of many Metropolis-Hastings steps. Then \\(x_1\\) will be approximately distributed according to the limiting distribution of \\(R_1\\), which is \\(\\pi\\). Of course, the tradeoff is that this would effectively require running an entire MCMC algorithm to generate a single sample from the proposal. In practice, the \\(R_k\\) will be defined to balance this computational cost with the proposal quality."
  },
  {
    "objectID": "blog/posts/annealed-IS/annealed-IS.html#importance-weights",
    "href": "blog/posts/annealed-IS/annealed-IS.html#importance-weights",
    "title": "Annealed Importance Sampling",
    "section": "1.4 Importance Weights",
    "text": "1.4 Importance Weights\nThe preceding sections define the target \\(p\\) and proposal \\(q\\) on the extended state space. The IS algorithm in the extended space proceeds via the steps:\n\nSample \\(x_{0:K} \\sim q\\).\nReturn \\((x_{0:K},w(x_{0:K}))\\), where \\(w(x_{0:K}) := f^p(x_{0:K})/f^q(x_{0:K})\\) is the importance weight.\n\nThis algorithm will only be useful if we can easily compute the importance weight. The Markov structure in \\(p\\) and \\(q\\) yields a very convenient form for \\(w\\). To see this, we start by applying Equation 9 to write \\(f^p\\) as \\[\nf^p(x_{0:K}) = f^q(x_{0:K}) \\frac{f(x_K)}{f_0(x_0)} \\prod_{k=1}^{K} \\frac{f_k(x_{k-1})}{f_k(x_k)}\n\\tag{10}\\]\n\n\n\n\n\n\nDerivation\n\n\n\n\n\n\\[\n\\begin{align}\nf^p(x_{0:K})\n&= f(x_K) \\prod_{k=1}^{K} R_k(x_k, x_{k-1}) \\\\\n&= f(x_K) \\prod_{k=1}^{K} \\frac{f_k(x_{k-1})}{f_k(x_k)} F_k(x_{k-1},x_k) \\\\\n&= \\left[f_0(x_0) \\prod_{k=1}^{K} F_k(x_{k-1},x_k) \\right] \\cdot\n\\frac{f(x_K)}{f_0(x_0)} \\prod_{k=1}^{K} \\frac{f_k(x_{k-1})}{f_k(x_k)} \\\\\n&= f^q(x_{0:K}) \\frac{f(x_K)}{f_0(x_0)} \\prod_{k=1}^{K} \\frac{f_k(x_{k-1})}{f_k(x_k)}\n\\end{align}\n\\]\n\n\n\nThe importance weight thus simplifies to \\[\nw(x_{0:K}) = \\frac{f^p(x_{0:K})}{f^q(x_{0:K})} = \\prod_{k=0}^{K-1} \\frac{f_{k+1}(x_k)}{f_k(x_k)} .\n\\tag{11}\\]\n\n\n\n\n\n\nDerivation\n\n\n\n\n\nUsing Equation 10, we have \\[\n\\begin{align}\n\\frac{f^p(x_{0:K})}{f^q(x_{0:K})}\n&= \\frac{f(x_K)}{f_0(x_0)} \\prod_{k=1}^{K} \\frac{f_k(x_{k-1})}{f_k(x_k)}\n\\end{align}\n\\]\n\n\n\nWe see in Equation 11 the precise reason why the absolute continuity assumption in Equation 3 is required. This assumption avoids dividing by zero in density ratios, ensuring the importance weight is well-defined.\n\n\n\n\n\n\nRemark\n\n\n\nNotice that the final value \\(x_K\\) from the proposal \\(x_{0:K} \\sim q\\) does not appear in the importance weight in Equation 11. This was hinted at by an earlier remark, which claimed that the algorithm would still be valid in the absence of the last kernel \\(F_K\\). Implicitly, this kernel contributes the term \\(f_K(x_K)/f_K(x_K)\\) in Equation 11."
  },
  {
    "objectID": "blog/posts/annealed-IS/annealed-IS.html#bridging-distributions-1",
    "href": "blog/posts/annealed-IS/annealed-IS.html#bridging-distributions-1",
    "title": "Annealed Importance Sampling",
    "section": "2.1 Bridging Distributions",
    "text": "2.1 Bridging Distributions\nTo start, let \\(x_K, x_{K-1}, \\dots, x_1\\) denote the sequence of updates, with \\(x_K\\) representing the initial proposal and \\(x_1 \\sim \\pi\\). That is, the sequence “bridges” from an initial proposal to a sample that is distributed from the target distribution. We introduce the notation \\[\nx_1 \\sim \\pi_1, x_2 \\sim \\pi_2, \\dots, \\pi_K \\sim \\pi_K\n\\tag{10}\\] for the intermediate distributions, with the constraint that \\(\\pi_1 = \\pi\\) so that we end up at the target. We might think of \\(\\pi_K\\) as an initial crude approximtion to \\(\\pi\\), with each update \\(\\pi_k \\mapsto \\pi_{k-1}\\) yielding a slightly better approximation, until we ultimately end up at the target \\(\\pi_1 = \\pi\\). As will be seen below, we require this sequence of distributions to satisfy \\[\n\\pi_k(x) = 0 \\implies \\pi_{k+1}(x) = 0.\n\\] In other words, the supports of the distributions cannot be growing as \\(\\pi_K\\) evolves to \\(\\pi_1\\)."
  },
  {
    "objectID": "blog/posts/annealed-IS/annealed-IS.html#markov-kernels-1",
    "href": "blog/posts/annealed-IS/annealed-IS.html#markov-kernels-1",
    "title": "Annealed Importance Sampling",
    "section": "2.3 Markov Kernels",
    "text": "2.3 Markov Kernels\nWhile we could have entertained many different target distributions in an extended state space, the conditional independence structure in Equation 11 is particularly attractive from a computational standpoint. Let’s encode this structure by defining a sequence of Markov kernels \\(T_2, \\dots, T_K\\) such that \\(T_k(x_{k-1},x_{k}) = \\rho_{k}(x_{k} \\mid x_{k-1})\\). 2 The joint \\(\\rho\\) can thus equivalently be written as \\[\n\\rho(x_1, \\dots, x_K) \\propto\n\\pi(x_1)T_1(x_1,x_2) \\cdots T_{K-1}(x_{K-1},x_K).\n\\tag{12}\\]\nWe require that \\(T_k\\) admits \\(\\pi_k\\) as an invariant distribution, meaning that if \\(x \\sim \\pi_k\\), then the sample \\(x^\\prime \\sim T_k(x,\\cdot)\\) has distribution \\(\\pi_{k}\\).\nNote that we have defined the kernels \\(T_k\\) such that sequential application of these operators define the sequence \\(x_1, \\dots, x_K\\). This sequence is in the opposite order of our heuristic discussions above, where we describe an evolution starting at \\(x_K\\) and ending at \\(x_1\\). As we will see, while it makes sense to define the extended target \\(\\rho\\) in Equation 12 in the forward order, the proposals for the IS scheme will be generated in the reversed order."
  },
  {
    "objectID": "blog/posts/annealed-IS/annealed-IS.html#extended-state-space",
    "href": "blog/posts/annealed-IS/annealed-IS.html#extended-state-space",
    "title": "Annealed Importance Sampling",
    "section": "1.3 Extended State Space",
    "text": "1.3 Extended State Space\nA clever way around this issue is to view Equation 4 as an IS proposal in an extended state space over \\((x_0, \\dots, x_K)\\). The joint density \\(q_K(x_K)\\) in Equation 6 is thus the associated proposal density. The question now becomes how to define the target distribution \\(p(x_{0:K}) = f^p(x_{0:K})/Z^p\\) over the extended state space. Most importantly, we will define the target such that it admits \\(\\pi\\) as a marginal. This implies that if we formulate a valid IS algorithm in the extended state space, then we will have solved the original problem by simply extracting the relevant component from samples of the extended state. We will also choose \\(p\\) to have a specific Markov structure that leads \\(f^p(x_{0:K})/f^q(x_{0:K})\\) to have a convenient, computable form. To this end, consider \\[\np(x_{0:K}) := \\frac{1}{Z^p} f(x_K) \\prod_{k=0}^{K-1} R_k(x_{k+1}, x_k),\n\\tag{7}\\] where the \\(R_k\\) are “backwards” Markov kernels – they evolve backwards in time. This joint distribution admits \\(\\pi\\) as the \\(K^{\\text{th}}\\) marginal, since \\[\\begin{align}\n\\int p(x_{0:K}) dx_{0:K-1}\n&= \\frac{1}{Z^p} f(x_K) \\prod_{k=0}^{K-1} \\int_{x_k} R_k(x_{k+1}, x_k) dx_k\n= \\frac{1}{Z^p} f(x_K),\n\\end{align}\\] following from the fact that each \\(R_k(x_k, \\cdot)\\) is a probability measure by definition. In AIS, the backward kernel \\(R_k\\) is chosen to be the reversal of \\(F_k\\). The reversed Markov kernel (with respect to invariant distribution \\(\\pi_k\\)) is defined by the identity \\[\n\\pi_k(x)F_k(x,x^\\prime) = \\pi_k(x^\\prime) R_k(x^\\prime, x)\n\\tag{8}\\] Rearranging Equation 8, we obtain the explicit expression \\[\nR_k(x^\\prime, x)\n= \\frac{\\pi_k(x)}{\\pi_k(x^\\prime)} F_k(x,x^\\prime)\n= \\frac{f_k(x)}{f_k(x^\\prime)} F_k(x,x^\\prime).\n\\tag{9}\\] The kernel \\(R_k\\) defines a valid Markov chain that preserves the stationary distribution \\(\\pi_k\\) when run backwards in time.\n\n\n\n\n\n\nProof\n\n\n\n\n\nPutting aside measure-theoretic technicalities, we simply verify that \\(R_k(x^\\prime, \\cdot)\\) defines a valid probability distribution for each \\(x^\\prime\\). This follows from \\[\n\\begin{align}\n\\int R_k(x^\\prime, x) dx\n&= \\int \\frac{\\pi_k(x)}{\\pi_k(x^\\prime)} F_k(x, x^\\prime) dx \\\\\n&= \\frac{1}{\\pi_k(x^\\prime)} \\int \\pi_k(x) F_k(x, x^\\prime) dx \\\\\n&= \\frac{\\pi_k(x^\\prime)}{\\pi_k(x^\\prime)} \\\\\n&= 1,\n\\end{align}\n\\] where the penultimate equality uses the fact that \\(F_k\\) is \\(\\pi_k\\)-invariant. To show that \\(R_k\\) is also \\(\\pi_k\\)-invariant, see that \\[\n\\begin{align}\n\\int \\pi_k(x^\\prime) R_k(x^\\prime, x) dx^\\prime\n&= \\int \\pi_k(x^\\prime) \\frac{\\pi_k(x)}{\\pi_k(x^\\prime)} F_k(x, x^\\prime) dx^\\prime \\\\\n&= \\int \\pi_k(x) R_k(x, x^\\prime) dx^\\prime \\\\\n&= \\pi_k(x) \\int R_k(x, x^\\prime) dx^\\prime \\\\\n&= \\pi_k(x),\n\\end{align}\n\\] where the final equality uses the fact that \\(R_k\\) is a Markov kernel.\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nCertain Markov kernels (e.g., Metropolis-Hastings kernels) are explicitly constructed to satisfy \\(F_k = R_k\\) (the detailed balance condition). Kernels that are not in detailed balance with \\(\\pi_k\\) require the re-weighting \\(\\pi_k(x)/\\pi_k(x^\\prime)\\) in Equation 8 to construct a kernel that preserves the stationary distribution when run in reverse."
  },
  {
    "objectID": "blog/posts/annealed-IS/annealed-IS.html#algorithm",
    "href": "blog/posts/annealed-IS/annealed-IS.html#algorithm",
    "title": "Annealed Importance Sampling",
    "section": "1.5 Algorithm",
    "text": "1.5 Algorithm\nThe core of the AIS algorithm is the generation of independent weighted samples \\((x^{(n)}_{0:K}, w(x_{0:K}^{(n)}))\\). We discuss the use of these samples for estimating expectations and normalizing constants in the next section. Here, we summarize the above derivations in an algorithm. The procedure simply requires sampling \\(x_{0:K} \\sim q\\) and then computing the weight in Equation 11. As seen below, this can be done in an online fashion, which shoes that AIS can be viewed as alternating between MCMC steps and IS updates. Note that the algorithm actually returns the logarithm of the importance weight for numerical stability.\n\nAlgorithm 1: Calculate AIS weighted sample \\[\n\\begin{array}{ll}\n&\\textbf{Input:} && (f_k)_{k=1}^K, \\ (F_k)_{k=1}^K \\\\\n& \\textbf{Output:} && \\text{Weighted sample } \\{x_{0:K}, \\log w(x_{0:K})\\} \\\\\n\\hline \\\\[0.1em]\n& x_0 \\sim \\pi_0 \\\\\n& \\ell_w \\gets 0 \\\\\n& \\textbf{for } k = 1 \\textbf{ to } K \\textbf{ do} \\\\\n& \\quad \\ell_w \\gets \\ell_w + \\log f_k(x_{k-1}) - \\log f_{k-1}(x_{k-1}) \\\\\n& \\quad x_k \\sim F_{k+1}(x_{k-1}, \\cdot) \\\\\n& \\textbf{end for} \\\\[0.1em]\n& x \\gets (x_0, \\dots, x_K) \\\\\n& \\textbf{Return:} \\ (x, \\ell_w)\n\\end{array}\n\\]"
  },
  {
    "objectID": "blog/posts/annealed-IS/annealed-IS.html#bayesian-inference",
    "href": "blog/posts/annealed-IS/annealed-IS.html#bayesian-inference",
    "title": "Annealed Importance Sampling",
    "section": "2.1 Bayesian Inference",
    "text": "2.1 Bayesian Inference\nThroughout this post, we have provided a generic discussion of AIS with respect to the goal of characterizing some target distribution \\(\\pi\\). A very common application is the setting where \\(\\pi\\) is a Bayesian posterior distribution. A Bayesian model consists of a joint probability distribution over parameters \\(x\\) and data \\(y\\), typically specified as \\[\n\\begin{equation}\n\\rho(x,y) := \\pi_0(x)\\mathsf{L}(x; y)\n\\end{equation}\n\\] where \\(\\pi_0\\) and \\(\\mathsf{L}\\) are the prior distribution and likelihood function, respectively. The posterior distribution is then given by \\[\n\\begin{equation}\n\\pi(x) := \\rho(x \\mid y) = \\frac{1}{Z} \\pi_0(x)\\mathsf{L}(x; y),\n\\end{equation}\n\\] which we now consider to be our target distribution in AIS. Using the notation from Equation 1, we write \\(f(x) = \\pi_0(x)\\mathsf{L}(x; y)\\) to denote the unnormalized target. 2 As the notation suggests, we will take the prior \\(\\pi_0\\) as the initial distribution in the sequence of bridging distributions. If we consider the geometric path in Equation 2, we obtain \\[\n\\pi_k(x)\n\\propto \\pi_0(x)^{1-\\beta_k} f(x)^{\\beta_k}\n= \\pi_0(x)^{1-\\beta_k} [\\pi_0(x)\\mathsf{L}(x; y)]^{\\beta_k}\n= \\pi_0(x) \\mathsf{L}(x; y)^{\\beta_k},\n\\] which shows that the intermediate distributions arise from a likelihood tempering schedule in this setting."
  },
  {
    "objectID": "blog/posts/annealed-IS/annealed-IS.html#intermediate-targets-on-extended-space",
    "href": "blog/posts/annealed-IS/annealed-IS.html#intermediate-targets-on-extended-space",
    "title": "Annealed Importance Sampling",
    "section": "2.2 Intermediate Targets on Extended Space",
    "text": "2.2 Intermediate Targets on Extended Space\nTo acheive the goal of producing weighted samples from \\(\\pi(x)\\), where \\(x \\in \\mathbb{R}^d\\), AIS considers a sequence of intermediate distributions \\((\\pi_k)_{k=0}^{K}\\) that bridge from a simple distribution \\(\\pi_0\\) to the target \\(\\pi_K = \\pi\\). However, as we discovered above, the correctness of the method relies on the fact that the IS is actually conducted on an extended state space. We further clarify this viewpoint here, and note connections to sequential Monte Carlo (SMC).\nRecall that the extended state is denoted by \\(x_{0:K}\\), where each entry \\(x_k\\) lives in \\(\\mathbb{R}^d\\), the same space as the original variable \\(x\\). The extended target distribution \\(p(x_{0:K})\\) defined in Equation 7 can be viewed as the final distribution \\(p_K\\) within a sequence of extended intermediate distributions \\((p_k)_{k=0}^{K}\\) given by \\[\n\\begin{equation}\np_k(x_{0:k}) := \\frac{1}{Z_k^p} f_k(x_k) \\prod_{k=0}^{k-1} R_k(x_{k+1}, x_k).\n\\end{equation}\n\\tag{12}\\]\nNotice that \\(p_K = p\\) indeed holds. While the supports of the intermediate distributions \\((\\pi_k)_{k=0}^{K}\\) are all \\(\\mathbb{R}^d\\), the supports of the extended intermediate distributions \\((p_k)_{k=0}^{K}\\) grow with \\(k\\). For example, \\(p_k\\) is defined over \\(\\mathbb{R}^{d(k+1)}\\). As the AIS weighted sample is incrementally constructed, the partial weighted sample \\(\\{x_{0:k}, w(x_{0:k})\\}\\) targets the intermediate distribution \\(p_k\\). These intermediate distributions are of interest only in that they help bridge to the final distribution, which is the one we ultimately care about.\nThe extended intermediate distributions in Equation 12 are utilized in algorithms other than AIS. For example, these same distributions are used in SMC, but they are approximated using particle filtering techniques rather than IS (e.g., see equation 1.12 in Naesseth, Lindsten, and Schön (2024)). In such settings, the \\(R_k\\) may be defined as other “backwards” Markov kernels, rather than restricting them to be the reversals of the \\(F_k\\)."
  },
  {
    "objectID": "blog/posts/involutive-mcmc/MH_Tierney.html",
    "href": "blog/posts/involutive-mcmc/MH_Tierney.html",
    "title": "Metropolis-Hastings Kernels for General State Spaces",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\Pr}{\\mathbb{P}}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\Def}{:=}\n\\newcommand{\\stateSpace}{\\mathsf{X}}\n\\newcommand{\\sigAlg}{\\mathcal{A}}\n\\newcommand{\\target}{\\mu}\n\\newcommand{\\Ker}{P}\n\\newcommand{\\propKer}{Q}\n\\newcommand{\\stateProp}{x^\\prime}\n\\newcommand{\\rejectProb}{s}\n\\]\nIn this post we walk through Tierney (1998), which describes a very general formulation of Metropolis-Hastings algorithms. In particular, the paper provides necessary and sufficient conditions on the proposal kernel and acceptance probability in order to define a Markov chain with a desired invariant distribution."
  },
  {
    "objectID": "blog/posts/involutive-mcmc/MH_Tierney.html#mcmc-and-reversibility",
    "href": "blog/posts/involutive-mcmc/MH_Tierney.html#mcmc-and-reversibility",
    "title": "Metropolis-Hastings Kernels for General State Spaces",
    "section": "1.1 MCMC and Reversibility",
    "text": "1.1 MCMC and Reversibility\nOur goal is to draw samples from a probability measure \\(\\target\\) defined on a measurable space \\((\\stateSpace, \\sigAlg)\\). Markov chain Monte Carlo (MCMC) algorithms address this goal by defining a Markov chain with \\(\\target\\)-invariant transition kernel \\(\\Ker\\), meaning \\[\n\\begin{equation}\n\\target(A) = \\int \\target(dx) \\Ker(x,A), \\qquad \\forall A \\in \\sigAlg.\n\\end{equation}\n\\tag{1}\\] In other words, the target distribution \\(\\target\\) is a fixed point of the operator \\(\\Ker\\). While it is generally difficult to construct \\(\\Ker\\) to satisfy the integral equation Equation 1, it is much easier to construct a \\(\\target\\)-reversible chain, which turns out to be a sufficient condition for Equation 1 to hold. A Markov chain is said to be \\(\\target\\)-reversible provided that the detailed balance relation \\[\n\\begin{equation}\n\\int_A \\target(dx)P(x,B) = \\int_B \\target(dy) P(y,A),\n\\qquad \\forall A,B \\in \\sigAlg\n\\end{equation}\n\\tag{2}\\] is satisfied; that is, the product measures \\(\\target(dx)P(x,dy)\\) and \\(\\target(dy)P(y,dx)\\) on \\((\\stateSpace \\times \\stateSpace, \\sigAlg \\otimes \\sigAlg)\\) are identical. To see that Equation 1 follows from Equation 2, simply set \\(B \\Def \\stateSpace\\). The detailed balance relation imposes a symmetry condition between pairs of sets \\((A,B)\\) when the chain is at equilibrium; i.e., if initialized at \\(\\target\\), then it is equally likely to start in \\(A\\) and transition to \\(B\\) as it is to start in \\(B\\) and transition to \\(A\\)."
  },
  {
    "objectID": "blog/posts/involutive-mcmc/MH_Tierney.html#metropolis-hastings",
    "href": "blog/posts/involutive-mcmc/MH_Tierney.html#metropolis-hastings",
    "title": "Metropolis-Hastings Kernels for General State Spaces",
    "section": "1.2 Metropolis-Hastings",
    "text": "1.2 Metropolis-Hastings\nMetropolis-Hastings (MH) algorithms are a popular class of MCMC methods, defined by a particular recipe for constructing \\(\\target\\)-reversible Markov kernels. Given a current state \\(x \\in \\stateSpace\\), a MH update proceeds by first proposing a new state \\(y \\sim \\propKer(x, \\cdot)\\) and then accepting or rejecting the proposed state according to some probability \\(\\alpha(x,y)\\). If rejected, the system remains at the current state \\(x\\). This generic recipe is thus defined by the choice of proposal kernel \\(\\propKer: \\stateSpace \\times \\sigAlg \\to [0,1]\\) and (measurable) acceptance probability function \\(\\alpha: \\stateSpace \\times \\stateSpace \\to [0,1]\\). This accept-reject mechanism yields a Markov chain with transition kernel \\[\n\\begin{align}\n&\\Ker(x, A) = \\int \\alpha(x,y)\\propKer(x,dy) + \\rejectProb(x)\\delta_x(A),\n&&\\rejectProb(x) = \\int [1 - \\alpha(x,y)] \\propKer(x,dy),\n\\end{align}\n\\tag{3}\\] with \\(s(x)\\) denoting the probability of a rejection occurring at state \\(x\\). The first term is the probability of proposing and accepting a new state in \\(A\\). The second term accounts for the case where the chain is already in \\(A\\), so that rejecting leaves the chain in \\(A\\)."
  }
]